{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Jupyter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "import sys, inspect, argparse, importlib, traceback, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# percentage of samples that exactly match\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    argmax_true = tf.math.argmax(y_true, axis=-1)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_pred = tf.math.argmax(y_pred, axis=-1)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    match_char = tf.math.equal(argmax_true, argmax_pred)     # match characters              (batch, width:int) -> (batch, width:bool)\n",
    "    match_word = tf.math.reduce_all(match_char, axis=-1)     # require all character in sample to match      (batch, width:bool) -> (batch:bool)\n",
    "    match_int = tf.cast(match_word, tf.float32)              # bool to int                                   (batch:bool) -> (batch:float)\n",
    "    return tf.reduce_mean(match_int)                         # percentage of samples that are an exact match (batch:float) -> float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def log(*l, **d): \n",
    "    if verbose: print(*l, **d)\n",
    "        \n",
    "training_history = []\n",
    "\n",
    "def training_log(x, y, a, b, e, l, m):\n",
    "    training_history.append({'x':x, 'y':y, 'architecture':a, 'batch size':b, 'epochs':e, 'loss':l, 'accuracy':m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tokens and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokens used to communicate non character entities\n",
    "# tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>', '<SurveyNum>', '<SurveyName>', '<LineName>', '<SurveyType>', '<PrimaryDataType>', '<SecondaryDataType>', '<TertiaryDataType>', '<Quaternary>', '<File_Range>', '<First_SP_CDP>', '<Last_SP_CDP>', '<CompletionYear>', '<TenureType>', '<Operator Name>', '<GSQBarcode>', '<EnergySource>', '<LookupDOSFilePath>', '<Source Of Data>']\n",
    "tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>']\n",
    "\n",
    "# get set of characters to be used, use static preset list of characters\n",
    "#available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890-_().,\\\\/\\\"':&\")\n",
    "available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-_().,\\\\/\\\"':&\")\n",
    "\n",
    "# generate character to int and int to character maps\n",
    "char_to_int = {c: i for i, c in enumerate(tokens + available_chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "char_count = len(char_to_int) # number of character available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Unique Record ID', 'FileName', 'Original_FileName', 'SurveyNum', 'SurveyName', 'LineName', 'SurveyType', 'PrimaryDataType', 'SecondaryDataType', 'TertiaryDataType', 'Quaternary', 'File_Range', 'First_SP_CDP', 'Last_SP_CDP', 'CompletionYear', 'TenureType', 'Operator Name', 'GSQBarcode', 'EnergySource', 'LookupDOSFilePath', 'Source Of Data', 'LookupDOSFilePath_Words', 'FileName_Words', 'LineName_Words'])\n"
     ]
    }
   ],
   "source": [
    "raw_source_file = 'SHUP 2D Files Training Data.csv'\n",
    "\n",
    "# read raw training data\n",
    "data_df = pd.read_csv(raw_source_file, dtype=str)\n",
    "data = {feature:data_df[feature].values for feature in data_df.columns.values}\n",
    "\n",
    "data['LookupDOSFilePath_Words'] = np.array([s.split('\\\\')[2:] for s in data['LookupDOSFilePath']])\n",
    "data['FileName_Words'] = np.array([s.split('_') for s in data['FileName']])\n",
    "data['LineName_Words'] = np.array([s.split('-') for s in data['LineName']])\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Vectorize Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data):\n",
    "    if type(data) == str:\n",
    "        return [char_to_int[char] for char in data.upper()]\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            return [vectorize_data(d) for d in data]\n",
    "        except Exception as e:\n",
    "            #traceback.print_exc()\n",
    "            #print(data, type(data))\n",
    "            return []\n",
    "\n",
    "\n",
    "def devectorise_data(data):\n",
    "        \n",
    "    length = data.shape[0]\n",
    "    \n",
    "    data = data.reshape(length, -1)\n",
    "    strings = np.full((length,), '', dtype=object)\n",
    "    \n",
    "    for i in range(length):\n",
    "        strings[i] = ''.join([int_to_char[int(i)] for i in data[i]])\n",
    "    \n",
    "    return strings\n",
    "    \n",
    "    \n",
    "#     ndim = data.ndim\n",
    "#     if data.dtype != object:\n",
    "#         data = data.astype(object)\n",
    "    \n",
    "#     # decode vector into string\n",
    "#     if ndim == 1:\n",
    "#         return ''.join([int_to_char[int(i)] for i in data])\n",
    "    \n",
    "#     # go to next level\n",
    "#     else:\n",
    "#         for i in range(len(data)):\n",
    "#             data[i] = devectorise_data(data[i])\n",
    "            \n",
    "#         return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Add Padding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find size of largest array across each dimension to computer shape of bounding ndarray\n",
    "def size(data):\n",
    "    \n",
    "    if type(data) == int:\n",
    "        return ()\n",
    "    \n",
    "    this_size = len(data)\n",
    "    \n",
    "    if this_size > 0:\n",
    "        inner_sizes = np.array([size(d) for d in data])\n",
    "        inner_sizes = tuple(np.amax(inner_sizes, axis=0))\n",
    "    else:\n",
    "        inner_sizes = ()\n",
    "    \n",
    "    return (this_size,) + inner_sizes\n",
    "    \n",
    "    \n",
    "def insert_vector(matrix, data, indices):\n",
    "    if type(data) == int:\n",
    "        matrix[indices] = data\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            insert_vector(matrix, data[i], indices + (i,))\n",
    "    \n",
    "\n",
    "def pad_vector_data(data, pad_token, pad_shape=None):\n",
    "    \n",
    "    shape = size(data)\n",
    "    if pad_shape != None:\n",
    "        shape = tuple(np.maximum(pad_shape, shape))\n",
    "\n",
    "    # empty matrix\n",
    "    matrix = np.full(shape, pad_token, np.int32)\n",
    "\n",
    "    insert_vector(matrix, data, ())\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split(data, sizes):\n",
    "    sizes = list(sizes)\n",
    "    \n",
    "    for i in range(1, len(sizes)):\n",
    "        sizes[i] += sizes[i-1]\n",
    "    \n",
    "    slices = [slice(i,j) for i, j in zip([0]+sizes, sizes)]\n",
    "    \n",
    "    return [data[s] for s in slices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*data):\n",
    "    order = np.arange(len(data[0]))         # default order of elements\n",
    "    np.random.shuffle(order)                # randomise order\n",
    "    return [d[order] for d in data]         # new array with items in the randimised order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract(*keys, **cuts):\n",
    "    \n",
    "    onehots, shapes = [], []\n",
    "        \n",
    "    for key in keys:\n",
    "        \n",
    "        # get data from dictionary\n",
    "        onehot = onehot_data[key]\n",
    "\n",
    "        # apply cuts\n",
    "        cut = cuts.get(key, [[None]])\n",
    "        cut = [slice(*c) for c in cut]\n",
    "        cut = len(onehot.shape)*[slice(None)] + cut + [slice(None)]\n",
    "        cut = tuple(cut[-len(onehot.shape):])\n",
    "        onehot = onehot[cut]\n",
    "        \n",
    "        # calculate shape\n",
    "        shape = (None, *onehot.shape[1:])[-3:]\n",
    "\n",
    "        onehots.append(onehot)\n",
    "        shapes.append(shape)\n",
    "\n",
    "    return onehots, shapes\n",
    "    \n",
    "def split_and_shuffle(*onehots, sizes=None, shuffle_before=False, shuffle_after=True):\n",
    "    sizes = sizes or [None]\n",
    "    key_count = len(onehots)\n",
    "    subset_count = len(sizes)\n",
    "    \n",
    "    if shuffle_before:\n",
    "        onehots = shuffle(*onehots)\n",
    "\n",
    "    onehots_subsets = np.full((key_count, subset_count), None)\n",
    "    onehots_subsets[:,:] = [split(onehot, sizes) for onehot in onehots]\n",
    "\n",
    "    if shuffle_after:\n",
    "        for i in range(subset_count):\n",
    "            onehots_subsets[:,i] = shuffle(*onehots_subsets[:,i])\n",
    "\n",
    "    return onehots_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unique Record ID':            (23903, 6, 53)\n",
      "'FileName':                    (23903, 87, 53)\n",
      "'Original_FileName':           (23903, 71, 53)\n",
      "'SurveyNum':                   (23903, 5, 53)\n",
      "'SurveyName':                  (23903, 39, 53)\n",
      "'LineName':                    (23903, 23, 53)\n",
      "'SurveyType':                  (23903, 6, 53)\n",
      "'PrimaryDataType':             (23903, 14, 53)\n",
      "'SecondaryDataType':           (23903, 36, 53)\n",
      "'TertiaryDataType':            (23903, 17, 53)\n",
      "'Quaternary':                  (23903, 8, 53)\n",
      "'File_Range':                  (23903, 13, 53)\n",
      "'First_SP_CDP':                (23903, 8, 53)\n",
      "'Last_SP_CDP':                 (23903, 7, 53)\n",
      "'CompletionYear':              (23903, 4, 53)\n",
      "'TenureType':                  (23903, 3, 53)\n",
      "'Operator Name':               (23903, 47, 53)\n",
      "'GSQBarcode':                  (23903, 17, 53)\n",
      "'EnergySource':                (23903, 29, 53)\n",
      "'LookupDOSFilePath':           (23903, 181, 53)\n",
      "'Source Of Data':              (23903, 8, 53)\n",
      "'LookupDOSFilePath_Words':     (23903, 6, 87, 53)\n",
      "'FileName_Words':              (23903, 13, 23, 53)\n",
      "'LineName_Words':              (23903, 4, 11, 53)\n"
     ]
    }
   ],
   "source": [
    "# extract LookupDOSFilePath for speccial processing\n",
    "\n",
    "vectorized_data = {f: vectorize_data(data[f])    for f in data}\n",
    "padded_data =     {f: pad_vector_data(vectorized_data[f], char_to_int['<Padding>'])    for f in vectorized_data}\n",
    "onehot_data =     {f: keras.utils.to_categorical(padded_data[f], char_count)    for f in padded_data}\n",
    "\n",
    "for f in onehot_data: print(f\"'{f}':\".ljust(30), onehot_data[f].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function: Test and show samlpe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, x_test=None, y_test=None, x_preview=None, y_preview=None):\n",
    "    \n",
    "    if (x_preview is not None) and (y_preview is not None):\n",
    "        t_size = len(x_preview)\n",
    "        \n",
    "        p_one_hot = model.predict(x_preview)\n",
    "        p_vector = np.argmax(p_one_hot, -1)\n",
    "        p_vector = p_vector.reshape((t_size, -1))\n",
    "        p_strings = devectorise_data(p_vector)\n",
    "\n",
    "        y_vector = np.argmax(y_preview, -1)\n",
    "        y_vector = y_vector.reshape((t_size, -1))\n",
    "        y_strings = devectorise_data(y_vector)\n",
    "\n",
    "        x_vector = np.argmax(x_preview, -1)\n",
    "        x_vector = x_vector.reshape((t_size, -1))\n",
    "        x_strings = devectorise_data(x_vector)\n",
    "\n",
    "        n_strings = [f'{i}. ' for i in range(t_size)]\n",
    "        x_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in x_strings]\n",
    "        y_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in y_strings]\n",
    "        p_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in p_strings]\n",
    "        n_w, x_w, y_w, p_w = max([len(s) for s in n_strings]), max([len(s) for s in x_strings]), max([len(s) for s in y_strings]), max([len(s) for s in p_strings])\n",
    "        y_p_strings = [\"'  '\".join([n.ljust(n_w), x.ljust(x_w), y.ljust(y_w), p.ljust(p_w), str(y==p)]) for n, x, y, p in zip(n_strings, x_strings, y_strings, p_strings)]\n",
    "\n",
    "        print(*y_p_strings, sep='\\n', end='\\n\\n')\n",
    "\n",
    "    if (x_test is not None) and (y_test is not None):\n",
    "        \n",
    "        # metric names\n",
    "        metrics = [model.loss] + model.metrics\n",
    "        \n",
    "        # accuracy on entire training set\n",
    "        accuracies = model.evaluate(x_test, y_test)\n",
    "        print(*list(zip(metrics, accuracies)), sep='\\n', end='\\n\\n') # evaluate and list loss and each metric\n",
    "\n",
    "        return accuracies[0], accuracies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "character_embedding_size = 10\n",
    "word_embedding_size = 40\n",
    "architecture = ''\n",
    "\n",
    "metrics = ['mean_absolute_error', 'categorical_accuracy', exact_match_accuracy] # binary_accuracy\n",
    "loss = 'categorical_crossentropy' # poisson mean_squared_logarithmic_error categorical_crossentropy\n",
    "\n",
    "embed_loss='categorical_crossentropy'\n",
    "embed_metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', exact_match_accuracy]\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Character Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto Encoder Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create offset input and output sequences to training a preditive embedding model.\n",
    "\n",
    "(x_char_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract('LookupDOSFilePath')\n",
    "(x_char_onehot,) = shuffle(x_char_onehot)\n",
    "shape = x_char_onehot.shape\n",
    "x_embed_size = len(x_char_onehot)\n",
    "\n",
    "# create columns of padding tokens\n",
    "padding = np.full((*shape[:-2], 1), char_to_int['<Padding>'])\n",
    "#padding = np.array([[char_to_int['<Padding>']]] * x_embed_size)\n",
    "padding = keras.utils.to_categorical(padding, char_count)\n",
    "padding = padding.reshape(*shape[:-2], 1, shape[-1])\n",
    "\n",
    "# 'abcd' -> ('_abcd', 'abcd_')\n",
    "x_embed_train = np.concatenate((x_char_onehot, padding), axis=-2)\n",
    "y_embed_train = np.concatenate((padding, x_char_onehot), axis=-2)\n",
    "x_embed_test = np.concatenate((x_char_onehot, padding), axis=-2)\n",
    "y_embed_test = np.concatenate((padding, x_char_onehot), axis=-2)\n",
    "\n",
    "embed_char_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto Encoder: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lh (Dense)                   (None, 182, 10)           540       \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 182, 53)           583       \n",
      "=================================================================\n",
      "Total params: 1,123\n",
      "Trainable params: 1,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Character-Embedding'\n",
    "\n",
    "model_E_D_NN = keras.Sequential()\n",
    "model_E_D_NN.add(keras.layers.Dense(character_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_D_NN.add(keras.layers.Dense(char_count, activation='sigmoid', name='lo'))\n",
    "#model_E_D_NN.add(keras.layers.Dropout(0.001))\n",
    "model_E_D_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_D_NN\n",
    "print(model_E_D_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "23903/23903 [==============================] - 8s 346us/step - loss: 2.1350 - acc: 0.4395 - mean_absolute_error: 0.1154 - categorical_accuracy: 0.4395 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "23903/23903 [==============================] - 7s 307us/step - loss: 1.4294 - acc: 0.5608 - mean_absolute_error: 0.0340 - categorical_accuracy: 0.5608 - exact_match_accuracy: 0.0000e+00\n",
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  'A_SSUA\\ED_SORSG_AUOROS__SEDTAED_SUUROADETATA\\118AEOSEROGD\\ED_TOC_RADTAED\\RATAD1EA_SEGAEOSEROGD\\ED_TOC_RADTAED\\RATAD1ED111SUAD_FAASOSED_OTASED_O8\\1118D\\191\\91_EG'  'False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> y   predictive\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, y_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], y_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23903/23903 [==============================] - 7s 295us/step - loss: 0.2876 - acc: 0.9249 - mean_absolute_error: 0.0208 - categorical_accuracy: 0.9249 - exact_match_accuracy: 0.1995\n",
      "Epoch 2/3\n",
      "23903/23903 [==============================] - 7s 283us/step - loss: 0.0074 - acc: 1.0000 - mean_absolute_error: 0.0171 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9939\n",
      "Epoch 3/3\n",
      "23903/23903 [==============================] - 7s 285us/step - loss: 0.0020 - acc: 1.0000 - mean_absolute_error: 0.0169 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9995\n",
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  'True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> x   direct\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, x_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], x_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# w1 = model.layers[0].get_weights()\n",
    "# w2 = model.layers[1].get_weights()\n",
    "\n",
    "# w = [np.copy(w1[0]), np.zeros(w1[1].shape)]\n",
    "# wi = [np.linalg.pinv(w1[0]), np.zeros(w2[1].shape)]\n",
    "\n",
    "# m = keras.Sequential()\n",
    "# m.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(embed_char_count, voc_size,)))\n",
    "# m.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "# m.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "# m.layers[0].set_weights(w)\n",
    "# m.layers[1].set_weights(wi)\n",
    "\n",
    "# encode_weights, decode_weights = w, wi\n",
    "\n",
    "# accuracy = model.evaluate(embed_test_x, embed_test_x)\n",
    "# metric_names = [embed_loss] + embed_metrics\n",
    "# dict(zip(metric_names, accuracy))\n",
    "char_encode_weights, char_decode_weights = models['Character-Embedding'].layers[0].get_weights(), models['Character-Embedding'].layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EPL_FILTERED_SCALED_QR014370_138618.SGY'  'True\n",
      "1. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\JUANDAH\\SEGY\\JUANDAH_CSJ88-25X_FINAL_MIGRATED_SDU10912TA_130518.SGY                                            '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\JUANDAH\\SEGY\\JUANDAH_CSJ88-25X_FINAL_MIGRATED_SDU10912TA_130518.SGY                                            '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\JUANDAH\\SEGY\\JUANDAH_CSJ88-25X_FINAL_MIGRATED_SDU10912TA_130518.SGY                                            '  'True\n",
      "2. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRAHE\\SEGY\\BRAHE_84-TGP_FINAL_FILTERED_MIGRATED_QR014106_166858.SGY                                            '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRAHE\\SEGY\\BRAHE_84-TGP_FINAL_FILTERED_MIGRATED_QR014106_166858.SGY                                            '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRAHE\\SEGY\\BRAHE_84-TGP_FINAL_FILTERED_MIGRATED_QR014106_166858.SGY                                            '  'True\n",
      "3. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\TIGGRIGIE_CREEK_DETAIL\\SEGY\\TIGGRIGIE_CREEK_DETAIL_C83T-6_STACK_SDU10912TA_129857.SGY                          '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\TIGGRIGIE_CREEK_DETAIL\\SEGY\\TIGGRIGIE_CREEK_DETAIL_C83T-6_STACK_SDU10912TA_129857.SGY                          '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\TIGGRIGIE_CREEK_DETAIL\\SEGY\\TIGGRIGIE_CREEK_DETAIL_C83T-6_STACK_SDU10912TA_129857.SGY                          '  'True\n",
      "4. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\DINGO_CREEK\\SEGY\\DINGO_CREEK_ME88-70_MIGRATED_GSC026557TA_199487.SGY                                           '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\DINGO_CREEK\\SEGY\\DINGO_CREEK_ME88-70_MIGRATED_GSC026557TA_199487.SGY                                           '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\DINGO_CREEK\\SEGY\\DINGO_CREEK_ME88-70_MIGRATED_GSC026557TA_199487.SGY                                           '  'True\n",
      "\n",
      "23903/23903 [==============================] - 4s 178us/step\n",
      "('categorical_crossentropy', 0.0011421034636629125)\n",
      "('accuracy', 0.9999905756087112)\n",
      "('mean_absolute_error', 0.016861919375853483)\n",
      "('categorical_accuracy', 0.9999905756087112)\n",
      "(<function exact_match_accuracy at 0x000002634309C048>, 0.9995398067188219)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0011421034636629125, 0.9995398067188219)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test(models['Character-Embedding'], x_embed_train, x_embed_train, x_embed_train[:5], x_embed_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto Encoder Folder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 23, 53)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_word_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract('FileName_Words')\n",
    "(x_word_onehot,) = shuffle(x_word_onehot)\n",
    "embed_word_count, embed_char_count, embed_ones_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto Encoder Model: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 13, 23, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 13, 230)           0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 13, 40)            9240      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 13, 230)           9430      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 13, 23, 10)        0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 13, 23, 53)        583       \n",
      "=================================================================\n",
      "Total params: 19,793\n",
      "Trainable params: 19,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Word-Embedding'\n",
    "\n",
    "model_E_W_NN = keras.Sequential()\n",
    "# character embedding\n",
    "model_E_W_NN.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=True, input_shape=(embed_word_count, embed_char_count, embed_ones_count)))\n",
    "model_E_W_NN.add(keras.layers.Reshape((embed_word_count, embed_char_count * character_embedding_size,)))\n",
    "\n",
    "# word auto encoder\n",
    "model_E_W_NN.add(keras.layers.Dense(word_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_W_NN.add(keras.layers.Dense(embed_char_count * character_embedding_size, activation='sigmoid', name='lo'))\n",
    "\n",
    "# character de embedding\n",
    "model_E_W_NN.add(keras.layers.Reshape((embed_word_count, embed_char_count, character_embedding_size)))\n",
    "model_E_W_NN.add(keras.layers.Dense(embed_ones_count, activation='sigmoid', trainable=True, name='char_decode')) \n",
    "\n",
    "model_E_W_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_W_NN\n",
    "print(model_E_W_NN.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_E_W_NN.layers[0].set_weights(char_encode_weights)\n",
    "model_E_W_NN.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "23903/23903 [==============================] - 14s 577us/step - loss: 0.9796 - acc: 0.8350 - mean_absolute_error: 0.0439 - categorical_accuracy: 0.8350 - exact_match_accuracy: 0.4748\n",
      "Epoch 2/6\n",
      "23903/23903 [==============================] - 12s 518us/step - loss: 0.2680 - acc: 0.9066 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9066 - exact_match_accuracy: 0.5846\n",
      "Epoch 3/6\n",
      "23903/23903 [==============================] - 14s 578us/step - loss: 0.0527 - acc: 0.9967 - mean_absolute_error: 0.0086 - categorical_accuracy: 0.9967 - exact_match_accuracy: 0.9455\n",
      "Epoch 4/6\n",
      "23903/23903 [==============================] - 16s 649us/step - loss: 0.0171 - acc: 0.9992 - mean_absolute_error: 0.0096 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.98656s - \n",
      "Epoch 5/6\n",
      "23903/23903 [==============================] - 15s 608us/step - loss: 0.0080 - acc: 0.9997 - mean_absolute_error: 0.0098 - categorical_accuracy: 0.9997 - exact_match_accuracy: 0.9946\n",
      "Epoch 6/6\n",
      "23903/23903 [==============================] - 12s 509us/step - loss: 0.0042 - acc: 0.9998 - mean_absolute_error: 0.0100 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9969\n",
      "0. '  'EAST BALLYMENA NOMBY BRENTWOOD M97-NO03 FINAL MIGRATED SDU02060TA 238515.SGY'  'EAST BALLYMENA NOMBY BRENTWOOD M97-NO03 FINAL MIGRATED SDU02060TA 238515.SGY'  'EAST BALLYMENA NOMBY BRENTWOOD M97-NO03 FINAL MIGRATED SDU02060TA 238515.SGY'  'True\n",
      "1. '  'OXLEY 90-GKB FINAL STACK SDU10912TA 211694.SGY                              '  'OXLEY 90-GKB FINAL STACK SDU10912TA 211694.SGY                              '  'OXLEY 90-GKB FINAL STACK SDU10912TA 211694.SGY                              '  'True\n",
      "2. '  'CATHOO EHC85-104 UNFILTERED FINAL SDU06351TA 198149.SGY                     '  'CATHOO EHC85-104 UNFILTERED FINAL SDU06351TA 198149.SGY                     '  'CATHOO EHC85-104 UNFILTERED FINAL SDU06351TA 198149.SGY                     '  'True\n",
      "\n",
      "23903/23903 [==============================] - 7s 286us/step\n",
      "('categorical_crossentropy', 0.003113500618446009)\n",
      "('accuracy', 0.9998554733215164)\n",
      "('mean_absolute_error', 0.010085613616532459)\n",
      "('categorical_accuracy', 0.9998554733215164)\n",
      "(<function exact_match_accuracy at 0x000002634309C048>, 0.9976217906400159)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.003113500618446009, 0.9976217906400159)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x -> x   direct\n",
    "epochs = 6\n",
    "batch_size = 16\n",
    "models['Word-Embedding'].fit(x_word_onehot, x_word_onehot, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Word-Embedding'], x_word_onehot, x_word_onehot, x_word_onehot[:3], x_word_onehot[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_encode_weights, word_decode_weights = models['Word-Embedding'].layers[2].get_weights(), models['Word-Embedding'].layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### P-NN: Input, Embedding, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-984d49fe0004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'P-NN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_P_NN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                       \u001b[1;31m# flatten to 1D per sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'P-NN'\n",
    "\n",
    "model_P_NN = keras.Sequential()\n",
    "model_P_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_P_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_P_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_P_NN.add(keras.layers.Dropout(0.001))                                                                  # dropout to prevent overfitting\n",
    "model_P_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_P_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_P_NN\n",
    "print(model_P_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FF-NN: Input, Embedding, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 460)               0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1264)              582704    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 948)               1199220   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 1,783,504\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'FF-NN'\n",
    "hidden_size = (y_shape_ones*embedding_size + y_shape_char*y_shape_ones) // 2\n",
    "\n",
    "model_FF_NN = keras.Sequential()\n",
    "model_FF_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_FF_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_FF_NN.add(keras.layers.Dense(hidden_size, activation='exponential', name='lh'))                         # dense layer\n",
    "model_FF_NN.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_FF_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_FF_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_FF_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_FF_NN\n",
    "print(model_FF_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN1: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 948)               3674448   \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 3,676,028\n",
      "Trainable params: 3,676,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN1'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN1 = keras.Sequential()\n",
    "model_LSTM_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_LSTM_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN1.add(keras.activation.exponential())\n",
    "model_LSTM_RNN1.add(keras.layers.LSTM(y_shape_char * y_shape_ones, activation='exponential', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN1\n",
    "print(model_LSTM_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN2: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 6, 20)             1080      \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 240)               250560    \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               28920     \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 6, 53)             1113      \n",
      "=================================================================\n",
      "Total params: 281,673\n",
      "Trainable params: 279,480\n",
      "Non-trainable params: 2,193\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN2'\n",
    "\n",
    "x_name, y_name = 'LineName', 'LineName'\n",
    "cuts = {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = y_char_count * character_embedding_size * 2\n",
    "\n",
    "model_LSTM_RNN2 = keras.Sequential()\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_char_count, x_ones_count)))                 # embed characters into dense embedded space\n",
    "#model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.LSTM(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='lstm1'))     # lstm recurrent cell\n",
    "#model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_LSTM_RNN2.add(keras.layers.Reshape((y_char_count, character_embedding_size)))                                        # un flatten\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_LSTM_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN2\n",
    "print(model_LSTM_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_LSTM_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_LSTM_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-RNN3: Input, Embed Characters, Embed Words, (LSTM), De-embed Words, De-embed Characters, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 13, 23, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 13, 230)           0         \n",
      "_________________________________________________________________\n",
      "word_encode (Dense)          (None, 13, 40)            9240      \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 100)               56400     \n",
      "_________________________________________________________________\n",
      "lstm_decode (Dense)          (None, 40)                4040      \n",
      "_________________________________________________________________\n",
      "word_decode (Dense)          (None, 230)               9430      \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 23, 10)            0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 23, 53)            583       \n",
      "=================================================================\n",
      "Total params: 80,233\n",
      "Trainable params: 80,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# WARNING the maximum word length of FileName_words == 23 == LineName length\n",
    "# this is only a conincidance and other data will differe and cause errors\n",
    "architecture = 'LSTM-RNN3'\n",
    "\n",
    "x_name, y_name = 'FileName_Words', 'LineName'\n",
    "cuts = {} # {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = 100\n",
    "embed_trainable = True\n",
    "\n",
    "model_LSTM_RNN3 = keras.Sequential()\n",
    "\n",
    "# character embedding\n",
    "model_LSTM_RNN3.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=embed_trainable, input_shape=(x_word_count, x_char_count, x_ones_count)))\n",
    "\n",
    "# word embedding\n",
    "model_LSTM_RNN3.add(keras.layers.Reshape((x_word_count, x_char_count * character_embedding_size,)))\n",
    "model_LSTM_RNN3.add(keras.layers.Dense(word_embedding_size, trainable=embed_trainable, name='word_encode'))\n",
    "\n",
    "# lstm processing\n",
    "model_LSTM_RNN3.add(keras.layers.LSTM(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='lstm1'))\n",
    "model_LSTM_RNN3.add(keras.layers.Dense(word_embedding_size, activation='sigmoid', name='lstm_decode'))\n",
    "\n",
    "# word de embedding\n",
    "model_LSTM_RNN3.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', trainable=embed_trainable, name='word_decode'))\n",
    "model_LSTM_RNN3.add(keras.layers.Reshape((y_char_count, character_embedding_size)))\n",
    "\n",
    "# character de embedding\n",
    "model_LSTM_RNN3.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=embed_trainable, name='char_decode')) \n",
    "\n",
    "\n",
    "model_LSTM_RNN3.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN3\n",
    "print(model_LSTM_RNN3.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_LSTM_RNN3.layers[0].set_weights(char_encode_weights)\n",
    "model_LSTM_RNN3.layers[2].set_weights(word_encode_weights)\n",
    "model_LSTM_RNN3.layers[-3].set_weights(word_decode_weights)\n",
    "model_LSTM_RNN3.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN1: Input, Embedding, (GRU), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voc_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-05384da3bcb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'GRU-RNN1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlstm_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoc_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_GRU_RNN1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_GRU_RNN1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'voc_size' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN1'\n",
    "lstm_hidden_size = voc_size * 15\n",
    "\n",
    "model_GRU_RNN1 = keras.Sequential()\n",
    "model_GRU_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_GRU_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_GRU_RNN1.add(keras.layers.GRU(y_shape_char * y_shape_ones, activation='relu', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_GRU_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_GRU_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN1\n",
    "print(model_GRU_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN2: Imput Embedding, (GRU), Decoder, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 12, 10)            540       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, 160)               82080     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               19320     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 12, 53)            583       \n",
      "=================================================================\n",
      "Total params: 102,523\n",
      "Trainable params: 101,400\n",
      "Non-trainable params: 1,123\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN2'\n",
    "\n",
    "x_name, y_name = 'LineName', 'LineName'\n",
    "cuts = {x_name:[[None, 12]] , y_name:[[None, 12]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = y_char_count * character_embedding_size + 40\n",
    "\n",
    "model_GRU_RNN2 = keras.Sequential()\n",
    "model_GRU_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_char_count, x_ones_count)))                 # embed characters into dense embedded space\n",
    "model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.GRU(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='gru1'))     # gru recurrent cell\n",
    "model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.add(keras.layers.Reshape((y_char_count, character_embedding_size)))                                        # un flatten\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN2\n",
    "print(model_GRU_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_GRU_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_GRU_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save/Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DE = models['E-D-NN'].get_weights()\n",
    "#model_GRU_1 = model\n",
    "#model_GRU_2 = model\n",
    "#model_GRU_3 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.set_weights(GRU)\n",
    "#model = model_GRU_3\n",
    "#models['E-D-NN'].set_weights(DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "20000/20000 [==============================] - 9s 466us/step - loss: 1.1259 - mean_absolute_error: 0.0479 - categorical_accuracy: 0.7076 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/70\n",
      "20000/20000 [==============================] - 8s 388us/step - loss: 1.0626 - mean_absolute_error: 0.0446 - categorical_accuracy: 0.7076 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 3/70\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 1.0280 - mean_absolute_error: 0.0440 - categorical_accuracy: 0.7102 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 4/70\n",
      "20000/20000 [==============================] - 7s 358us/step - loss: 1.0070 - mean_absolute_error: 0.0440 - categorical_accuracy: 0.7150 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 5/70\n",
      "20000/20000 [==============================] - 8s 390us/step - loss: 0.9933 - mean_absolute_error: 0.0438 - categorical_accuracy: 0.7150 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 6/70\n",
      "20000/20000 [==============================] - 9s 425us/step - loss: 0.9836 - mean_absolute_error: 0.0438 - categorical_accuracy: 0.7150 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 7/70\n",
      "20000/20000 [==============================] - 9s 455us/step - loss: 0.9763 - mean_absolute_error: 0.0439 - categorical_accuracy: 0.7330 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 8/70\n",
      "20000/20000 [==============================] - 7s 367us/step - loss: 0.9688 - mean_absolute_error: 0.0438 - categorical_accuracy: 0.7540 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 9/70\n",
      "20000/20000 [==============================] - 7s 374us/step - loss: 0.9379 - mean_absolute_error: 0.0384 - categorical_accuracy: 0.7707 - exact_match_accuracy: 0.0000e+00 5s - loss: 0.9610 - mean_absolute_er\n",
      "Epoch 10/70\n",
      "20000/20000 [==============================] - 7s 367us/step - loss: 0.8154 - mean_absolute_error: 0.0382 - categorical_accuracy: 0.7758 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 11/70\n",
      "20000/20000 [==============================] - ETA: 0s - loss: 0.7431 - mean_absolute_error: 0.0337 - categorical_accuracy: 0.7901 - exact_match_accuracy: 0.0000e+0 - 8s 384us/step - loss: 0.7428 - mean_absolute_error: 0.0337 - categorical_accuracy: 0.7901 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 12/70\n",
      "20000/20000 [==============================] - 8s 413us/step - loss: 0.6957 - mean_absolute_error: 0.0301 - categorical_accuracy: 0.7989 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 13/70\n",
      "20000/20000 [==============================] - 9s 442us/step - loss: 0.6582 - mean_absolute_error: 0.0261 - categorical_accuracy: 0.8050 - exact_match_accuracy: 5.0000e-05 5s - loss: 0.6690 - mean_absolute_error: 0.0273 - categorical_accuracy: 0.8041  - ETA: 3s - loss: 0.6665 - mean_absolute_error: 0\n",
      "Epoch 14/70\n",
      "20000/20000 [==============================] - 8s 407us/step - loss: 0.6215 - mean_absolute_error: 0.0238 - categorical_accuracy: 0.8103 - exact_match_accuracy: 1.0000e-04\n",
      "Epoch 15/70\n",
      "20000/20000 [==============================] - 9s 464us/step - loss: 0.5876 - mean_absolute_error: 0.0219 - categorical_accuracy: 0.8231 - exact_match_accuracy: 2.5000e-04 7s - loss: 0.5985 - mean_absolute_er - ETA: 2s - loss: 0.5927 - mean_absolute_error: 0.0222 - categorical_accuracy: 0.8219 - exact_match_accu - ETA: 1s - loss: 0.5909 - mean_absolute_error: 0.0222 - categorical_accuracy: 0.8225 -\n",
      "Epoch 16/70\n",
      "20000/20000 [==============================] - 9s 450us/step - loss: 0.5596 - mean_absolute_error: 0.0189 - categorical_accuracy: 0.8296 - exact_match_accuracy: 7.0000e-04\n",
      "Epoch 17/70\n",
      "20000/20000 [==============================] - 7s 369us/step - loss: 0.5330 - mean_absolute_error: 0.0156 - categorical_accuracy: 0.8370 - exact_match_accuracy: 1.0000e-03- loss: 0.5365 - mean_absolute_error: 0.0161 - categorical_accur\n",
      "Epoch 18/70\n",
      "20000/20000 [==============================] - 8s 387us/step - loss: 0.5066 - mean_absolute_error: 0.0131 - categorical_accuracy: 0.8459 - exact_match_accuracy: 0.0017 0s - loss: 0.5078 - mean_absolute_error: 0.0132 - categorical_accuracy: 0.8455 - exact_match_a\n",
      "Epoch 19/70\n",
      "20000/20000 [==============================] - 8s 393us/step - loss: 0.4822 - mean_absolute_error: 0.0115 - categorical_accuracy: 0.8548 - exact_match_accuracy: 0.0032\n",
      "Epoch 20/70\n",
      "20000/20000 [==============================] - 8s 394us/step - loss: 0.4617 - mean_absolute_error: 0.0106 - categorical_accuracy: 0.8603 - exact_match_accuracy: 0.0039\n",
      "Epoch 21/70\n",
      "20000/20000 [==============================] - 8s 393us/step - loss: 0.4418 - mean_absolute_error: 0.0099 - categorical_accuracy: 0.8656 - exact_match_accuracy: 0.0061 5s - loss: 0.4468 - mean_a\n",
      "Epoch 22/70\n",
      "20000/20000 [==============================] - 8s 391us/step - loss: 0.4218 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.8711 - exact_match_accuracy: 0.0084\n",
      "Epoch 23/70\n",
      "20000/20000 [==============================] - 8s 383us/step - loss: 0.4028 - mean_absolute_error: 0.0088 - categorical_accuracy: 0.8766 - exact_match_accuracy: 0.0136 4s - loss: 0.4064 - mean_absolute_error:\n",
      "Epoch 24/70\n",
      "20000/20000 [==============================] - 8s 378us/step - loss: 0.3842 - mean_absolute_error: 0.0084 - categorical_accuracy: 0.8830 - exact_match_accuracy: 0.0211\n",
      "Epoch 25/70\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.3662 - mean_absolute_error: 0.0082 - categorical_accuracy: 0.8902 - exact_match_accuracy: 0.0336\n",
      "Epoch 26/70\n",
      "20000/20000 [==============================] - 7s 364us/step - loss: 0.3481 - mean_absolute_error: 0.0079 - categorical_accuracy: 0.8977 - exact_match_accuracy: 0.0500\n",
      "Epoch 27/70\n",
      "20000/20000 [==============================] - 8s 392us/step - loss: 0.3303 - mean_absolute_error: 0.0077 - categorical_accuracy: 0.9052 - exact_match_accuracy: 0.0706\n",
      "Epoch 28/70\n",
      "20000/20000 [==============================] - 10s 519us/step - loss: 0.3126 - mean_absolute_error: 0.0075 - categorical_accuracy: 0.9118 - exact_match_accuracy: 0.09190s - loss: 0.3130 - mean_absolute_error: 0.0075 - categorical_accuracy: 0.9117 - exact_match_accur\n",
      "Epoch 29/70\n",
      "20000/20000 [==============================] - 10s 486us/step - loss: 0.2955 - mean_absolute_error: 0.0074 - categorical_accuracy: 0.9183 - exact_match_accuracy: 0.1216\n",
      "Epoch 30/70\n",
      "20000/20000 [==============================] - 10s 501us/step - loss: 0.2783 - mean_absolute_error: 0.0072 - categorical_accuracy: 0.9246 - exact_match_accuracy: 0.1497\n",
      "Epoch 31/70\n",
      "20000/20000 [==============================] - 9s 438us/step - loss: 0.2626 - mean_absolute_error: 0.0071 - categorical_accuracy: 0.9308 - exact_match_accuracy: 0.1852\n",
      "Epoch 32/70\n",
      "20000/20000 [==============================] - 8s 385us/step - loss: 0.2474 - mean_absolute_error: 0.0070 - categorical_accuracy: 0.9369 - exact_match_accuracy: 0.2275\n",
      "Epoch 33/70\n",
      "20000/20000 [==============================] - 8s 403us/step - loss: 0.2331 - mean_absolute_error: 0.0069 - categorical_accuracy: 0.9428 - exact_match_accuracy: 0.2731 4s - loss: 0.2345 - mean_\n",
      "Epoch 34/70\n",
      "20000/20000 [==============================] - 8s 397us/step - loss: 0.2189 - mean_absolute_error: 0.0068 - categorical_accuracy: 0.9490 - exact_match_accuracy: 0.3305 4s - loss: 0.2222 - mean_absolute_error:\n",
      "Epoch 35/70\n",
      "20000/20000 [==============================] - 8s 403us/step - loss: 0.2057 - mean_absolute_error: 0.0068 - categorical_accuracy: 0.9539 - exact_match_accuracy: 0.3851\n",
      "Epoch 36/70\n",
      "20000/20000 [==============================] - 8s 411us/step - loss: 0.1934 - mean_absolute_error: 0.0067 - categorical_accuracy: 0.9587 - exact_match_accuracy: 0.4386\n",
      "Epoch 37/70\n",
      "20000/20000 [==============================] - 8s 410us/step - loss: 0.1807 - mean_absolute_error: 0.0067 - categorical_accuracy: 0.9630 - exact_match_accuracy: 0.4894\n",
      "Epoch 38/70\n",
      "20000/20000 [==============================] - 8s 395us/step - loss: 0.1691 - mean_absolute_error: 0.0066 - categorical_accuracy: 0.9667 - exact_match_accuracy: 0.5332\n",
      "Epoch 39/70\n",
      "20000/20000 [==============================] - 8s 392us/step - loss: 0.1581 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9701 - exact_match_accuracy: 0.5776\n",
      "Epoch 40/70\n",
      "20000/20000 [==============================] - 8s 402us/step - loss: 0.1481 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9730 - exact_match_accuracy: 0.6179\n",
      "Epoch 41/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 8s 377us/step - loss: 0.1378 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9759 - exact_match_accuracy: 0.6580\n",
      "Epoch 42/70\n",
      "20000/20000 [==============================] - 8s 422us/step - loss: 0.1285 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9784 - exact_match_accuracy: 0.6906 0s - loss: 0.1296 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9782 - exact_matc\n",
      "Epoch 43/70\n",
      "20000/20000 [==============================] - 8s 419us/step - loss: 0.1197 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9806 - exact_match_accuracy: 0.7255 5s - loss: 0.12\n",
      "Epoch 44/70\n",
      "20000/20000 [==============================] - 9s 426us/step - loss: 0.1120 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9823 - exact_match_accuracy: 0.7489 7s - loss: 0.1133 - mean_absolute_error: 0.0064 - categorical_accuracy: 0 - ETA: 5s - loss: 0.1121 -\n",
      "Epoch 45/70\n",
      "20000/20000 [==============================] - 10s 504us/step - loss: 0.1042 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9842 - exact_match_accuracy: 0.7751\n",
      "Epoch 46/70\n",
      "20000/20000 [==============================] - 8s 392us/step - loss: 0.0974 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9855 - exact_match_accuracy: 0.7931\n",
      "Epoch 47/70\n",
      "20000/20000 [==============================] - 8s 404us/step - loss: 0.0908 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9868 - exact_match_accuracy: 0.8112\n",
      "Epoch 48/70\n",
      "20000/20000 [==============================] - 8s 378us/step - loss: 0.0849 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9879 - exact_match_accuracy: 0.8258 1s - loss: 0.0857 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9878 -\n",
      "Epoch 49/70\n",
      "20000/20000 [==============================] - 8s 389us/step - loss: 0.0794 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9890 - exact_match_accuracy: 0.8387\n",
      "Epoch 50/70\n",
      "20000/20000 [==============================] - 8s 408us/step - loss: 0.0742 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9899 - exact_match_accuracy: 0.8510 3s - loss: 0.0761 - mean_absolute_error: 0.0065 - \n",
      "Epoch 51/70\n",
      "20000/20000 [==============================] - 9s 430us/step - loss: 0.0692 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9907 - exact_match_accuracy: 0.8645\n",
      "Epoch 52/70\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 0.0652 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9913 - exact_match_accuracy: 0.8716\n",
      "Epoch 53/70\n",
      "20000/20000 [==============================] - 7s 373us/step - loss: 0.0606 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9920 - exact_match_accuracy: 0.8819\n",
      "Epoch 54/70\n",
      "20000/20000 [==============================] - 8s 380us/step - loss: 0.0568 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9927 - exact_match_accuracy: 0.8905\n",
      "Epoch 55/70\n",
      "20000/20000 [==============================] - 8s 377us/step - loss: 0.0533 - mean_absolute_error: 0.0066 - categorical_accuracy: 0.9931 - exact_match_accuracy: 0.8985\n",
      "Epoch 56/70\n",
      "20000/20000 [==============================] - 8s 384us/step - loss: 0.0498 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9937 - exact_match_accuracy: 0.9079\n",
      "Epoch 57/70\n",
      "20000/20000 [==============================] - 8s 383us/step - loss: 0.0469 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9941 - exact_match_accuracy: 0.9118\n",
      "Epoch 58/70\n",
      "20000/20000 [==============================] - 8s 375us/step - loss: 0.0446 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9943 - exact_match_accuracy: 0.9153\n",
      "Epoch 59/70\n",
      "20000/20000 [==============================] - 9s 429us/step - loss: 0.0413 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9948 - exact_match_accuracy: 0.9225\n",
      "Epoch 60/70\n",
      "20000/20000 [==============================] - 11s 543us/step - loss: 0.0388 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9951 - exact_match_accuracy: 0.9273\n",
      "Epoch 61/70\n",
      "20000/20000 [==============================] - 8s 388us/step - loss: 0.0364 - mean_absolute_error: 0.0065 - categorical_accuracy: 0.9956 - exact_match_accuracy: 0.9351 5s - loss: 0.03\n",
      "Epoch 62/70\n",
      "20000/20000 [==============================] - 8s 402us/step - loss: 0.0347 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9958 - exact_match_accuracy: 0.9370 5s - loss: 0.034\n",
      "Epoch 63/70\n",
      "20000/20000 [==============================] - 8s 411us/step - loss: 0.0326 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9961 - exact_match_accuracy: 0.9416 6\n",
      "Epoch 64/70\n",
      "20000/20000 [==============================] - 8s 397us/step - loss: 0.0304 - mean_absolute_error: 0.0063 - categorical_accuracy: 0.9965 - exact_match_accuracy: 0.9480\n",
      "Epoch 65/70\n",
      "20000/20000 [==============================] - 9s 449us/step - loss: 0.0288 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9966 - exact_match_accuracy: 0.9500\n",
      "Epoch 66/70\n",
      "20000/20000 [==============================] - 10s 498us/step - loss: 0.0272 - mean_absolute_error: 0.0063 - categorical_accuracy: 0.9969 - exact_match_accuracy: 0.9541\n",
      "Epoch 67/70\n",
      "20000/20000 [==============================] - 9s 446us/step - loss: 0.0258 - mean_absolute_error: 0.0064 - categorical_accuracy: 0.9970 - exact_match_accuracy: 0.9565\n",
      "Epoch 68/70\n",
      "20000/20000 [==============================] - 8s 393us/step - loss: 0.0242 - mean_absolute_error: 0.0062 - categorical_accuracy: 0.9973 - exact_match_accuracy: 0.9611\n",
      "Epoch 69/70\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 0.0230 - mean_absolute_error: 0.0063 - categorical_accuracy: 0.9974 - exact_match_accuracy: 0.9630\n",
      "Epoch 70/70\n",
      "20000/20000 [==============================] - 7s 362us/step - loss: 0.0221 - mean_absolute_error: 0.0063 - categorical_accuracy: 0.9975 - exact_match_accuracy: 0.9643\n",
      "0.  '  'AR91 AR91-31 FINAL FILTERED SDU02007TA 238981.SGY                              '  'AR91-31 '  'AR91-31 '  'True\n",
      "1.  '  'AR91 AR91-32 FINAL FILTERED SDU02007TA 238980.SGY                              '  'AR91-32 '  'AR91-32 '  'True\n",
      "2.  '  'WALLABELLA AND EXTENSION 84-W36 FINAL FILTERED MIGRATED SDU02007TA 238970.SGY  '  '84-W36  '  '84-W36  '  'True\n",
      "3.  '  'WOODDUCK 87-WD15 FINAL FILTERED SDU02007TA 238983.SGY                          '  '87-WD15 '  '87-WD15 '  'True\n",
      "4.  '  'AR91 AR91-31 FINAL FILTERED MIGRATED SDU02007TA 238977.SGY                     '  'AR91-31 '  'AR91-31 '  'True\n",
      "5.  '  'AR91 AR91-30 FINAL FILTERED SDU02007TA 238982.SGY                              '  'AR91-30 '  'AR91-30 '  'True\n",
      "6.  '  'LYNDON CAVES AND EXTENSION 84-L8 FINAL FILTERED SDU02007TA 238978.SGY          '  '84-L8   '  '84-L8   '  'True\n",
      "7.  '  'LYNDON CAVES AND EXTENSION 84-L8 FINAL FILTERED MIGRATED SDU02007TA 238975.SGY '  '84-L8   '  '84-L8   '  'True\n",
      "8.  '  'WALLABELLA AND EXTENSION 84-W32 FINAL FILTERED MIGRATED SDU02007TA 238971.SGY  '  '84-W32  '  '84-W32  '  'True\n",
      "9.  '  'LYNDON CAVES AND EXTENSION 84-L9 FINAL FILTERED MIGRATED SDU02007TA 238974.SGY '  '84-L9   '  '84-L9   '  'True\n",
      "10. '  'LYNDON CAVES AND EXTENSION 84-L61 FINAL FILTERED MIGRATED SDU02007TA 238972.SGY'  '84-L61  '  '84-L61  '  'True\n",
      "11. '  'NIELLA AND EXTENSION 85-N50 FINAL FILTERED SDU02007TA 238984.SGY               '  '85-N50  '  '85-N50  '  'True\n",
      "12. '  'EAST BALLYMENA NOMBY BRENTWOOD M97-EB03 FINAL MIGRATED SDU02062TA 238979.SGY   '  'M97-EB03'  'M97-EB03'  'True\n",
      "13. '  'LYNDON CAVES AND EXTENSION 84-L68 RAW MIGRATED SDU02007TA 238973.SGY           '  '84-L68  '  '84-L78  '  'False\n",
      "14. '  'NIELLA AND EXTENSION 85-N31 FINAL FILTERED SDU02007TA 238985.SGY               '  '85-N31  '  '85-N31  '  'True\n",
      "\n",
      "1000/1000 [==============================] - 1s 554us/step\n",
      "('categorical_crossentropy', 0.02258594837784767)\n",
      "('mean_absolute_error', 0.006008269745856523)\n",
      "('categorical_accuracy', 0.9956521716117859)\n",
      "(<function exact_match_accuracy at 0x000002634309C048>, 0.935)\n",
      "\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 70, 'loss': 0.02258594837784767, 'accuracy': 0.935}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 10, 'loss': 0.027491306215524675, 'accuracy': 0.907}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 0, 'loss': 0.037413076400756834, 'accuracy': 0.892}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 0, 'loss': 0.03741307596117258, 'accuracy': 0.892}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 60, 'loss': 0.037413076281547546, 'accuracy': 0.892}\n"
     ]
    }
   ],
   "source": [
    "epochs = 70\n",
    "batch_size = 64\n",
    "\n",
    "model = models['LSTM-RNN3']\n",
    "((x_train, x_test, x_preview), (y_train, y_test, y_preview)) = split_and_shuffle(x_onehot, y_onehot, sizes=(20000, 1000, 15))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "l, a = test(model, x_test, y_test, x_preview, y_preview)\n",
    "training_log(x_name, y_name, architecture, batch_size, epochs, l, a)\n",
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 16, 'epochs': 1, 'loss': 3.552898067474365, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 16, 'epochs': 1, 'loss': 3.563540615081787, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 16, 'epochs': 1, 'loss': 3.5646687889099122, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 16, 'epochs': 1, 'loss': 3.4647589626312256, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 128, 'epochs': 1, 'loss': 3.4894262046813966, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 0, 'loss': 3.9734037208557127, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
