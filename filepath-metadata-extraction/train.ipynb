{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Jupyter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'H:\\\\gsq-metadata-extraction\\\\filepath-metadata-extraction\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "import sys, inspect, argparse, importlib, traceback\n",
    "\n",
    "importlib.reload(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# percentage of samples that exactly match\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    argmax_true = tf.math.argmax(y_true, axis=2)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_pred = tf.math.argmax(y_pred, axis=2)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    match_char = tf.math.equal(argmax_true, argmax_pred)    # match characters              (batch, width:int) -> (batch, width:bool)\n",
    "    match_word = tf.math.reduce_all(match_char, axis=1)     # require all character in sample to match      (batch, width:bool) -> (batch:bool)\n",
    "    match_int = tf.cast(match_word, tf.float32)             # bool to int                                   (batch:bool) -> (batch:int)\n",
    "    return tf.reduce_mean(match_int)                        # percentage of samples that are an exact match (batch:int) -> int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def log(*l, **d): \n",
    "    if verbose: print(*l, **d)\n",
    "        \n",
    "training_history = []\n",
    "\n",
    "def training_log(x, y, a, b, e, l, m):\n",
    "    training_history.append({'x':x, 'y':y, 'architecture':a, 'batch size':b, 'epochs':e, 'loss':l, 'accuracy':m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tokens and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokens used to communicate non character entities\n",
    "# tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>', '<SurveyNum>', '<SurveyName>', '<LineName>', '<SurveyType>', '<PrimaryDataType>', '<SecondaryDataType>', '<TertiaryDataType>', '<Quaternary>', '<File_Range>', '<First_SP_CDP>', '<Last_SP_CDP>', '<CompletionYear>', '<TenureType>', '<Operator Name>', '<GSQBarcode>', '<EnergySource>', '<LookupDOSFilePath>', '<Source Of Data>']\n",
    "tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>']\n",
    "\n",
    "# get set of characters to be used, use static preset list of characters\n",
    "#available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890-_().,\\\\/\\\"':&\")\n",
    "available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-_().,\\\\/\\\"':&\")\n",
    "\n",
    "# generate character to int and int to character maps\n",
    "char_to_int = {c: i for i, c in enumerate(tokens + available_chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "char_count = len(char_to_int) # number of character available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Unique Record ID', 'FileName', 'Original_FileName', 'SurveyNum', 'SurveyName', 'LineName', 'SurveyType', 'PrimaryDataType', 'SecondaryDataType', 'TertiaryDataType', 'Quaternary', 'File_Range', 'First_SP_CDP', 'Last_SP_CDP', 'CompletionYear', 'TenureType', 'Operator Name', 'GSQBarcode', 'EnergySource', 'LookupDOSFilePath', 'Source Of Data', 'LookupDOSFilePath_Words', 'FileName_Words', 'LineName_Words'])\n"
     ]
    }
   ],
   "source": [
    "raw_source_file = 'SHUP 2D Files Training Data.csv'\n",
    "\n",
    "# read raw training data\n",
    "data_df = pd.read_csv(raw_source_file, dtype=str)\n",
    "data = {feature:data_df[feature].values for feature in data_df.columns.values}\n",
    "\n",
    "data['LookupDOSFilePath_Words'] = np.array([s.split('\\\\')[2:] for s in data['LookupDOSFilePath']])\n",
    "data['FileName_Words'] = np.array([s.split('_') for s in data['FileName']])\n",
    "data['LineName_Words'] = np.array([s.split('-') for s in data['LineName']])\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Vectorize Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data):\n",
    "    if type(data) == str:\n",
    "        return [char_to_int[char] for char in data.upper()]\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            return [vectorize_data(d) for d in data]\n",
    "        except Exception as e:\n",
    "            #traceback.print_exc()\n",
    "            #print(data, type(data))\n",
    "            return []\n",
    "\n",
    "\n",
    "def devectorise_data(data):\n",
    "        \n",
    "    length = data.shape[0]\n",
    "    \n",
    "    data = data.reshape(length, -1)\n",
    "    strings = np.full((length,), '', dtype=object)\n",
    "    \n",
    "    for i in range(length):\n",
    "        strings[i] = ''.join([int_to_char[int(i)] for i in data[i]])\n",
    "    \n",
    "    return strings\n",
    "    \n",
    "    \n",
    "#     ndim = data.ndim\n",
    "#     if data.dtype != object:\n",
    "#         data = data.astype(object)\n",
    "    \n",
    "#     # decode vector into string\n",
    "#     if ndim == 1:\n",
    "#         return ''.join([int_to_char[int(i)] for i in data])\n",
    "    \n",
    "#     # go to next level\n",
    "#     else:\n",
    "#         for i in range(len(data)):\n",
    "#             data[i] = devectorise_data(data[i])\n",
    "            \n",
    "#         return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Add Padding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find size of largest array across each dimension to computer shape of bounding ndarray\n",
    "def size(data):\n",
    "    \n",
    "    if type(data) == int:\n",
    "        return ()\n",
    "    \n",
    "    this_size = len(data)\n",
    "    \n",
    "    if this_size > 0:\n",
    "        inner_sizes = np.array([size(d) for d in data])\n",
    "        inner_sizes = tuple(np.amax(inner_sizes, axis=0))\n",
    "    else:\n",
    "        inner_sizes = ()\n",
    "    \n",
    "    return (this_size,) + inner_sizes\n",
    "    \n",
    "    \n",
    "def insert_vector(matrix, data, indices):\n",
    "    if type(data) == int:\n",
    "        matrix[indices] = data\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            insert(matrix, data[i], indices + (i,))\n",
    "    \n",
    "\n",
    "def pad_vector_data(data, pad_token, pad_shape=None):\n",
    "    \n",
    "    shape = size(data)\n",
    "    if pad_shape != None:\n",
    "        shape = tuple(np.maximum(pad_shape, shape))\n",
    "\n",
    "    # empty matrix\n",
    "    matrix = np.full(shape, pad_token, np.int32)\n",
    "\n",
    "    insert_vector(matrix, data, ())\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split(data, sizes):\n",
    "    sizes = list(sizes)\n",
    "    \n",
    "    for i in range(1, len(sizes)):\n",
    "        sizes[i] += sizes[i-1]\n",
    "    \n",
    "    slices = [slice(i,j) for i, j in zip([0]+sizes, sizes)]\n",
    "    \n",
    "    return [data[s] for s in slices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*data):\n",
    "    order = np.arange(len(data[0]))         # default order of elements\n",
    "    np.random.shuffle(order)                # randomise order\n",
    "    return [d[order] for d in data]         # new array with items in the randimised order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unique Record ID':       (23903, 6, 53)\n",
      "'FileName':               (23903, 87, 53)\n",
      "'Original_FileName':      (23903, 71, 53)\n",
      "'SurveyNum':              (23903, 5, 53)\n",
      "'SurveyName':             (23903, 39, 53)\n",
      "'LineName':               (23903, 23, 53)\n",
      "'SurveyType':             (23903, 6, 53)\n",
      "'PrimaryDataType':        (23903, 14, 53)\n",
      "'SecondaryDataType':      (23903, 36, 53)\n",
      "'TertiaryDataType':       (23903, 17, 53)\n",
      "'Quaternary':             (23903, 8, 53)\n",
      "'File_Range':             (23903, 13, 53)\n",
      "'First_SP_CDP':           (23903, 8, 53)\n",
      "'Last_SP_CDP':            (23903, 7, 53)\n",
      "'CompletionYear':         (23903, 4, 53)\n",
      "'TenureType':             (23903, 3, 53)\n",
      "'Operator Name':          (23903, 47, 53)\n",
      "'GSQBarcode':             (23903, 17, 53)\n",
      "'EnergySource':           (23903, 29, 53)\n",
      "'LookupDOSFilePath':      (23903, 181, 53)\n",
      "'Source Of Data':         (23903, 8, 53)\n",
      "'LookupDOSFilePath_Words': (23903, 6, 87, 53)\n",
      "'FileName_Words':         (23903, 13, 23, 53)\n",
      "'LineName_Words':         (23903, 4, 11, 53)\n"
     ]
    }
   ],
   "source": [
    "# extract LookupDOSFilePath for speccial processing\n",
    "\n",
    "vectorized_data = {f: vectorize_data(data[f])    for f in data}\n",
    "padded_data =     {f: pad_vector_data(vectorized_data[f], char_to_int['<Padding>'])    for f in vectorized_data}\n",
    "onehot_data =     {f: keras.utils.to_categorical(padded_data[f], char_count)    for f in padded_data}\n",
    "\n",
    "for f in onehot_data: print(f\"'{f}':\".ljust(25), onehot_data[f].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract(*keys, **cuts):\n",
    "    \n",
    "    onehots, shapes = [], []\n",
    "        \n",
    "    for key in keys:\n",
    "        \n",
    "        # get data from dictionary\n",
    "        onehot = onehot_data[key]\n",
    "\n",
    "        # apply cuts\n",
    "        cut = cuts.get(key, [[None]])\n",
    "        cut = [slice(*c) for c in cut]\n",
    "        cut = len(onehot.shape)*[slice(None)] + cut + [slice(None)]\n",
    "        cut = tuple(cut[-len(onehot.shape):])\n",
    "        onehot = onehot[cut]\n",
    "        \n",
    "        # calculate shape\n",
    "        shape = (None, *onehot.shape[1:])[-3:]\n",
    "\n",
    "        onehots.append(onehot)\n",
    "        shapes.append(shape)\n",
    "\n",
    "    return onehots, shapes\n",
    "    \n",
    "def split_and_shuffle(*onehots, sizes=None, shuffle_before=False, shuffle_after=True):\n",
    "    sizes = sizes or [None]\n",
    "    key_count = len(onehots)\n",
    "    subset_count = len(sizes)\n",
    "    \n",
    "    if shuffle_before:\n",
    "        onehots = shuffle(*onehots)\n",
    "\n",
    "    onehots_subsets = np.full((key_count, subset_count), None)\n",
    "    onehots_subsets[:,:] = [split(onehot, sizes) for onehot in onehots]\n",
    "\n",
    "    if shuffle_after:\n",
    "        for i in range(subset_count):\n",
    "            onehots_subsets[:,i] = shuffle(*onehots_subsets[:,i])\n",
    "\n",
    "    return onehots_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 2\n",
      "e[0] 1\n",
      "e[1] 1\n",
      "e[1][0] 3\n"
     ]
    }
   ],
   "source": [
    "e = extract('LookupDOSFilePath_Words')\n",
    "print('e', len(e))\n",
    "print('e[0]', len(e[0]))\n",
    "print('e[1]', len(e[1]))\n",
    "print('e[1][0]', len(e[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function: Test and show samlpe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, x_test=None, y_test=None, x_preview=None, y_preview=None):\n",
    "    \n",
    "    if (x_preview is not None) and (y_preview is not None):\n",
    "        p_one_hot = model.predict(x_preview)\n",
    "        p_vector = np.argmax(p_one_hot, 2)\n",
    "        p_strings = devectorise_data(p_vector)\n",
    "\n",
    "        y_vector = np.argmax(y_preview, 2)\n",
    "        y_strings = devectorise_data(y_vector)\n",
    "\n",
    "        x_vector = np.argmax(x_preview, 2)\n",
    "        x_strings = devectorise_data(x_vector)\n",
    "\n",
    "        x_strings = [s.replace('<Padding>', ' ').strip() for s in x_strings]\n",
    "        y_strings = [s.replace('<Padding>', ' ').strip() for s in y_strings]\n",
    "        p_strings = [s.replace('<Padding>', ' ').strip() for s in p_strings]\n",
    "        x_w, y_w, p_w = max([len(s) for s in x_strings]), max([len(s) for s in y_strings]), max([len(s) for s in p_strings])\n",
    "        y_p_strings = ['  '.join([x.ljust(x_w), y.ljust(y_w), p.ljust(p_w), str(y==p)]) for x, y, p in zip(x_strings, y_strings, p_strings)]\n",
    "\n",
    "        print(*y_p_strings, sep='\\n', end='\\n\\n')\n",
    "\n",
    "    if (x_test is not None) and (y_test is not None):\n",
    "        # accuracy on entire training set\n",
    "        accuracies = model.evaluate(x_test, y_test)\n",
    "        print(*list(zip([loss]+metrics, accuracies)), sep='\\n', end='\\n\\n') # evaluate and list loss and each metric\n",
    "\n",
    "        return accuracies[0], accuracies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15\n",
    "character_embedding_size = 20\n",
    "word_embedding_size = 200\n",
    "architecture = ''\n",
    "\n",
    "metrics = ['mean_absolute_error', 'categorical_accuracy', exact_match_accuracy] # binary_accuracy\n",
    "loss = 'categorical_crossentropy' # poisson mean_squared_logarithmic_error categorical_crossentropy\n",
    "\n",
    "embed_loss='categorical_crossentropy'\n",
    "embed_metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', exact_match_accuracy]\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Character Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto Encoder Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create offset input and output sequences to training a preditive embedding model.\n",
    "\n",
    "(x_char_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract('LookupDOSFilePath')\n",
    "(x_char_onehot,) = shuffle(x_char_onehot)\n",
    "x_embed_size = len(x_char_onehot)\n",
    "\n",
    "# create columns of padding tokens\n",
    "padding = np.array([[char_to_int['<Padding>']]] * x_embed_size)\n",
    "padding = keras.utils.to_categorical(padding, char_count)\n",
    "padding = padding.reshape(x_embed_size, 1, char_count)\n",
    "\n",
    "# 'abcd' -> ('_abcd', 'abcd_')\n",
    "x_embed_train = np.concatenate((x_char_onehot, padding), axis=1)\n",
    "y_embed_train = np.concatenate((padding, x_char_onehot), axis=1)\n",
    "x_embed_test = np.concatenate((x_char_onehot, padding), axis=1)\n",
    "y_embed_test = np.concatenate((padding, x_char_onehot), axis=1)\n",
    "\n",
    "embed_char_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Auto Encoder: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lh (Dense)                   (None, 182, 20)           1080      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 182, 53)           1113      \n",
      "=================================================================\n",
      "Total params: 2,193\n",
      "Trainable params: 2,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Character-Embedding'\n",
    "\n",
    "model_E_D_NN = keras.Sequential()\n",
    "model_E_D_NN.add(keras.layers.Dense(character_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_D_NN.add(keras.layers.Dense(char_count, activation='sigmoid', name='lo'))\n",
    "#model_E_D_NN.add(keras.layers.Dropout(0.001))\n",
    "model_E_D_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_D_NN\n",
    "print(model_E_D_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23903/23903 [==============================] - 53s 2ms/step - loss: 1.4288 - acc: 0.5250 - mean_absolute_error: 0.0366 - categorical_accuracy: 0.5250 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "23903/23903 [==============================] - 49s 2ms/step - loss: 1.3944 - acc: 0.5553 - mean_absolute_error: 0.0303 - categorical_accuracy: 0.5553 - exact_match_accuracy: 0.0000e+00\n",
      "\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GUM_CREEK_PHASE_1_3\\SEGY\\GUM_CREEK_PHASE_1_3_P85-426_RAW_STACK_QR020123_170500.SGY  \\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GUM_CREEK_PHASE_1_3\\SEGY\\GUM_CREEK_PHASE_1_3_P85-426_RAW_STACK_QR020123_170500.SGY  A_SSPA\\ED_SORSG_APOROS__SEDTAED_SPPROADETATA\\198AES_DOOSSCDPST_SD\\D3A_SEGAES_DOOSSCDPST_SD\\D3DP9044\\1DOT\\D_ATOCD_O8\\8\\\\3D\\180889_EG  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> y   predictive\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, y_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], y_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23903/23903 [==============================] - 51s 2ms/step - loss: 0.1141 - acc: 0.9647 - mean_absolute_error: 0.0174 - categorical_accuracy: 0.9647 - exact_match_accuracy: 0.8269\n",
      "Epoch 2/3\n",
      "23903/23903 [==============================] - 53s 2ms/step - loss: 0.0020 - acc: 1.0000 - mean_absolute_error: 0.0165 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9995\n",
      "Epoch 3/3\n",
      "23903/23903 [==============================] - 56s 2ms/step - loss: 6.7222e-04 - acc: 1.0000 - mean_absolute_error: 0.0164 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9995\n",
      "\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GUM_CREEK_PHASE_1_3\\SEGY\\GUM_CREEK_PHASE_1_3_P85-426_RAW_STACK_QR020123_170500.SGY  \\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GUM_CREEK_PHASE_1_3\\SEGY\\GUM_CREEK_PHASE_1_3_P85-426_RAW_STACK_QR020123_170500.SGY  \\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GUM_CREEK_PHASE_1_3\\SEGY\\GUM_CREEK_PHASE_1_3_P85-426_RAW_STACK_QR020123_170500.SGY  True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> x   direct\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, x_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], x_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# w1 = model.layers[0].get_weights()\n",
    "# w2 = model.layers[1].get_weights()\n",
    "\n",
    "# w = [np.copy(w1[0]), np.zeros(w1[1].shape)]\n",
    "# wi = [np.linalg.pinv(w1[0]), np.zeros(w2[1].shape)]\n",
    "\n",
    "# m = keras.Sequential()\n",
    "# m.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(embed_char_count, voc_size,)))\n",
    "# m.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "# m.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "# m.layers[0].set_weights(w)\n",
    "# m.layers[1].set_weights(wi)\n",
    "\n",
    "# encode_weights, decode_weights = w, wi\n",
    "\n",
    "# accuracy = model.evaluate(embed_test_x, embed_test_x)\n",
    "# metric_names = [embed_loss] + embed_metrics\n",
    "# dict(zip(metric_names, accuracy))\n",
    "char_encode_weights, char_decode_weights = models['Character-Embedding'].layers[0].get_weights(), models['Character-Embedding'].layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder Folder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 11, 53)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_word_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract('LineName_Words')\n",
    "(x_word_onehot,) = shuffle(x_word_onehot)\n",
    "embed_word_count, embed_char_count, embed_ones_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder Model: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 4, 11, 20)         1080      \n",
      "_________________________________________________________________\n",
      "reshape_44 (Reshape)         (None, 4, 220)            0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 4, 200)            44200     \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 4, 220)            44220     \n",
      "_________________________________________________________________\n",
      "reshape_45 (Reshape)         (None, 4, 11, 20)         0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 4, 11, 53)         1113      \n",
      "=================================================================\n",
      "Total params: 90,613\n",
      "Trainable params: 90,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Word-Embedding'\n",
    "\n",
    "model_E_W_NN = keras.Sequential()\n",
    "# character embedding\n",
    "model_E_W_NN.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=True, input_shape=(embed_word_count, embed_char_count, embed_ones_count)))\n",
    "model_E_W_NN.add(keras.layers.Reshape((embed_word_count, embed_char_count * character_embedding_size,)))\n",
    "\n",
    "# word auto encoder\n",
    "model_E_W_NN.add(keras.layers.Dense(word_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_W_NN.add(keras.layers.Dense(embed_char_count * character_embedding_size, activation='sigmoid', name='lo'))\n",
    "\n",
    "# character de embedding\n",
    "model_E_W_NN.add(keras.layers.Reshape((embed_word_count, embed_char_count, character_embedding_size)))\n",
    "model_E_W_NN.add(keras.layers.Dense(embed_ones_count, activation='sigmoid', trainable=True, name='char_decode')) \n",
    "\n",
    "model_E_W_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_W_NN\n",
    "print(model_E_W_NN.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_E_W_NN.layers[0].set_weights(char_encode_weights)\n",
    "model_E_W_NN.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23903/23903 [==============================] - 46s 2ms/step - loss: 0.0263 - acc: 0.9996 - mean_absolute_error: 0.0086 - categorical_accuracy: 0.9996 - exact_match_accuracy: 0.0653\n",
      "<EndOfString>                                       <Go>            <UnknownChar>         <Go>  <EndOfString>                                       <Go>            <UnknownChar>         <Go>  D<EndOfString><EndOfString><EndOfString><EndOfString><Go>   <Go><Go>   A<Go><Go> <Go>  <Go><Go><Go><Go> <Go><Go><Go><Go><Go>   <Go><Go><Go><Go> <Go><Go><Go>  <EndOfString><Go><Go> <Go><Go><EndOfString><Go><EndOfString> <Go><Go><Go><Go>  <Go><Go><Go><Go><EndOfString> <EndOfString> <Go><EndOfString><Go><Go><Go>   <Go><Go><Go><EndOfString> <Go> <Go> <Go><Go><Go>     <Go><Go> <Go><Go> <Go><Go><Go><Go><Go><Go><Go><UnknownChar><UnknownChar><UnknownChar>  F F  CBAA<Go>BAAF    A      <Go>  A <UnknownChar>C    E  <UnknownChar> <UnknownChar><Go><UnknownChar><UnknownChar><UnknownChar> <UnknownChar><UnknownChar><UnknownChar><UnknownChar>  F F  CBAA<Go>BAAF    A      <Go>  A <UnknownChar>C    E  <UnknownChar> <UnknownChar><Go><UnknownChar><UnknownChar><UnknownChar> <UnknownChar>  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> x   direct\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "models['Word-Embedding'].fit(x_word_onehot, x_word_onehot, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Word-Embedding'], None, None, x_word_onehot[:1], x_word_onehot[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### P-NN: Input, Embedding, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-984d49fe0004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'P-NN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_P_NN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                       \u001b[1;31m# flatten to 1D per sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'P-NN'\n",
    "\n",
    "model_P_NN = keras.Sequential()\n",
    "model_P_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_P_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_P_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_P_NN.add(keras.layers.Dropout(0.001))                                                                  # dropout to prevent overfitting\n",
    "model_P_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_P_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_P_NN\n",
    "print(model_P_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FF-NN: Input, Embedding, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 460)               0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1264)              582704    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 948)               1199220   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 1,783,504\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'FF-NN'\n",
    "hidden_size = (y_shape_ones*embedding_size + y_shape_char*y_shape_ones) // 2\n",
    "\n",
    "model_FF_NN = keras.Sequential()\n",
    "model_FF_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_FF_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_FF_NN.add(keras.layers.Dense(hidden_size, activation='exponential', name='lh'))                         # dense layer\n",
    "model_FF_NN.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_FF_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_FF_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_FF_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_FF_NN\n",
    "print(model_FF_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN1: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 948)               3674448   \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 3,676,028\n",
      "Trainable params: 3,676,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN1'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN1 = keras.Sequential()\n",
    "model_LSTM_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_LSTM_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN1.add(keras.activation.exponential())\n",
    "model_LSTM_RNN1.add(keras.layers.LSTM(y_shape_char * y_shape_ones, activation='exponential', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN1\n",
    "print(model_LSTM_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-RNN2: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 6, 20)             1080      \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 240)               250560    \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               28920     \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 6, 53)             1113      \n",
      "=================================================================\n",
      "Total params: 281,673\n",
      "Trainable params: 279,480\n",
      "Non-trainable params: 2,193\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN2'\n",
    "\n",
    "x_name, y_name = 'LineName', 'LineName'\n",
    "cuts = {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = y_char_count * character_embedding_size * 2\n",
    "\n",
    "model_LSTM_RNN2 = keras.Sequential()\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_char_count, x_ones_count)))                 # embed characters into dense embedded space\n",
    "#model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.LSTM(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='lstm1'))     # lstm recurrent cell\n",
    "#model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_LSTM_RNN2.add(keras.layers.Reshape((y_char_count, character_embedding_size)))                                        # un flatten\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_LSTM_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN2\n",
    "print(model_LSTM_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_LSTM_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_LSTM_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN1: Input, Embedding, (GRU), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "architecture = 'GRU-RNN1'\n",
    "lstm_hidden_size = voc_size * 15\n",
    "\n",
    "model_GRU_RNN1 = keras.Sequential()\n",
    "model_GRU_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_GRU_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_GRU_RNN1.add(keras.layers.GRU(y_shape_char * y_shape_ones, activation='relu', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_GRU_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_GRU_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN1\n",
    "print(model_GRU_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN2: Imput Embedding, (GRU), Decoder, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 12, 10)            540       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, 160)               82080     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               19320     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 12, 53)            583       \n",
      "=================================================================\n",
      "Total params: 102,523\n",
      "Trainable params: 101,400\n",
      "Non-trainable params: 1,123\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN2'\n",
    "\n",
    "x_name, y_name = 'LineName', 'LineName'\n",
    "cuts = {x_name:[[None, 12]] , y_name:[[None, 12]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = y_char_count * character_embedding_size + 40\n",
    "\n",
    "model_GRU_RNN2 = keras.Sequential()\n",
    "model_GRU_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_char_count, x_ones_count)))                 # embed characters into dense embedded space\n",
    "model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.GRU(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='gru1'))     # gru recurrent cell\n",
    "model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.add(keras.layers.Reshape((y_char_count, character_embedding_size)))                                        # un flatten\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN2\n",
    "print(model_GRU_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_GRU_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_GRU_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save/Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DE = models['E-D-NN'].get_weights()\n",
    "#model_GRU_1 = model\n",
    "#model_GRU_2 = model\n",
    "#model_GRU_3 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.set_weights(GRU)\n",
    "#model = model_GRU_3\n",
    "#models['E-D-NN'].set_weights(DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85-N50  85-N50  80---0  False\n",
      "M97-EB  M97-EB  P84-E-  False\n",
      "84-L8   84-L8   84-08   False\n",
      "AR91-3  AR91-3  AR80-3  False\n",
      "84-W32  84-W32  84-R42  False\n",
      "85-N31  85-N31  80-N31  False\n",
      "AR91-3  AR91-3  AR80-3  False\n",
      "84-L9   84-L9   84-0E   False\n",
      "AR91-3  AR91-3  AR80-3  False\n",
      "84-L68  84-L68  84-008  False\n",
      "84-L61  84-L61  84-001  False\n",
      "AR91-3  AR91-3  AR80-3  False\n",
      "87-WD1  87-WD1  80-B-1  False\n",
      "84-W36  84-W36  84-R36  False\n",
      "84-L8   84-L8   84-08   False\n",
      "\n",
      "1000/1000 [==============================] - 1s 556us/step\n",
      "('categorical_crossentropy', 1.5881596450805664)\n",
      "('mean_absolute_error', 0.046552691459655765)\n",
      "('categorical_accuracy', 0.6251666712760925)\n",
      "(<function exact_match_accuracy at 0x0000013216711D90>, 0.003)\n",
      "\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 0, 'loss': 1.5881596450805664, 'accuracy': 0.003}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 0, 'loss': 1.5881596336364745, 'accuracy': 0.003}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.588159631729126, 'accuracy': 0.003}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.689903561592102, 'accuracy': 0.01}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.792127700805664, 'accuracy': 0.009}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.7085705099105835, 'accuracy': 0.01}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 5, 'loss': 1.3410148220062257, 'accuracy': 0.227}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.4720412578582764, 'accuracy': 0.085}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.7099846067428588, 'accuracy': 0.007}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 2.6454844398498536, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 5, 'loss': 1.7647799472808838, 'accuracy': 0.168}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 1.8834026641845703, 'accuracy': 0.065}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 5, 'loss': 2.126100830078125, 'accuracy': 0.004}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 64, 'epochs': 1, 'loss': 1.748256908416748, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 8, 'loss': 0.006235820516943932, 'accuracy': 0.002}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 15, 'loss': 0.006268837265670299, 'accuracy': 0.002}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 15, 'loss': 0.006549110695719719, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 5, 'loss': 0.007222863543778658, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 5, 'loss': 0.007835018768906594, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.007255152925848961, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.00679586298763752, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.006820704355835915, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.006912377312779426, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 5, 'loss': 0.007257403738796711, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 32, 'epochs': 5, 'loss': 0.14055853486061096, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 0, 'loss': 0.009530285626649856, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.00958696822822094, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'LSTM-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.009612252667546272, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.00953885905444622, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 16, 'epochs': 0, 'loss': 0.00939811820536852, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 16, 'epochs': 0, 'loss': 0.00939811807870865, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 16, 'epochs': 5, 'loss': 0.009441895350813866, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 16, 'epochs': 1, 'loss': 0.009532618835568428, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 16, 'epochs': 1, 'loss': 0.009603651843965054, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 32, 'epochs': 1, 'loss': 0.00963422055542469, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 64, 'epochs': 1, 'loss': 0.009616718471050263, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 5, 'loss': 0.009629399374127387, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.009903429888188839, 'accuracy': 0.0}\n",
      "{'x': 'LineName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.023622165217995643, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.03426160505414009, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "batch_size = 64\n",
    "\n",
    "model = models['LSTM-RNN2']\n",
    "((x_train, x_test, x_preview), (y_train, y_test, y_preview)) = split_and_shuffle(x_onehot, y_onehot, sizes=(20000, 1000, 15))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "l, a = test(model, x_test, y_test, x_preview, y_preview)\n",
    "training_log(x_name, y_name, architecture, batch_size, epochs, l, a)\n",
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.03426160505414009, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
