{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Jupyter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'H:\\\\gsq-metadata-extraction\\\\filepath-metadata-extraction\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "import sys, inspect, argparse, importlib, traceback\n",
    "\n",
    "importlib.reload(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# percentage of samples that exactly match\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    argmax_true = tf.math.argmax(y_true, axis=2)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_pred = tf.math.argmax(y_pred, axis=2)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    match_char = tf.math.equal(argmax_true, argmax_pred)    # match characters              (batch, width:int) -> (batch, width:bool)\n",
    "    match_word = tf.math.reduce_all(match_char, axis=1)     # require all character in sample to match      (batch, width:bool) -> (batch:bool)\n",
    "    match_int = tf.cast(match_word, tf.float32)             # bool to int                                   (batch:bool) -> (batch:int)\n",
    "    return tf.reduce_mean(match_int)                        # percentage of samples that are an exact match (batch:int) -> int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def log(*l, **d): \n",
    "    if verbose: print(*l, **d)\n",
    "        \n",
    "training_history = []\n",
    "\n",
    "def training_log(x, y, a, b, e, l, m):\n",
    "    training_history.append({'x':x, 'y':y, 'architecture':a, 'batch size':b, 'epochs':e, 'loss':l, 'accuracy':m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pp.load('training_data.p')\n",
    "voc_size = pp.char_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tokens and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokens used to communicate non character entities\n",
    "# tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>', '<SurveyNum>', '<SurveyName>', '<LineName>', '<SurveyType>', '<PrimaryDataType>', '<SecondaryDataType>', '<TertiaryDataType>', '<Quaternary>', '<File_Range>', '<First_SP_CDP>', '<Last_SP_CDP>', '<CompletionYear>', '<TenureType>', '<Operator Name>', '<GSQBarcode>', '<EnergySource>', '<LookupDOSFilePath>', '<Source Of Data>']\n",
    "tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>']\n",
    "\n",
    "# get set of characters to be used, use static preset list of characters\n",
    "#available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890-_().,\\\\/\\\"':&\")\n",
    "available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-_().,\\\\/\\\"':&\")\n",
    "\n",
    "# generate character to int and int to character maps\n",
    "char_to_int = {c: i for i, c in enumerate(tokens + available_chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "char_count = len(char_to_int) # number of character available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Unique Record ID', 'FileName', 'Original_FileName', 'SurveyNum', 'SurveyName', 'LineName', 'SurveyType', 'PrimaryDataType', 'SecondaryDataType', 'TertiaryDataType', 'Quaternary', 'File_Range', 'First_SP_CDP', 'Last_SP_CDP', 'CompletionYear', 'TenureType', 'Operator Name', 'GSQBarcode', 'EnergySource', 'LookupDOSFilePath', 'Source Of Data', 'LookupDOSFilePath_Words', 'FileName_Words'])\n"
     ]
    }
   ],
   "source": [
    "raw_source_file = 'SHUP 2D Files Training Data.csv'\n",
    "\n",
    "# read raw training data\n",
    "data_df = pd.read_csv(raw_source_file, dtype=str)\n",
    "data = {feature:data_df[feature].values for feature in data_df.columns.values}\n",
    "\n",
    "data['LookupDOSFilePath_Words'] = np.array([s.split('\\\\')[2:] for s in data['LookupDOSFilePath']])\n",
    "data['FileName_Words'] = np.array([s.split('_') for s in data['FileName']])\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vectorize Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data):\n",
    "    if type(data) == str:\n",
    "        return [char_to_int[char] for char in data.upper()]\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            return [vectorize_data(d) for d in data]\n",
    "        except Exception as e:\n",
    "            #traceback.print_exc()\n",
    "            #print(data, type(data))\n",
    "            return []\n",
    "\n",
    "def decode_vector(vector):\n",
    "    return ''.join([int_to_char[int(i)] for i in vector])\n",
    "\n",
    "def decode_data(data):\n",
    "    return [decode(v) for v in data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Add Padding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find size of largest array across each dimension to computer shape of bounding ndarray\n",
    "def size(data):\n",
    "    \n",
    "    if type(data) == int:\n",
    "        return ()\n",
    "    \n",
    "    this_size = len(data)\n",
    "    \n",
    "    if this_size > 0:\n",
    "        inner_sizes = np.array([size(d) for d in data])\n",
    "        inner_sizes = tuple(np.amax(inner_sizes, axis=0))\n",
    "    else:\n",
    "        inner_sizes = ()\n",
    "    \n",
    "    return (this_size,) + inner_sizes\n",
    "    \n",
    "    \n",
    "def insert(matrix, data, indices):\n",
    "    if type(data) == int:\n",
    "        matrix[indices] = data\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            insert(matrix, data[i], indices + (i,))\n",
    "    \n",
    "\n",
    "def pad_vector_data(data, pad_token, pad_shape=None):\n",
    "    \n",
    "    shape = size(data)\n",
    "    if pad_shape != None:\n",
    "        shape = tuple(np.maximum(pad_shape, shape))\n",
    "\n",
    "    # empty matrix\n",
    "    matrix = np.full(shape, pad_token, np.int32)\n",
    "\n",
    "    insert(matrix, data, ())\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split(data, *sizes):\n",
    "    sizes = list(sizes)\n",
    "    \n",
    "    for i in range(1, len(sizes)):\n",
    "        sizes[i] += sizes[i-1]\n",
    "    \n",
    "    slices = [slice(i,j) for i, j in zip([0]+sizes, sizes)]\n",
    "    \n",
    "    return [data[s] for s in slices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*data):\n",
    "    order = np.arange(len(data[0]))         # default order of elements\n",
    "    np.random.shuffle(order)                # randomise order\n",
    "    return [d[order] for d in data]         # new array with items in the randimised order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unique Record ID': (23903, 6, 53),\n",
       " 'FileName': (23903, 87, 53),\n",
       " 'Original_FileName': (23903, 71, 53),\n",
       " 'SurveyNum': (23903, 5, 53),\n",
       " 'SurveyName': (23903, 39, 53),\n",
       " 'LineName': (23903, 23, 53),\n",
       " 'SurveyType': (23903, 6, 53),\n",
       " 'PrimaryDataType': (23903, 14, 53),\n",
       " 'SecondaryDataType': (23903, 36, 53),\n",
       " 'TertiaryDataType': (23903, 17, 53),\n",
       " 'Quaternary': (23903, 8, 53),\n",
       " 'File_Range': (23903, 13, 53),\n",
       " 'First_SP_CDP': (23903, 8, 53),\n",
       " 'Last_SP_CDP': (23903, 7, 53),\n",
       " 'CompletionYear': (23903, 4, 53),\n",
       " 'TenureType': (23903, 3, 53),\n",
       " 'Operator Name': (23903, 47, 53),\n",
       " 'GSQBarcode': (23903, 17, 53),\n",
       " 'EnergySource': (23903, 29, 53),\n",
       " 'LookupDOSFilePath': (23903, 181, 53),\n",
       " 'Source Of Data': (23903, 8, 53),\n",
       " 'LookupDOSFilePath_Words': (23903, 6, 87, 53),\n",
       " 'FileName_Words': (23903, 13, 23, 53)}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract LookupDOSFilePath for speccial processing\n",
    "\n",
    "vectorized_data = {f: vectorize_data(data[f])    for f in data}\n",
    "padded_data =     {f: pad_vector_data(vectorized_data[f], char_to_int['<Padding>'])    for f in vectorized_data}\n",
    "onehot_data =     {f: keras.utils.to_categorical(padded_data[f], char_count)    for f in padded_data}\n",
    "{f:onehot_data[f].shape for f in onehot_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_count_words': None,\n",
       " 'x_count_chars': 87,\n",
       " 'x_count_ones': 53,\n",
       " 'y_count_words': None,\n",
       " 'y_count_chars': 23,\n",
       " 'y_count_ones': 53}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cut = slice(None, None)\n",
    "y_cut = slice(None, None)\n",
    "\n",
    "train_size = 5000\n",
    "test_size = 1000\n",
    "preview_size = 15\n",
    "\n",
    "x_onehot = onehot_data['FileName']\n",
    "y_onehot = onehot_data['LineName']\n",
    "\n",
    "# x_count_words, x_count_chars, x_count_ones, *_ = (x_onehot.shape[1], x_onehot.shape[2], x_onehot.shape[3])\n",
    "# y_count_words, y_count_chars, y_count_ones, *_ = (y_onehot.shape[1], y_onehot.shape[2], y_onehot.shape[3])\n",
    "x_count_words, x_count_chars, x_count_ones, *_ = (None, x_onehot.shape[1], x_onehot.shape[2])\n",
    "y_count_words, y_count_chars, y_count_ones, *_ = (None, y_onehot.shape[1], y_onehot.shape[2])\n",
    "\n",
    "#x_onehot, y_onehot = shuffle(x_onehot, y_onehot)\n",
    "\n",
    "x_train, x_test, x_preview = split(x_onehot, train_size, test_size, preview_size)\n",
    "y_train, y_test, y_preview = split(y_onehot, train_size, test_size, preview_size)\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "{\n",
    "'x_count_words': x_count_words, 'x_count_chars': x_count_chars, 'x_count_ones': x_count_ones, \n",
    "'y_count_words': y_count_words, 'y_count_chars': y_count_chars, 'y_count_ones': y_count_ones, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test and show samlpe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test, x_preview, y_preview):\n",
    "    p_one_hot = model.predict(x_preview)\n",
    "    p_vector = np.argmax(p_one_hot, 2)\n",
    "    p_strings = decode_data(p_vector)\n",
    "\n",
    "    y_vector = np.argmax(y_preview, 2)\n",
    "    y_strings = decode_data(y_vector)\n",
    "\n",
    "    x_vector = np.argmax(x_preview, 2)\n",
    "    x_strings = decode_data(x_vector)\n",
    "\n",
    "    x_strings = [s.replace('<Padding>', ' ').strip() for s in x_strings]\n",
    "    y_strings = [s.replace('<Padding>', ' ').strip() for s in y_strings]\n",
    "    p_strings = [s.replace('<Padding>', ' ').strip() for s in p_strings]\n",
    "    x_w, y_w, p_w = max([len(s) for s in x_strings]), max([len(s) for s in y_strings]), max([len(s) for s in p_strings])\n",
    "    y_p_strings = ['  '.join([x.ljust(x_w), y.ljust(y_w), p.ljust(p_w), str(y==p)]) for x, y, p in zip(x_strings, y_strings, p_strings)]\n",
    "\n",
    "    print(*y_p_strings, sep='\\n', end='\\n\\n')\n",
    "\n",
    "    # accuracy on entire training set\n",
    "    accuracies = model.evaluate(x_test, y_test)\n",
    "    print(*list(zip([loss]+metrics, accuracies)), sep='\\n', end='\\n\\n') # evaluate and list loss and each metric\n",
    "    \n",
    "    return accuracies[0], accuracies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15\n",
    "character_embedding_size = 10\n",
    "architecture = ''\n",
    "\n",
    "metrics = ['mean_absolute_error', 'categorical_accuracy', 'binary_accuracy', exact_match_accuracy]\n",
    "loss = 'mean_squared_logarithmic_error' # poisson mean_squared_logarithmic_error categorical_crossentropy\n",
    "\n",
    "embed_loss='categorical_crossentropy'\n",
    "embed_metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', 'binary_accuracy']\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create offset input and output sequences to training a preditive embedding model.\n",
    "\n",
    "# create columns of padding tokens\n",
    "padding_train = np.array([[char_to_int['<Padding>']]] * train_size)\n",
    "padding_train = keras.utils.to_categorical(padding_train, char_count)\n",
    "padding_train = padding_train.reshape(train_size, 1, char_count)\n",
    "\n",
    "padding_test = np.array([[char_to_int['<Padding>']]] * test_size)\n",
    "padding_test = keras.utils.to_categorical(padding_test, char_count)\n",
    "padding_test = padding_test.reshape(test_size, 1, char_count)\n",
    "\n",
    "# 'abcd' -> ('_abcd', 'abcd_')\n",
    "embed_train_x = np.concatenate((padding_train, x_train), axis=1)\n",
    "embed_train_y = np.concatenate((x_train, padding_train), axis=1)\n",
    "embed_test_x = np.concatenate((padding_test, x_test), axis=1)\n",
    "embed_test_y = np.concatenate((x_test, padding_test), axis=1)\n",
    "\n",
    "embed_char_count = embed_train_x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lh (Dense)                   (None, 88, 10)            540       \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 88, 53)            583       \n",
      "=================================================================\n",
      "Total params: 1,123\n",
      "Trainable params: 1,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Character-Embedding'\n",
    "\n",
    "model_E_D_NN = keras.Sequential()\n",
    "model_E_D_NN.add(keras.layers.Dense(character_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_D_NN.add(keras.layers.Dense(char_count, activation='sigmoid', name='lo'))\n",
    "#model_E_D_NN.add(keras.layers.Dropout(0.001))\n",
    "model_E_D_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_D_NN\n",
    "print(model_E_D_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 1s 192us/step - loss: 1.5529 - acc: 0.3988 - mean_absolute_error: 0.0592 - categorical_accuracy: 0.3988 - binary_accuracy: 0.9787\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 1s 195us/step - loss: 1.2991 - acc: 0.3988 - mean_absolute_error: 0.0484 - categorical_accuracy: 0.3988 - binary_accuracy: 0.9816\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 1s 188us/step - loss: 1.1087 - acc: 0.3988 - mean_absolute_error: 0.0403 - categorical_accuracy: 0.3988 - binary_accuracy: 0.9841\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 1s 191us/step - loss: 0.9554 - acc: 0.4210 - mean_absolute_error: 0.0338 - categorical_accuracy: 0.4210 - binary_accuracy: 0.9851\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 1s 220us/step - loss: 0.7994 - acc: 0.6934 - mean_absolute_error: 0.0282 - categorical_accuracy: 0.6934 - binary_accuracy: 0.9878\n",
      "1000/1000 [==============================] - 0s 98us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'categorical_crossentropy': 0.7513767862319947,\n",
       " 'accuracy': 0.90248863363266,\n",
       " 'mean_absolute_error': 0.0266893962174654,\n",
       " 'categorical_accuracy': 0.90248863363266,\n",
       " 'binary_accuracy': 0.9934578099250794}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "model = models['Character-Embedding']\n",
    "\n",
    "model.fit(embed_train_x, embed_train_x, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "accuracy = model.evaluate(embed_test_x, embed_test_x)\n",
    "metric_names = [embed_loss] + embed_metrics\n",
    "dict(zip(metric_names, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = model.layers[0].get_weights()\n",
    "# w2 = model.layers[1].get_weights()\n",
    "\n",
    "# w = [np.copy(w1[0]), np.zeros(w1[1].shape)]\n",
    "# wi = [np.linalg.pinv(w1[0]), np.zeros(w2[1].shape)]\n",
    "\n",
    "# m = keras.Sequential()\n",
    "# m.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(embed_char_count, voc_size,)))\n",
    "# m.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "# m.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "# m.layers[0].set_weights(w)\n",
    "# m.layers[1].set_weights(wi)\n",
    "\n",
    "# encode_weights, decode_weights = w, wi\n",
    "\n",
    "# accuracy = model.evaluate(embed_test_x, embed_test_x)\n",
    "# metric_names = [embed_loss] + embed_metrics\n",
    "# dict(zip(metric_names, accuracy))\n",
    "char_encode_weights, char_decode_weights = models['Character-Embedding'].layers[0].get_weights(), models['Character-Embedding'].layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder Folder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text by '\\' to seperate into 'words'\n",
    "# resulting data shape (dataset_size, max_words_per_sample, max_chars_per_word, categorical_onehot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder Model: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### P-NN: Input, Embedding, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-984d49fe0004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'P-NN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_P_NN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                       \u001b[1;31m# flatten to 1D per sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'P-NN'\n",
    "\n",
    "model_P_NN = keras.Sequential()\n",
    "model_P_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_P_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_P_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_P_NN.add(keras.layers.Dropout(0.001))                                                                  # dropout to prevent overfitting\n",
    "model_P_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_P_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_P_NN\n",
    "print(model_P_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FF-NN: Input, Embedding, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 460)               0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1264)              582704    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 948)               1199220   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 1,783,504\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'FF-NN'\n",
    "hidden_size = (y_shape_ones*embedding_size + y_shape_char*y_shape_ones) // 2\n",
    "\n",
    "model_FF_NN = keras.Sequential()\n",
    "model_FF_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_FF_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_FF_NN.add(keras.layers.Dense(hidden_size, activation='exponential', name='lh'))                         # dense layer\n",
    "model_FF_NN.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_FF_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_FF_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_FF_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_FF_NN\n",
    "print(model_FF_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN1: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 948)               3674448   \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 3,676,028\n",
      "Trainable params: 3,676,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN1'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN1 = keras.Sequential()\n",
    "model_LSTM_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_LSTM_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN1.add(keras.activation.exponential())\n",
    "model_LSTM_RNN1.add(keras.layers.LSTM(y_shape_char * y_shape_ones, activation='exponential', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN1\n",
    "print(model_LSTM_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN2: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b75df2613e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                    \u001b[1;31m# dropout to prevent overfitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# lstm recurrent cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_char\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my_shape_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                              \u001b[1;31m# dense combine time series into single output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_shape_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                           \u001b[1;31m# un flatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 raise TypeError('All layers in a Sequential model '\n\u001b[0m\u001b[0;32m    184\u001b[0m                                 \u001b[1;34m'should have a single output tensor. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                                 \u001b[1;34m'For multi-output layers, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API."
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN2'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN2 = keras.Sequential()\n",
    "model_LSTM_RNN2.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.LSTM(lstm_hidden_size, return_sequences=True, return_state=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_shape_char * y_shape_ones))                                              # dense combine time series into single output\n",
    "model_LSTM_RNN2.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN2\n",
    "print(model_LSTM_RNN2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN1: Input, Embedding, (GRU), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 948)               2755836   \n",
      "_________________________________________________________________\n",
      "reshape_24 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 2,757,416\n",
      "Trainable params: 2,757,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN1'\n",
    "lstm_hidden_size = voc_size * 15\n",
    "\n",
    "model_GRU_RNN1 = keras.Sequential()\n",
    "model_GRU_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_GRU_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_GRU_RNN1.add(keras.layers.GRU(y_shape_char * y_shape_ones, activation='relu', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_GRU_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_GRU_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN1\n",
    "print(model_GRU_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU-RNN2: Imput Embedding, (GRU), Decoder, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 87, 10)            800       \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, 160)               82080     \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               19320     \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 12, 79)            869       \n",
      "=================================================================\n",
      "Total params: 103,069\n",
      "Trainable params: 101,400\n",
      "Non-trainable params: 1,669\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN2'\n",
    "lstm_hidden_size = y_shape_char * character_embedding_size + 40\n",
    "\n",
    "model_GRU_RNN2 = keras.Sequential()\n",
    "model_GRU_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_shape_char, x_shape_ones)))                 # embed characters into dense embedded space\n",
    "#model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.GRU(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='gru1'))     # gru recurrent cell\n",
    "#model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_shape_char * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.add(keras.layers.Reshape((y_shape_char, character_embedding_size)))                                        # un flatten\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_shape_ones, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN2\n",
    "print(model_GRU_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "# model_GRU_RNN2.layers[0].set_weights( models['Character-Embedding'].layers[0].get_weights())\n",
    "# model_GRU_RNN2.layers[-1].set_weights(models['Character-Embedding'].layers[-1].get_weights())\n",
    "model_GRU_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_GRU_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save/Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DE = models['E-D-NN'].get_weights()\n",
    "#model_GRU_1 = model\n",
    "#model_GRU_2 = model\n",
    "#model_GRU_3 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.set_weights(GRU)\n",
    "#model = model_GRU_3\n",
    "#models['E-D-NN'].set_weights(DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 233s 58ms/step - loss: 0.0820 - mean_absolute_error: 0.2712 - categorical_accuracy: 0.0070 - binary_accuracy: 0.7966 - exact_match_accuracy: 0.0000e+00\n",
      "XYLEM_XM86-7_STACK_SDU10912TA_130263.sgy             XM86-7   LLLLLLLLLLLL  False\n",
      "METEOR_79-D20A_FINAL_MIGRATED_SDU10912TA_129215.sgy  79-D20A  LLLLLLLLLLLL  False\n",
      "YOOTHAPINNA_83-E307_STACK_SDU10912TA_129872.sgy      83-E307  LLLLLLLLLLLL  False\n",
      "PIEBALD_MARMADUA_80-H14_STACK_SDU10912TA_129292.sgy  80-H14   LLLLLLLLLLLL  False\n",
      "XYLEM_XM86-4_STACK_SDU10912TA_130260.sgy             XM86-4   LLLLLLLLLLLL  False\n",
      "\n",
      "1000/1000 [==============================] - 37s 37ms/step\n",
      "('mean_squared_logarithmic_error', 0.016132571235299112)\n",
      "('mean_absolute_error', 0.09493823343515397)\n",
      "('categorical_accuracy', 0.012416666604578495)\n",
      "('binary_accuracy', 0.9749974575042725)\n",
      "(<function exact_match_accuracy at 0x0000026AC9ECCC80>, 0.0)\n",
      "\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.016132571235299112, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 0, 'loss': 0.013096654660999775, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 512, 'epochs': 1, 'loss': 0.02188005402684212, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 512, 'epochs': 1, 'loss': 0.02344049382209778, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'Character-Embedding', 'batch size': 512, 'epochs': 1, 'loss': 0.006153970964252949, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.16289387238025665, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.004683957852423191, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.004685713063925505, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.0038579754158854485, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.005442343842238188, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.005693509742617607, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.009779514268040657, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "model = models['GRU-RNN2']\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs)\n",
    "l, a = test(model, test_x, test_y, showcase_x, showcase_y)\n",
    "training_log(x_name, y_name, architecture, batch_size, epochs, l, a)\n",
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.004683957852423191, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.004685713063925505, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.0038579754158854485, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 10, 'loss': 0.005442343842238188, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.005693509742617607, 'accuracy': 0.0}\n",
      "{'x': 'FileName', 'y': 'LineName', 'architecture': 'GRU-RNN2', 'batch size': 128, 'epochs': 1, 'loss': 0.009779514268040657, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
