{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Jupyter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# expand cell widths to 100% for better output viewing\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, inspect, argparse, importlib, traceback, re \n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    \"\"\"Custom accuracy function, Measure prediction accuracy against the ground truth.\n",
    "    A sample'accuracy is 1 if all characters exactly match the ground truth and 0 otherwise.\n",
    "    \n",
    "    # Arguments\n",
    "        y_true: A Tensor holding the true y values for the batch\n",
    "        y_pred: A Tensor holding the predicted y values for the batch\n",
    "        \n",
    "    # Returns\n",
    "        A Tensor of rank 0 containing the % of correctly predicted batches\n",
    "    \"\"\"\n",
    "    \n",
    "    # onehot to index     (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_true = tf.math.argmax(y_true, axis=-1)\n",
    "    \n",
    "    # onehot to index    (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    # match characters   (batch, width:int) -> (batch, width:bool)\n",
    "    match_char = tf.math.equal(argmax_true, argmax_pred)\n",
    "    \n",
    "    # require all character in sample to match (batch, width:bool) -> (batch:bool)\n",
    "    match_word = tf.math.reduce_all(match_char, axis=-1)\n",
    "    \n",
    "    # bool to int                                   (batch:bool) -> (batch:float)\n",
    "    match_int = tf.cast(match_word, tf.float32)\n",
    "    \n",
    "    # percentage of samples that are an exact match (batch:float) -> float\n",
    "    return tf.reduce_mean(match_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def log(*l, **d): \n",
    "    \"\"\"Print function that supresses the print if 'verbose' is set False.\n",
    "    \n",
    "    # Arguments\n",
    "        *l, **d: unnamed and named arguments forwarded to the print function\n",
    "    \"\"\"\n",
    "    if verbose: print(*l, **d)\n",
    "        \n",
    "        \n",
    "        \n",
    "training_history = []\n",
    "def training_log(x, y, a, b, e, l, m):\n",
    "    \"\"\"Logs the training result for a training run\n",
    "    \n",
    "    # Arguments\n",
    "        values for the history dictionary\n",
    "    \"\"\"\n",
    "    training_history.append({'x':x, 'y':y, 'architecture':a, 'batch size':b, 'epochs':e, 'loss':l, 'accuracy':m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Vectorize Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data):\n",
    "    \"\"\"Vectorise multidimensional arrays of strings, \n",
    "    Convert a tensor rank n of strings into a tensor rank n+1 of int.\n",
    "    \n",
    "    # Arguments\n",
    "        data: multidimensional array (lists) of strings\n",
    "        \n",
    "    # Returns\n",
    "        multidimensional array with string replaced with vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recursively calls it self with a tensor rank n-1 until rank=0 and vectorize the string\n",
    "    \n",
    "    # base case, return vectorised string (characters replaced with numbers)\n",
    "    if type(data) == str:\n",
    "        return [char_to_int[char] for char in data.upper()]\n",
    "    \n",
    "    # call self on each item (rank n-1) along the top level axis\n",
    "    else:\n",
    "        try:\n",
    "            # return list of the vectorised items\n",
    "            return [vectorize_data(d) for d in data]\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "\n",
    "def devectorise_data(data):\n",
    "    \"\"\"'De-vectorize' data, convert vectors to string.\n",
    "    Each sample produces one string, 'words' are concatinated.\n",
    "    \n",
    "    # Arguments\n",
    "        data: multidimensional numpy array of ints\n",
    "        \n",
    "    # Returns\n",
    "        array of strings, one string per sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of samples in batch\n",
    "    length = data.shape[0]\n",
    "    \n",
    "    # reduce dimensionality to batch size x nnumber of characters\n",
    "    data = data.reshape(length, -1)\n",
    "    \n",
    "    # blank array of strings\n",
    "    strings = np.full((length,), '', dtype=object)\n",
    "    \n",
    "    # generate string for each sample\n",
    "    for i in range(length):\n",
    "        strings[i] = ''.join([int_to_char[int(i)] for i in data[i]])\n",
    "    \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Add Padding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find size of largest array across each dimension to computer shape of bounding ndarray\n",
    "def size(data):\n",
    "    \"\"\"Deetrmines the shape (sizes along each axis) of a list based multidimensional array\n",
    "    \n",
    "    # Arguments\n",
    "        data: list based multidimensional array\n",
    "        \n",
    "    # Returns\n",
    "        a tuple holding the sizes the dimensions of the array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recursivelt call it self on \n",
    "    \n",
    "    # base case, this item is a value, return empty shape\n",
    "    if type(data) == int:\n",
    "        return ()\n",
    "    \n",
    "    # number of items in the top level list\n",
    "    this_size = len(data)\n",
    "    \n",
    "    # if has items, inner shape is max of items shape\n",
    "    if this_size > 0:\n",
    "        inner_sizes = np.array([size(d) for d in data])\n",
    "        inner_sizes = tuple(np.amax(inner_sizes, axis=0))\n",
    "        \n",
    "    # if no items, inner empty shape\n",
    "    else:\n",
    "        inner_sizes = ()\n",
    "    \n",
    "    # this hsape is size of top level list and max shape of inner shapes\n",
    "    return (this_size,) + inner_sizes\n",
    "    \n",
    "    \n",
    "def insert_vector(matrix, data, indices=()):\n",
    "    \"\"\"insert vectors from list based multidimensional arrays into a numpy ndarray\n",
    "    \n",
    "    # Arguments\n",
    "        matrix: the ndarray to insert vectors into\n",
    "        data: the list based multimensional array of vectors\n",
    "        indices: \n",
    "    \"\"\"\n",
    "    \n",
    "    # Recursively call it self to insert rank n-1 arrays into the array\n",
    "    \n",
    "    # base case, insert int into ndarray\n",
    "    if type(data) == int:\n",
    "        matrix[indices] = data\n",
    "        \n",
    "    # data is rank >0, insert each item at the next level into the matrix, indices indicates sublocations\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            insert_vector(matrix, data[i], indices + (i,))\n",
    "    \n",
    "\n",
    "def pad_vector_data(data, pad_token, pad_shape=None):\n",
    "    \"\"\"create a uniformly shaped numpy ndarray filled with padding and insert the data into it.\n",
    "    \n",
    "    # Arguments\n",
    "        data: list based multidimensional array\n",
    "        pad_token: padding token (int)\n",
    "        pad_shape (optional): shape of ndarray, otherwise fit data\n",
    "        \n",
    "    # Returns \n",
    "        numpy ndarray containing padded data\n",
    "    \"\"\"\n",
    "    \n",
    "    # determine the shape needed to fit the data\n",
    "    shape = size(data)\n",
    "    if pad_shape != None:\n",
    "        shape = tuple(np.maximum(pad_shape, shape))\n",
    "\n",
    "    # empty matrix filled with the padding token\n",
    "    matrix = np.full(shape, pad_token, np.int32)\n",
    "\n",
    "    # insert data into the ndarray\n",
    "    insert_vector(matrix, data, ())\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split(data, sizes):\n",
    "    \"\"\"Split the data into subsets of the specified sizes.\n",
    "    \n",
    "    # Arguments\n",
    "        data: array of samples\n",
    "        sizes: list of sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    sizes = list(sizes)\n",
    "    \n",
    "    # convert sizes running totals, (5, 10, 5) -> (5, 15, 20)\n",
    "    for i in range(1, len(sizes)):\n",
    "        sizes[i] += sizes[i-1]\n",
    "    \n",
    "    # extract sized subsets, (5, 15, 20) -> 0:5, 5:15, 15:20\n",
    "    slices = [slice(i,j) for i, j in zip([0]+sizes, sizes)]\n",
    "    \n",
    "    return [data[s] for s in slices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*data):\n",
    "    \"\"\"Shuffle the datasets togeather (same order).\n",
    "    \n",
    "    # Arguments\n",
    "        *data: list of datasets\n",
    "        \n",
    "    # Returns \n",
    "        list of shuffled datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    order = np.arange(len(data[0]))         # default order of elements\n",
    "    np.random.shuffle(order)                # randomise order\n",
    "    return [d[order] for d in data]         # new array with items in the randimised order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract(*keys, **cuts):\n",
    "    \"\"\"Extract and return required features (datasets). E.g.: get FileName and LineName.\n",
    "    Optionally apply cuts to the data, E.g.: if cuts['a'] = (3,9) then a = a[:, 3:9]\n",
    "    \n",
    "    # Arguments\n",
    "        *keys: list of names of the datasets\n",
    "        **cuts: cuts to aplly to dataset of corresponding name/key\n",
    "    \n",
    "    # Returns\n",
    "        list of datasets\n",
    "        list of corresponding dat shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    onehots, shapes = [], []\n",
    "        \n",
    "    for key in keys:\n",
    "        \n",
    "        # get data from dictionary\n",
    "        onehot = onehot_data[key]\n",
    "\n",
    "        # apply cuts\n",
    "        cut = cuts.get(key, [[None]])\n",
    "        cut = [slice(*c) for c in cut]\n",
    "        cut = len(onehot.shape)*[slice(None)] + cut + [slice(None)]\n",
    "        cut = tuple(cut[-len(onehot.shape):])\n",
    "        onehot = onehot[cut]\n",
    "        \n",
    "        # calculate shape\n",
    "        shape = (None, *onehot.shape[1:])[-3:]\n",
    "\n",
    "        onehots.append(onehot)\n",
    "        shapes.append(shape)\n",
    "\n",
    "    return onehots, shapes\n",
    "    \n",
    "    \n",
    "def split_and_shuffle(*onehots, sizes=None, shuffle_before=False, shuffle_after=True):\n",
    "    \"\"\"Shuffle datasets, Split datasets into subsets, shuffle subsets\n",
    "    \n",
    "    # Arguments\n",
    "        *onehots: list of datasets\n",
    "        sizes: list of subset sizes\n",
    "        shuffle_before: whether to shuffle before the split\n",
    "        shuffle_after: whether to shuffle after the split\n",
    "        \n",
    "    # Returns\n",
    "        matrix of subsets with shape dadaset x subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    # sizes of subjets\n",
    "    sizes = sizes or [None]\n",
    "    key_count = len(onehots)\n",
    "    subset_count = len(sizes)\n",
    "    \n",
    "    # shuffle sets before splitting them\n",
    "    if shuffle_before:\n",
    "        onehots = shuffle(*onehots)\n",
    "\n",
    "    # split datasets\n",
    "    onehots_subsets = np.full((key_count, subset_count), None)\n",
    "    onehots_subsets[:,:] = [split(onehot, sizes) for onehot in onehots]\n",
    "\n",
    "    # shuffle subsets\n",
    "    if shuffle_after:\n",
    "        for i in range(subset_count):\n",
    "            onehots_subsets[:,i] = shuffle(*onehots_subsets[:,i])\n",
    "\n",
    "    return onehots_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tokens and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokens used to communicate non character entities\n",
    "tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>']\n",
    "\n",
    "# get set of characters to be used, use static preset list of characters\n",
    "#available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890-_().,\\\\/\\\"':&\")\n",
    "available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-_().,\\\\/\\\"':&\")\n",
    "\n",
    "# generate character to int and int to character maps\n",
    "char_to_int = {c: i for i, c in enumerate(tokens + available_chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "char_count = len(char_to_int) # number of character available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_source_file = 'SHUP 2D Files Training Data.csv'\n",
    "\n",
    "# read raw training data\n",
    "data_df = pd.read_csv(raw_source_file, dtype=str)\n",
    "\n",
    "# read columns into dictionary (column name -> column values)\n",
    "data = {feature:data_df[feature].values for feature in data_df.columns.values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unique Record ID':            (23903, 6, 53)\n",
      "'FileName':                    (23903, 87, 53)\n",
      "'Original_FileName':           (23903, 71, 53)\n",
      "'SurveyNum':                   (23903, 5, 53)\n",
      "'SurveyName':                  (23903, 39, 53)\n",
      "'LineName':                    (23903, 23, 53)\n",
      "'SurveyType':                  (23903, 6, 53)\n",
      "'PrimaryDataType':             (23903, 14, 53)\n",
      "'SecondaryDataType':           (23903, 36, 53)\n",
      "'TertiaryDataType':            (23903, 17, 53)\n",
      "'Quaternary':                  (23903, 8, 53)\n",
      "'File_Range':                  (23903, 13, 53)\n",
      "'First_SP_CDP':                (23903, 8, 53)\n",
      "'Last_SP_CDP':                 (23903, 7, 53)\n",
      "'CompletionYear':              (23903, 4, 53)\n",
      "'TenureType':                  (23903, 3, 53)\n",
      "'Operator Name':               (23903, 47, 53)\n",
      "'GSQBarcode':                  (23903, 17, 53)\n",
      "'EnergySource':                (23903, 29, 53)\n",
      "'LookupDOSFilePath':           (23903, 181, 53)\n",
      "'Source Of Data':              (23903, 8, 53)\n",
      "'LookupDOSFilePath_Words':     (23903, 63, 13, 53)\n",
      "'FileName_Words':              (23903, 29, 13, 53)\n",
      "'LineName_Words':              (23903, 15, 10, 53)\n",
      "'SurveyName_Words':            (23903, 17, 13, 53)\n"
     ]
    }
   ],
   "source": [
    "# split strings into words\n",
    "delimiters = r'( |_|-|\\.|\\,|/|\\\\|\\(|\\)|&|:|\\'|\")' # any of '-_().,\\\\/\\\"':& '\n",
    "replacement = r'\\0\\g<1>\\0' # surround delimiter with splitting token\n",
    "\n",
    "# split strings into words acording to RegEx for some datasets\n",
    "data['LookupDOSFilePath_Words'] = np.array([re.sub(delimiters, replacement, s).split('\\0') for s in data['LookupDOSFilePath']])\n",
    "data['FileName_Words'] = np.array([re.sub(delimiters, replacement, s).split('\\0') for s in data['FileName']])\n",
    "data['LineName_Words'] = np.array([re.sub(delimiters, replacement, s).split('\\0') for s in data['LineName']])\n",
    "data['SurveyName_Words'] = np.array([re.sub(delimiters, replacement, s).split('\\0') for s in data['SurveyName']])\n",
    "\n",
    "# vectorise string in each dataset\n",
    "vectorized_data = {f: vectorize_data(data[f])    for f in data}\n",
    "\n",
    "# move datasets into padded ndarrays\n",
    "padded_data =     {f: pad_vector_data(vectorized_data[f], char_to_int['<Padding>'])    for f in vectorized_data}\n",
    "\n",
    "# convert int to one hot encodings\n",
    "onehot_data =     {f: keras.utils.to_categorical(padded_data[f], char_count)    for f in padded_data}\n",
    "\n",
    "for f in onehot_data: print(f\"'{f}':\".ljust(30), onehot_data[f].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function: Test and show samlpe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, x_test=None, y_test=None, x_preview=None, y_preview=None):\n",
    "    \"\"\"Test a model, calculate accuracy from test data, compute and show previews.\n",
    "    \n",
    "    # Arguments\n",
    "         model: model to test\n",
    "         x_test: accuracy test inputs\n",
    "         y_test: accuracy test true outputs\n",
    "         x_preview: inputs to print and show\n",
    "         y_preview: true outputs to print and show\n",
    "         \n",
    "    # Returns\n",
    "        loss computed from test set\n",
    "        accuracy of predictions on test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # if preview data is provided, run predictions on preview data and decode them to strings\n",
    "    if (x_preview is not None) and (y_preview is not None):\n",
    "        t_size = len(x_preview)\n",
    "        \n",
    "        # run predictions and decode\n",
    "        p_one_hot = model.predict(x_preview)\n",
    "        p_vector = np.argmax(p_one_hot, -1)\n",
    "        p_vector = p_vector.reshape((t_size, -1))\n",
    "        p_strings = devectorise_data(p_vector)\n",
    "\n",
    "        # decode ground truth\n",
    "        y_vector = np.argmax(y_preview, -1)\n",
    "        y_vector = y_vector.reshape((t_size, -1))\n",
    "        y_strings = devectorise_data(y_vector)\n",
    "\n",
    "        # decode inputs\n",
    "        x_vector = np.argmax(x_preview, -1)\n",
    "        x_vector = x_vector.reshape((t_size, -1))\n",
    "        x_strings = devectorise_data(x_vector)\n",
    "\n",
    "        # numbering for samples\n",
    "        n_strings = [f'{i}. ' for i in range(t_size)]\n",
    "        \n",
    "        # replace '<Padding>' with ' ' and remove spaces at start and end\n",
    "        x_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in x_strings]\n",
    "        y_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in y_strings]\n",
    "        p_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in p_strings]\n",
    "        \n",
    "        # compute minimum width of each column\n",
    "        n_w = max([len(s) for s in n_strings])\n",
    "        x_w = max([len(s) for s in x_strings])\n",
    "        y_w = max([len(s) for s in y_strings])\n",
    "        p_w = max([len(s) for s in p_strings])\n",
    "        \n",
    "        # create equal width rows with number(n), input (x), true output (y), predicted output (p) and if it matches\n",
    "        y_p_strings = [\"'  '\".join([n.ljust(n_w), x.ljust(x_w), y.ljust(y_w), p.ljust(p_w), str(y==p)]) for n, x, y, p in zip(n_strings, x_strings, y_strings, p_strings)]\n",
    "\n",
    "        print(*y_p_strings, sep='\\n', end='\\n\\n')\n",
    "\n",
    "        \n",
    "    # if test data is provided, run predictions and measure accuracies\n",
    "    if (x_test is not None) and (y_test is not None):\n",
    "        \n",
    "        # metric names\n",
    "        metrics = [model.loss] + model.metrics\n",
    "        \n",
    "        # accuracy on entire training set\n",
    "        accuracies = model.evaluate(x_test, y_test)\n",
    "        print(*list(zip(metrics, accuracies)), sep='\\n', end='\\n\\n') # evaluate and list loss and each metric\n",
    "\n",
    "        return accuracies[0], accuracies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maps model name onto model object\n",
    "models = {}\n",
    "\n",
    "# maps embedding name onto embedding object\n",
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#architecture = ''\n",
    "\n",
    "# dimensionality of embedded characters\n",
    "character_embedding_size = 10\n",
    "\n",
    "# compile parameters for models\n",
    "metrics = ['mean_absolute_error', 'categorical_accuracy', exact_match_accuracy] # binary_accuracy\n",
    "loss = 'categorical_crossentropy' # poisson mean_squared_logarithmic_error categorical_crossentropy\n",
    "\n",
    "# compile parameters for embedding models\n",
    "embed_loss='categorical_crossentropy'\n",
    "embed_metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', exact_match_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Character Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Auto Encoder Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create offset input and output sequences to training a preditive embedding model.\n",
    "\n",
    "(x_char_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract('LookupDOSFilePath')\n",
    "(x_char_onehot,) = shuffle(x_char_onehot)\n",
    "shape = x_char_onehot.shape\n",
    "x_embed_size = len(x_char_onehot)\n",
    "\n",
    "# create columns of padding tokens\n",
    "padding = np.full((*shape[:-2], 1), char_to_int['<Padding>'])\n",
    "#padding = np.array([[char_to_int['<Padding>']]] * x_embed_size)\n",
    "padding = keras.utils.to_categorical(padding, char_count)\n",
    "padding = padding.reshape(*shape[:-2], 1, shape[-1])\n",
    "\n",
    "# 'abcd' -> ('_abcd', 'abcd_')\n",
    "x_embed_train = np.concatenate((x_char_onehot, padding), axis=-2)\n",
    "y_embed_train = np.concatenate((padding, x_char_onehot), axis=-2)\n",
    "x_embed_test = np.concatenate((x_char_onehot, padding), axis=-2)\n",
    "y_embed_test = np.concatenate((padding, x_char_onehot), axis=-2)\n",
    "\n",
    "embed_char_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Auto Encoder: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lh (Dense)                   (None, 182, 10)           540       \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 182, 53)           583       \n",
      "=================================================================\n",
      "Total params: 1,123\n",
      "Trainable params: 1,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Character-Embedding'\n",
    "\n",
    "model_E_D_NN = keras.Sequential()\n",
    "model_E_D_NN.add(keras.layers.Dense(character_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_D_NN.add(keras.layers.Dense(char_count, activation='sigmoid', name='lo'))\n",
    "#model_E_D_NN.add(keras.layers.Dropout(0.001))\n",
    "model_E_D_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_D_NN\n",
    "print(model_E_D_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "23903/23903 [==============================] - 9s 377us/step - loss: 2.1263 - acc: 0.4376 - mean_absolute_error: 0.1169 - categorical_accuracy: 0.4376 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "23903/23903 [==============================] - 8s 326us/step - loss: 1.4346 - acc: 0.5552 - mean_absolute_error: 0.0323 - categorical_accuracy: 0.5552 - exact_match_accuracy: 0.0000e+00\n",
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY'  'A\\SSUA\\ED\\SORSG\\AUOROS\\\\SEDTAED\\SUUROADETATA\\198A_T_ROD_ROROORA\\SSGA_T_ROD_ROROORDS\\\\09\\\\DUOROS\\\\SED\\ES80\\80ATD\\89\\190\\SG'  'False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> y   predictive\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, y_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], y_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23903/23903 [==============================] - 8s 322us/step - loss: 0.2766 - acc: 0.9204 - mean_absolute_error: 0.0211 - categorical_accuracy: 0.9204 - exact_match_accuracy: 0.3139\n",
      "Epoch 2/5\n",
      "23903/23903 [==============================] - 8s 314us/step - loss: 0.0070 - acc: 1.0000 - mean_absolute_error: 0.0173 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9950\n",
      "Epoch 3/5\n",
      "23903/23903 [==============================] - 7s 311us/step - loss: 0.0018 - acc: 1.0000 - mean_absolute_error: 0.0171 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9995\n",
      "Epoch 4/5\n",
      "23903/23903 [==============================] - 7s 305us/step - loss: 6.9150e-04 - acc: 1.0000 - mean_absolute_error: 0.0170 - categorical_accuracy: 1.0000 - exact_match_accuracy: 0.9995\n",
      "Epoch 5/5\n",
      "23903/23903 [==============================] - 7s 309us/step - loss: 2.8549e-04 - acc: 1.0000 - mean_absolute_error: 0.0170 - categorical_accuracy: 1.0000 - exact_match_accuracy: 1.0000\n",
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY'  'True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> x   direct\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, x_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], x_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# w1 = model.layers[0].get_weights()\n",
    "# w2 = model.layers[1].get_weights()\n",
    "\n",
    "# w = [np.copy(w1[0]), np.zeros(w1[1].shape)]\n",
    "# wi = [np.linalg.pinv(w1[0]), np.zeros(w2[1].shape)]\n",
    "\n",
    "# m = keras.Sequential()\n",
    "# m.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(embed_char_count, voc_size,)))\n",
    "# m.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "# m.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "# m.layers[0].set_weights(w)\n",
    "# m.layers[1].set_weights(wi)\n",
    "\n",
    "# encode_weights, decode_weights = w, wi\n",
    "\n",
    "# accuracy = model.evaluate(embed_test_x, embed_test_x)\n",
    "# metric_names = [embed_loss] + embed_metrics\n",
    "# dict(zip(metric_names, accuracy))\n",
    "\n",
    "embeddings['Character-Embedding'] = [models['Character-Embedding'].layers[0].get_weights(), models['Character-Embedding'].layers[1].get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY                                      '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY                                      '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\MAJOR_MOROCCO\\SEGY\\MAJOR_MOROCCO_HSB-821_PROCESSED_SDU07104TA_203293.SGY                                      '  'True\n",
      "1. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\ROCKY\\SEGY\\ROCKY_PR85-66_FINAL_QR017432_174230.SGY                                                            '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\ROCKY\\SEGY\\ROCKY_PR85-66_FINAL_QR017432_174230.SGY                                                            '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\ROCKY\\SEGY\\ROCKY_PR85-66_FINAL_QR017432_174230.SGY                                                            '  'True\n",
      "2. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GRAY\\SEGY\\GRAY_83-NTS_FINAL_MIGRATED_SDU02727TA_247323.SGY                                                    '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GRAY\\SEGY\\GRAY_83-NTS_FINAL_MIGRATED_SDU02727TA_247323.SGY                                                    '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\GRAY\\SEGY\\GRAY_83-NTS_FINAL_MIGRATED_SDU02727TA_247323.SGY                                                    '  'True\n",
      "3. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\LEICHHARDT\\SEGY\\LEICHHARDT_91-DAB_FILTERED_MIGRATION_QR018290_175259.SGY                                      '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\LEICHHARDT\\SEGY\\LEICHHARDT_91-DAB_FILTERED_MIGRATION_QR018290_175259.SGY                                      '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\LEICHHARDT\\SEGY\\LEICHHARDT_91-DAB_FILTERED_MIGRATION_QR018290_175259.SGY                                      '  'True\n",
      "4. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EEP_FINAL_MIGRATED_QR014158_170101.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EEP_FINAL_MIGRATED_QR014158_170101.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EEP_FINAL_MIGRATED_QR014158_170101.SGY'  'True\n",
      "\n",
      "23903/23903 [==============================] - 4s 157us/step\n",
      "('categorical_crossentropy', 0.00018033243587115954)\n",
      "('accuracy', 1.0)\n",
      "('mean_absolute_error', 0.016956020030733637)\n",
      "('categorical_accuracy', 1.0)\n",
      "(<function exact_match_accuracy at 0x000001C9A6F30378>, 1.0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00018033243587115954, 1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test(models['Character-Embedding'], x_embed_train, x_embed_train, x_embed_train[:5], x_embed_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Auto Encoder Folder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WordEmbedding():\n",
    "    \"\"\"Defines, holds, trains and provides word and character embedding models.\n",
    "    Allows other models to apply its embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x_name, compression=0.1, weight_constraint=10.0, clipnorm=1):\n",
    "        \"\"\"creates a new word embedding and copyes the pre-trained character embedding\n",
    "        \n",
    "        # Arguments\n",
    "            x_name: name of the dataset for which the embedding is\n",
    "            compression: ratio of of onehot to latent vector size\n",
    "            weight_constraint: keras.layers.Dense(kernel_constraint value)\n",
    "            clipnorm: keras.optimizers.OPTIMIZER(clipnorm value)\n",
    "        \"\"\"\n",
    "\n",
    "        # extract and shuffle data\n",
    "        (x_word_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract(x_name)\n",
    "        (x_word_onehot,) = shuffle(x_word_onehot)\n",
    "        \n",
    "        # add small amount to 0's to avoid 0 values\n",
    "        # e = 0.01\n",
    "        # x_word_onehot = x_word_onehot * (1-e) + e\n",
    "\n",
    "        # if data is not split into words, create extra word diemnstion\n",
    "        if(embed_word_count == None):\n",
    "            embed_word_count = 1\n",
    "            x_word_onehot = np.expand_dims(x_word_onehot, 1)\n",
    "        \n",
    "        self.x_word_onehot = x_word_onehot\n",
    "\n",
    "        # define embedding sizes\n",
    "        self.character_embedding_size = character_embedding_size\n",
    "        self.word_embedding_size = int(embed_char_count * self.character_embedding_size * compression) + 1\n",
    "        c_size, w_size = self.character_embedding_size, self.word_embedding_size\n",
    "        \n",
    "        \n",
    "        # define model\n",
    "\n",
    "        # character embedding\n",
    "        self.l_encode_character = keras.layers.Dense(\n",
    "            c_size, \n",
    "            name='char_encode', \n",
    "            input_shape=(embed_word_count, embed_char_count, embed_ones_count), \n",
    "            kernel_constraint=keras.constraints.max_norm(weight_constraint)\n",
    "        )\n",
    "        self.l_char_to_word = keras.layers.Reshape(\n",
    "            (embed_word_count, embed_char_count * c_size,)\n",
    "        )\n",
    "\n",
    "        # word auto encoder\n",
    "        self.l_encode_word = keras.layers.Dense(\n",
    "            w_size, name='lh', \n",
    "            input_shape=(embed_char_count, char_count,), \n",
    "            kernel_constraint=keras.constraints.max_norm(weight_constraint)\n",
    "        )\n",
    "        self.l_decode_word = keras.layers.Dense(\n",
    "            embed_char_count * c_size, \n",
    "            activation='sigmoid', name='lo', \n",
    "            kernel_constraint=keras.constraints.max_norm(weight_constraint)\n",
    "        )\n",
    "\n",
    "        # character de embedding\n",
    "        self.l_word_to_char = keras.layers.Reshape(\n",
    "            (embed_word_count, embed_char_count, c_size)\n",
    "        )\n",
    "        self.l_decode_character = keras.layers.Dense(\n",
    "            embed_ones_count, \n",
    "            activation='sigmoid', \n",
    "            name='char_decode', \n",
    "            kernel_constraint=keras.constraints.max_norm(weight_constraint)\n",
    "        )\n",
    "\n",
    "        self.model = keras.Sequential([\n",
    "            self.l_encode_character,\n",
    "            #keras.layers.Dropout(0.1),\n",
    "            self.l_char_to_word,\n",
    "            #keras.layers.Dropout(0.1),\n",
    "            self.l_encode_word,\n",
    "            #keras.layers.Dropout(0.1),\n",
    "            self.l_decode_word,\n",
    "            #keras.layers.Dropout(0.1),\n",
    "            self.l_word_to_char,\n",
    "            #keras.layers.Dropout(0.1),\n",
    "            self.l_decode_character,\n",
    "        ])\n",
    "\n",
    "        RMSprop = keras.optimizers.RMSprop(clipnorm=clipnorm)\n",
    "        self.model.compile(optimizer=RMSprop, loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "        \n",
    "        # set pre-trained character embedding\n",
    "        self.l_encode_character.set_weights(deepcopy(embeddings['Character-Embedding'][0]))\n",
    "        self.l_decode_character.set_weights(deepcopy(embeddings['Character-Embedding'][1]))\n",
    "\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "    def train(self, epochs=5, batch_size=32):\n",
    "        \"\"\"Train the model and print accuracies\n",
    "        \n",
    "        # Arguments\n",
    "            epochs: number of epochs to train for\n",
    "            batch_size: batch size\n",
    "        \"\"\"\n",
    "        \n",
    "        # perform training\n",
    "        self.model.fit(self.x_word_onehot, self.x_word_onehot, batch_size=batch_size, epochs=epochs)\n",
    "        test(self.model, self.x_word_onehot, self.x_word_onehot, self.x_word_onehot[:3], self.x_word_onehot[:3])\n",
    "\n",
    "        \n",
    "    def apply_encode(self, new_encode_character, new_encode_word):\n",
    "        \"\"\"Apply weights for character and word encoding to layers of outher models\n",
    "        \n",
    "        # Arguments\n",
    "            new_encode_character: layer to recieve character embedding weights\n",
    "            new_encode_word: layer to recieve word embedding weights\"\"\"\n",
    "        \n",
    "        new_encode_character.set_weights(deepcopy(self.l_encode_character.get_weights()))\n",
    "        new_encode_word.set_weights(deepcopy(self.l_encode_word.get_weights()))\n",
    "\n",
    "        \n",
    "    def apply_decode(self, new_decode_word, new_decode_character):\n",
    "        \"\"\"Apply weights for character and word decoding to layers of outher models\n",
    "        \n",
    "        # Arguments\n",
    "            new_encode_character: layer to recieve character embedding weights\n",
    "            new_encode_word: layer to recieve word embedding weights\"\"\"\n",
    "        \n",
    "        new_decode_word.set_weights(deepcopy(self.l_decode_word.get_weights()))\n",
    "        new_decode_character.set_weights(deepcopy(self.l_decode_character.get_weights()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### LineName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 1, 23, 10)         540       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 230)            0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1, 35)             8085      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 1, 230)            8280      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1, 23, 10)         0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 1, 23, 53)         583       \n",
      "=================================================================\n",
      "Total params: 17,488\n",
      "Trainable params: 17,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "23903/23903 [==============================] - 4s 156us/step - loss: 1.5529 - acc: 0.6252 - mean_absolute_error: 0.0913 - categorical_accuracy: 0.6252 - exact_match_accuracy: 0.0379\n",
      "Epoch 2/15\n",
      "23903/23903 [==============================] - 3s 143us/step - loss: 0.2246 - acc: 0.9752 - mean_absolute_error: 0.0084 - categorical_accuracy: 0.9752 - exact_match_accuracy: 0.6331\n",
      "Epoch 3/15\n",
      "23903/23903 [==============================] - 3s 143us/step - loss: 0.0683 - acc: 0.9940 - mean_absolute_error: 0.0083 - categorical_accuracy: 0.9940 - exact_match_accuracy: 0.9069\n",
      "Epoch 4/15\n",
      "23903/23903 [==============================] - 3s 142us/step - loss: 0.0313 - acc: 0.9966 - mean_absolute_error: 0.0097 - categorical_accuracy: 0.9966 - exact_match_accuracy: 0.9506\n",
      "Epoch 5/15\n",
      "23903/23903 [==============================] - 3s 143us/step - loss: 0.0211 - acc: 0.9976 - mean_absolute_error: 0.0109 - categorical_accuracy: 0.9976 - exact_match_accuracy: 0.9668\n",
      "Epoch 6/15\n",
      "23903/23903 [==============================] - 3s 141us/step - loss: 0.0169 - acc: 0.9981 - mean_absolute_error: 0.0114 - categorical_accuracy: 0.9981 - exact_match_accuracy: 0.9739\n",
      "Epoch 7/15\n",
      "23903/23903 [==============================] - 4s 146us/step - loss: 0.0140 - acc: 0.9984 - mean_absolute_error: 0.0118 - categorical_accuracy: 0.9984 - exact_match_accuracy: 0.9789\n",
      "Epoch 8/15\n",
      "23903/23903 [==============================] - 3s 141us/step - loss: 0.0126 - acc: 0.9986 - mean_absolute_error: 0.0121 - categorical_accuracy: 0.9986 - exact_match_accuracy: 0.9817\n",
      "Epoch 9/15\n",
      "23903/23903 [==============================] - 3s 142us/step - loss: 0.0112 - acc: 0.9987 - mean_absolute_error: 0.0124 - categorical_accuracy: 0.9987 - exact_match_accuracy: 0.9838\n",
      "Epoch 10/15\n",
      "23903/23903 [==============================] - 3s 140us/step - loss: 0.0102 - acc: 0.9989 - mean_absolute_error: 0.0127 - categorical_accuracy: 0.9989 - exact_match_accuracy: 0.9861\n",
      "Epoch 11/15\n",
      "23903/23903 [==============================] - 3s 140us/step - loss: 0.0094 - acc: 0.9990 - mean_absolute_error: 0.0128 - categorical_accuracy: 0.9990 - exact_match_accuracy: 0.9871\n",
      "Epoch 12/15\n",
      "23903/23903 [==============================] - 3s 144us/step - loss: 0.0089 - acc: 0.9991 - mean_absolute_error: 0.0129 - categorical_accuracy: 0.9991 - exact_match_accuracy: 0.9884\n",
      "Epoch 13/15\n",
      "23903/23903 [==============================] - 3s 145us/step - loss: 0.0084 - acc: 0.9991 - mean_absolute_error: 0.0130 - categorical_accuracy: 0.9991 - exact_match_accuracy: 0.9891\n",
      "Epoch 14/15\n",
      "23903/23903 [==============================] - 4s 147us/step - loss: 0.0081 - acc: 0.9991 - mean_absolute_error: 0.0134 - categorical_accuracy: 0.9991 - exact_match_accuracy: 0.9902\n",
      "Epoch 15/15\n",
      "23903/23903 [==============================] - 4s 147us/step - loss: 0.0074 - acc: 0.9992 - mean_absolute_error: 0.0136 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.9900\n",
      "0. '  '84-TSR  '  '84-TSR  '  '84-TSR  '  'True\n",
      "1. '  'SH84-108'  'SH84-108'  'SH84-108'  'True\n",
      "2. '  'OS96-404'  'OS96-404'  'OS96-404'  'True\n",
      "\n",
      "23903/23903 [==============================] - 1s 40us/step\n",
      "('categorical_crossentropy', 0.0069272571884630854)\n",
      "('accuracy', 0.9993215344747439)\n",
      "('mean_absolute_error', 0.013701214440496106)\n",
      "('categorical_accuracy', 0.9993215344747439)\n",
      "(<function exact_match_accuracy at 0x000001C9A6F30378>, 0.9922603857256411)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['LineName_Embedding'] = WordEmbedding('LineName', 0.15)\n",
    "embeddings['LineName_Embedding'].train(15, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### FileName_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 29, 13, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 29, 130)           0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 29, 27)            3537      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 29, 130)           3640      \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 29, 13, 10)        0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 29, 13, 53)        583       \n",
      "=================================================================\n",
      "Total params: 8,300\n",
      "Trainable params: 8,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "23903/23903 [==============================] - 21s 878us/step - loss: 1.3505 - acc: 0.6803 - mean_absolute_error: 0.0843 - categorical_accuracy: 0.6803 - exact_match_accuracy: 0.5103\n",
      "Epoch 2/10\n",
      "23903/23903 [==============================] - 20s 851us/step - loss: 0.0940 - acc: 0.9875 - mean_absolute_error: 0.0063 - categorical_accuracy: 0.9875 - exact_match_accuracy: 0.9085\n",
      "Epoch 3/10\n",
      "23903/23903 [==============================] - 20s 841us/step - loss: 0.0215 - acc: 0.9974 - mean_absolute_error: 0.0082 - categorical_accuracy: 0.9974 - exact_match_accuracy: 0.9793\n",
      "Epoch 4/10\n",
      "23903/23903 [==============================] - 21s 865us/step - loss: 0.0113 - acc: 0.9987 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9987 - exact_match_accuracy: 0.98866\n",
      "Epoch 5/10\n",
      "23903/23903 [==============================] - 22s 916us/step - loss: 0.0086 - acc: 0.9990 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9990 - exact_match_accuracy: 0.99159s - loss: 0.0089 - - ETA: 3s - loss: 0.0087 - acc: 0.9990 - mean_absolute_error: 0\n",
      "Epoch 6/10\n",
      "23903/23903 [==============================] - 21s 862us/step - loss: 0.0071 - acc: 0.9992 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.9928\n",
      "Epoch 7/10\n",
      "23903/23903 [==============================] - 20s 836us/step - loss: 0.0062 - acc: 0.9993 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9993 - exact_match_accuracy: 0.9937\n",
      "Epoch 8/10\n",
      "23903/23903 [==============================] - 20s 817us/step - loss: 0.0056 - acc: 0.9994 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9994 - exact_match_accuracy: 0.9944\n",
      "Epoch 9/10\n",
      "23903/23903 [==============================] - 21s 870us/step - loss: 0.0051 - acc: 0.9994 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9994 - exact_match_accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "23903/23903 [==============================] - 21s 876us/step - loss: 0.0048 - acc: 0.9995 - mean_absolute_error: 0.0098 - categorical_accuracy: 0.9995 - exact_match_accuracy: 0.9952\n",
      "0. '  'DARLING _ 88 - FKK _ FINAL _ QR013207 _ 168620 . SGY            '  'DARLING _ 88 - FKK _ FINAL _ QR013207 _ 168620 . SGY            '  'DARLING _ 88 - FKK _ FINAL _ QR013207 _ 168620 . SGY            '  'True\n",
      "1. '  'BECKER _ 86 - ZNT _ FINAL _ FILTERED _ SDU02101TA _ 244283 . SGY'  'BECKER _ 86 - ZNT _ FINAL _ FILTERED _ SDU02101TA _ 244283 . SGY'  'BECKER _ 86 - ZNT _ FINAL _ FILTERED _ SDU02101TA _ 244283 . SGY'  'True\n",
      "2. '  'GRAY _ 84 - SEE _ FINAL _ SDU06974TA _ 193178 . SGY             '  'GRAY _ 84 - SEE _ FINAL _ SDU06974TA _ 193178 . SGY             '  'GRAY _ 84 - SEE _ FINAL _ SDU06974TA _ 193178 . SGY             '  'True\n",
      "\n",
      "23903/23903 [==============================] - 7s 312us/step\n",
      "('categorical_crossentropy', 0.004599006657227946)\n",
      "('accuracy', 0.9995612273085384)\n",
      "('mean_absolute_error', 0.010131461496186676)\n",
      "('categorical_accuracy', 0.9995612273085384)\n",
      "(<function exact_match_accuracy at 0x000001C9A6F30378>, 0.9958135358973965)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['FileName_Words_Embedding'] = WordEmbedding('FileName_Words', 0.2)\n",
    "embeddings['FileName_Words_Embedding'].train(10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SurveyName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 1, 39, 10)         540       \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 1, 390)            0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1, 59)             23069     \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 1, 390)            23400     \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 1, 39, 10)         0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 1, 39, 53)         583       \n",
      "=================================================================\n",
      "Total params: 47,592\n",
      "Trainable params: 47,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "23903/23903 [==============================] - 5s 227us/step - loss: 1.5910 - acc: 0.5933 - mean_absolute_error: 0.0899 - categorical_accuracy: 0.5933 - exact_match_accuracy: 0.0110\n",
      "Epoch 2/15\n",
      "23903/23903 [==============================] - 5s 206us/step - loss: 0.2237 - acc: 0.9666 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9666 - exact_match_accuracy: 0.5016\n",
      "Epoch 3/15\n",
      "23903/23903 [==============================] - 5s 205us/step - loss: 0.0588 - acc: 0.9931 - mean_absolute_error: 0.0102 - categorical_accuracy: 0.9931 - exact_match_accuracy: 0.8247\n",
      "Epoch 4/15\n",
      "23903/23903 [==============================] - 5s 215us/step - loss: 0.0313 - acc: 0.9975 - mean_absolute_error: 0.0105 - categorical_accuracy: 0.9975 - exact_match_accuracy: 0.9447\n",
      "Epoch 5/15\n",
      "23903/23903 [==============================] - 5s 202us/step - loss: 0.0223 - acc: 0.9983 - mean_absolute_error: 0.0106 - categorical_accuracy: 0.9983 - exact_match_accuracy: 0.9612\n",
      "Epoch 6/15\n",
      "23903/23903 [==============================] - 5s 204us/step - loss: 0.0179 - acc: 0.9987 - mean_absolute_error: 0.0109 - categorical_accuracy: 0.9987 - exact_match_accuracy: 0.9698\n",
      "Epoch 7/15\n",
      "23903/23903 [==============================] - 5s 203us/step - loss: 0.0152 - acc: 0.9990 - mean_absolute_error: 0.0114 - categorical_accuracy: 0.9990 - exact_match_accuracy: 0.9755\n",
      "Epoch 8/15\n",
      "23903/23903 [==============================] - 5s 203us/step - loss: 0.0130 - acc: 0.9991 - mean_absolute_error: 0.0118 - categorical_accuracy: 0.9991 - exact_match_accuracy: 0.9792\n",
      "Epoch 9/15\n",
      "23903/23903 [==============================] - 5s 207us/step - loss: 0.0114 - acc: 0.9992 - mean_absolute_error: 0.0117 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.9783\n",
      "Epoch 10/15\n",
      "23903/23903 [==============================] - 5s 201us/step - loss: 0.0104 - acc: 0.9992 - mean_absolute_error: 0.0117 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.9792\n",
      "Epoch 11/15\n",
      "23903/23903 [==============================] - 5s 201us/step - loss: 0.0097 - acc: 0.9993 - mean_absolute_error: 0.0119 - categorical_accuracy: 0.9993 - exact_match_accuracy: 0.9803\n",
      "Epoch 12/15\n",
      "23903/23903 [==============================] - 5s 201us/step - loss: 0.0092 - acc: 0.9993 - mean_absolute_error: 0.0121 - categorical_accuracy: 0.9993 - exact_match_accuracy: 0.9801\n",
      "Epoch 13/15\n",
      "23903/23903 [==============================] - 5s 205us/step - loss: 0.0086 - acc: 0.9994 - mean_absolute_error: 0.0123 - categorical_accuracy: 0.9994 - exact_match_accuracy: 0.9822\n",
      "Epoch 14/15\n",
      "23903/23903 [==============================] - 5s 224us/step - loss: 0.0083 - acc: 0.9995 - mean_absolute_error: 0.0125 - categorical_accuracy: 0.9995 - exact_match_accuracy: 0.9845\n",
      "Epoch 15/15\n",
      "23903/23903 [==============================] - 5s 206us/step - loss: 0.0078 - acc: 0.9995 - mean_absolute_error: 0.0129 - categorical_accuracy: 0.9995 - exact_match_accuracy: 0.9851\n",
      "0. '  'WILPAROO      '  'WILPAROO      '  'WILPAROO      '  'True\n",
      "1. '  'GERALDA       '  'GERALDA       '  'GERALDA       '  'True\n",
      "2. '  'LOCKYER VALLEY'  'LOCKYER VALLEY'  'LOCKYER VALLEY'  'True\n",
      "\n",
      "23903/23903 [==============================] - 1s 59us/step\n",
      "('categorical_crossentropy', 0.007696121746588229)\n",
      "('accuracy', 0.9994883179609753)\n",
      "('mean_absolute_error', 0.012929529355297236)\n",
      "('categorical_accuracy', 0.9994883179609753)\n",
      "(<function exact_match_accuracy at 0x000001C9A6F30378>, 0.9855248295193072)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['SurveyName_Embedding'] = WordEmbedding('SurveyName', 0.15, 8, 0.01)\n",
    "embeddings['SurveyName_Embedding'].train(15, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### GSQBarcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 1, 17, 10)         540       \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 1, 170)            0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1, 26)             4446      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 1, 170)            4590      \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 1, 17, 10)         0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 1, 17, 53)         583       \n",
      "=================================================================\n",
      "Total params: 10,159\n",
      "Trainable params: 10,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "23903/23903 [==============================] - 4s 155us/step - loss: 1.5902 - acc: 0.5530 - mean_absolute_error: 0.0931 - categorical_accuracy: 0.5530 - exact_match_accuracy: 0.0682\n",
      "Epoch 2/15\n",
      "23903/23903 [==============================] - 3s 143us/step - loss: 0.1384 - acc: 0.9848 - mean_absolute_error: 0.0112 - categorical_accuracy: 0.9848 - exact_match_accuracy: 0.8589\n",
      "Epoch 3/15\n",
      "23903/23903 [==============================] - 3s 138us/step - loss: 0.0208 - acc: 0.9987 - mean_absolute_error: 0.0121 - categorical_accuracy: 0.9987 - exact_match_accuracy: 0.9831\n",
      "Epoch 4/15\n",
      "23903/23903 [==============================] - 3s 134us/step - loss: 0.0105 - acc: 0.9995 - mean_absolute_error: 0.0124 - categorical_accuracy: 0.9995 - exact_match_accuracy: 0.9958 1s - loss: 0.0111 - acc: 0.9994 - mean_absolute_error: 0.0124 - categorical_accuracy: 0.9994 \n",
      "Epoch 5/15\n",
      "23903/23903 [==============================] - 3s 135us/step - loss: 0.0074 - acc: 0.9997 - mean_absolute_error: 0.0128 - categorical_accuracy: 0.9997 - exact_match_accuracy: 0.9977\n",
      "Epoch 6/15\n",
      "23903/23903 [==============================] - 3s 145us/step - loss: 0.0059 - acc: 0.9997 - mean_absolute_error: 0.0131 - categorical_accuracy: 0.9997 - exact_match_accuracy: 0.9978\n",
      "Epoch 7/15\n",
      "23903/23903 [==============================] - 4s 152us/step - loss: 0.0046 - acc: 0.9998 - mean_absolute_error: 0.0133 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9979\n",
      "Epoch 8/15\n",
      "23903/23903 [==============================] - 3s 140us/step - loss: 0.0039 - acc: 0.9998 - mean_absolute_error: 0.0139 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9981\n",
      "Epoch 9/15\n",
      "23903/23903 [==============================] - 3s 138us/step - loss: 0.0034 - acc: 0.9998 - mean_absolute_error: 0.0144 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9986\n",
      "Epoch 10/15\n",
      "23903/23903 [==============================] - 3s 137us/step - loss: 0.0032 - acc: 0.9998 - mean_absolute_error: 0.0149 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9987\n",
      "Epoch 11/15\n",
      "23903/23903 [==============================] - 3s 142us/step - loss: 0.0029 - acc: 0.9998 - mean_absolute_error: 0.0153 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9990\n",
      "Epoch 12/15\n",
      "23903/23903 [==============================] - 4s 147us/step - loss: 0.0027 - acc: 0.9999 - mean_absolute_error: 0.0157 - categorical_accuracy: 0.9999 - exact_match_accuracy: 0.9990\n",
      "Epoch 13/15\n",
      "23903/23903 [==============================] - 3s 141us/step - loss: 0.0026 - acc: 0.9998 - mean_absolute_error: 0.0159 - categorical_accuracy: 0.9998 - exact_match_accuracy: 0.9990\n",
      "Epoch 14/15\n",
      "23903/23903 [==============================] - 4s 148us/step - loss: 0.0023 - acc: 0.9999 - mean_absolute_error: 0.0161 - categorical_accuracy: 0.9999 - exact_match_accuracy: 0.9992\n",
      "Epoch 15/15\n",
      "23903/23903 [==============================] - 3s 141us/step - loss: 0.0022 - acc: 0.9999 - mean_absolute_error: 0.0164 - categorical_accuracy: 0.9999 - exact_match_accuracy: 0.9990\n",
      "0. '  'GSC008014TA'  'GSC008014TA'  'GSC008014TA'  'True\n",
      "1. '  'QR015643   '  'QR015643   '  'QR015643   '  'True\n",
      "2. '  'SDU02275TA '  'SDU02275TA '  'SDU02275TA '  'True\n",
      "\n",
      "23903/23903 [==============================] - 1s 43us/step\n",
      "('categorical_crossentropy', 0.0020680119882663904)\n",
      "('accuracy', 0.9998843364574338)\n",
      "('mean_absolute_error', 0.016542927475444856)\n",
      "('categorical_accuracy', 0.9998843364574338)\n",
      "(<function exact_match_accuracy at 0x000001C9A6F30378>, 0.99933062795465)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['GSQBarcode_Embedding'] = WordEmbedding('GSQBarcode', 0.15, 8, 0.01)\n",
    "embeddings['GSQBarcode_Embedding'].train(15, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### LookupDOSFilePath_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 63, 13, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 63, 130)           0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 63, 20)            2620      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 63, 130)           2730      \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 63, 13, 10)        0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 63, 13, 53)        583       \n",
      "=================================================================\n",
      "Total params: 6,473\n",
      "Trainable params: 6,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "23903/23903 [==============================] - 43s 2ms/step - loss: 1.3612 - acc: 0.6727 - mean_absolute_error: 0.0856 - categorical_accuracy: 0.6727 - exact_match_accuracy: 0.4687\n",
      "Epoch 2/15\n",
      "23903/23903 [==============================] - 41s 2ms/step - loss: 0.1185 - acc: 0.9767 - mean_absolute_error: 0.0066 - categorical_accuracy: 0.9767 - exact_match_accuracy: 0.8692\n",
      "Epoch 3/15\n",
      "23903/23903 [==============================] - 42s 2ms/step - loss: 0.0463 - acc: 0.9914 - mean_absolute_error: 0.0087 - categorical_accuracy: 0.9914 - exact_match_accuracy: 0.9411: 2s - loss: 0.0470 - acc: 0.9913 - mean_absolute_error: 0.0087 - categor\n",
      "Epoch 4/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0322 - acc: 0.9943 - mean_absolute_error: 0.0087 - categorical_accuracy: 0.9943 - exact_match_accuracy: 0.9570\n",
      "Epoch 5/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0261 - acc: 0.9955 - mean_absolute_error: 0.0087 - categorical_accuracy: 0.9955 - exact_match_accuracy: 0.9652: 6s - loss\n",
      "Epoch 6/15\n",
      "23903/23903 [==============================] - 39s 2ms/step - loss: 0.0228 - acc: 0.9962 - mean_absolute_error: 0.0087 - categorical_accuracy: 0.9962 - exact_match_accuracy: 0.9700\n",
      "Epoch 7/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0205 - acc: 0.9966 - mean_absolute_error: 0.0087 - categorical_accuracy: 0.9966 - exact_match_accuracy: 0.9735\n",
      "Epoch 8/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0190 - acc: 0.9969 - mean_absolute_error: 0.0088 - categorical_accuracy: 0.9969 - exact_match_accuracy: 0.9753\n",
      "Epoch 9/15\n",
      "23903/23903 [==============================] - 41s 2ms/step - loss: 0.0178 - acc: 0.9971 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9971 - exact_match_accuracy: 0.9769: 3s - loss: 0.0179 - acc: 0.9971 - mean_absolute_error: 0.0089 - ca\n",
      "Epoch 10/15\n",
      "23903/23903 [==============================] - 41s 2ms/step - loss: 0.0169 - acc: 0.9973 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9973 - exact_match_accuracy: 0.9783\n",
      "Epoch 11/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0160 - acc: 0.9974 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9974 - exact_match_accuracy: 0.9792: 6\n",
      "Epoch 12/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0153 - acc: 0.9976 - mean_absolute_error: 0.0096 - categorical_accuracy: 0.9976 - exact_match_accuracy: 0.9802\n",
      "Epoch 13/15\n",
      "23903/23903 [==============================] - 39s 2ms/step - loss: 0.0147 - acc: 0.9977 - mean_absolute_error: 0.0098 - categorical_accuracy: 0.9977 - exact_match_accuracy: 0.9814\n",
      "Epoch 14/15\n",
      "23903/23903 [==============================] - 40s 2ms/step - loss: 0.0142 - acc: 0.9978 - mean_absolute_error: 0.0100 - categorical_accuracy: 0.9978 - exact_match_accuracy: 0.9819\n",
      "Epoch 15/15\n",
      "23903/23903 [==============================] - 41s 2ms/step - loss: 0.0137 - acc: 0.9979 - mean_absolute_error: 0.0102 - categorical_accuracy: 0.9979 - exact_match_accuracy: 0.9824\n",
      "0. '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ WATERLOO \\ SEGY \\ WATERLOO _ LW88 - 3 _ STACK _ QR020802 _ 176832 . SGY                                '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ WATERLOO \\ SEGY \\ WATERLOO _ LW88 - 3 _ STACK _ QR020802 _ 176832 . SGY                                '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ WATERLIO \\ SEGY \\ WATERLIO _ LW88 - 3 _ STACK _ QR020802 _ 176872 . SGY                                '  'False\n",
      "1. '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ NIELLA _ AND _ EXTENSION \\ SEGY \\ NIELLA _ AND _ EXTENSION _ 85 - N60 _ FINAL _ QR008444 _ 173144 . SGY'  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ NIELLA _ AND _ EXTENSION \\ SEGY \\ NIELLA _ AND _ EXTENSION _ 85 - N60 _ FINAL _ QR008444 _ 173144 . SGY'  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ NIELLA _ AND _ EXTENSION \\ SEGY \\ NIELLA _ AND _ EXTENSION _ 85 - N60 _ FINAL _ QR008444 _ 173144 . SGY'  'True\n",
      "2. '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ SOUTH _ DENISON \\ SEGY \\ SOUTH _ DENISON _ 84 - M423 _ PROCESSED _ SDU04055TA _ 192904 . SGY           '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ SOUTH _ DENISON \\ SEGY \\ SOUTH _ DENISON _ 84 - M423 _ PROCESSED _ SDU04055TA _ 192904 . SGY           '  '\\ SHUP \\ 2D _ SURVEYS \\ PROCESSED _ AND _ SUPPORT _ DATA \\ 1980 \\ SOUTH _ DENISON \\ SEGY \\ SOUTH _ DENISON _ 84 - M823 _ PROCESSED _ SDU04055TA _ 192904 . SGY           '  'False\n",
      "\n",
      "23903/23903 [==============================] - 15s 622us/step\n",
      "('categorical_crossentropy', 0.013458458585095387)\n",
      "('accuracy', 0.9979219512775274)\n",
      "('mean_absolute_error', 0.010368780278513203)\n",
      "('categorical_accuracy', 0.9979219512775274)\n",
      "(<function exact_match_accuracy at 0x000001C9A6F30378>, 0.982830743385609)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['LookupDOSFilePath_Words_Embedding'] = WordEmbedding('LookupDOSFilePath_Words', 0.15, 8, 0.01)\n",
    "embeddings['LookupDOSFilePath_Words_Embedding'].train(15, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LSTM-RNN3: Input, Embed Characters, Embed Words, (LSTM), De-embed Words, De-embed Characters, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 63, 13, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 63, 130)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 63, 20)            2620      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 105)               52920     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 35)                3710      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 230)               8280      \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 23, 10)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 23, 53)            583       \n",
      "=================================================================\n",
      "Total params: 68,653\n",
      "Trainable params: 68,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN3'\n",
    "\n",
    "# selected datasets\n",
    "x_name, y_name = 'LookupDOSFilePath_Words', 'LineName'\n",
    "cuts = {} # {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "# corresponding embeddings\n",
    "x_embed = embeddings[x_name+'_Embedding']\n",
    "y_embed = embeddings[y_name+'_Embedding']\n",
    "\n",
    "\n",
    "lstm_hidden_size = int(y_embed.word_embedding_size * 3)\n",
    "\n",
    "\n",
    "# character embedding\n",
    "l_encode_character = keras.layers.Dense(\n",
    "    x_embed.character_embedding_size, \n",
    "    input_shape=(x_word_count, x_char_count, x_ones_count), \n",
    "    kernel_constraint=keras.constraints.max_norm(10.)\n",
    ")\n",
    "\n",
    "# word embedding\n",
    "l_char_to_word = keras.layers.Reshape(\n",
    "    (x_word_count, x_char_count * x_embed.character_embedding_size,)\n",
    ")\n",
    "l_encode_word = keras.layers.Dense(\n",
    "    x_embed.word_embedding_size, \n",
    "    kernel_constraint=keras.constraints.max_norm(10.)\n",
    ")\n",
    "\n",
    "# lstm processing\n",
    "l_lstm = keras.layers.LSTM(\n",
    "    lstm_hidden_size,\n",
    "    activation='sigmoid', \n",
    "    implementation=2, \n",
    "    unroll=True, \n",
    "    kernel_constraint=keras.constraints.max_norm(10.)\n",
    ")\n",
    "l_decode = keras.layers.Dense(\n",
    "    y_embed.word_embedding_size,\n",
    "    activation='sigmoid', \n",
    "    kernel_constraint=keras.constraints.max_norm(10.)\n",
    ")\n",
    "\n",
    "# word de-embedding\n",
    "l_decode_word = keras.layers.Dense(\n",
    "    y_char_count * y_embed.character_embedding_size,\n",
    "    activation='sigmoid', \n",
    "    kernel_constraint=keras.constraints.max_norm(10.)\n",
    ")\n",
    "l_word_to_char = keras.layers.Reshape(\n",
    "    (y_char_count, character_embedding_size)\n",
    ")\n",
    "\n",
    "# character de-embedding\n",
    "l_decode_character = keras.layers.Dense(\n",
    "    y_ones_count,\n",
    "    activation='sigmoid',\n",
    "    kernel_constraint=keras.constraints.max_norm(10.)\n",
    ")\n",
    "\n",
    "\n",
    "# model definition\n",
    "models[architecture] = keras.Sequential([\n",
    "    l_encode_character,\n",
    "    l_char_to_word,\n",
    "    l_encode_word,\n",
    "    l_lstm,\n",
    "    l_decode,\n",
    "    l_decode_word,\n",
    "    l_word_to_char,\n",
    "    l_decode_character,\n",
    "])\n",
    "\n",
    "# compile model\n",
    "models[architecture].compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "print(models[architecture].summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "x_embed.apply_encode(l_encode_character, l_encode_word)\n",
    "y_embed.apply_decode(l_decode_word, l_decode_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# architecture = 'GRU-RNN3'\n",
    "\n",
    "# x_name, y_name = 'FileName_Words', 'SurveyName'\n",
    "# cuts = {} # {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "# (x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "# x_embed = embeddings[x_name+'_Embedding']\n",
    "# y_embed = embeddings[y_name+'_Embedding']\n",
    "\n",
    "# gru_hidden_size = int(y_embed.word_embedding_size * 3)\n",
    "\n",
    "\n",
    "# # character embedding\n",
    "# l_encode_character = keras.layers.Dense(   x_embed.character_embedding_size, input_shape=(x_word_count, x_char_count, x_ones_count))\n",
    "\n",
    "# # word embedding\n",
    "# l_char_to_word =     keras.layers.Reshape( (x_word_count, x_char_count * x_embed.character_embedding_size,))\n",
    "# l_encode_word =      keras.layers.Dense(   x_embed.word_embedding_size)\n",
    "\n",
    "# # lstm processing\n",
    "# l_lstm =             keras.layers.GRU(     gru_hidden_size,   activation='sigmoid',   implementation=2,   unroll=True)\n",
    "# l_decode =           keras.layers.Dense(   y_embed.word_embedding_size,   activation='sigmoid')\n",
    "\n",
    "# l_decode_word =      keras.layers.Dense(   y_char_count * y_embed.character_embedding_size,   activation='sigmoid')\n",
    "# l_word_to_char =     keras.layers.Reshape( (y_char_count, character_embedding_size))\n",
    "\n",
    "# l_decode_character = keras.layers.Dense(   y_ones_count,   activation='sigmoid')\n",
    "\n",
    "\n",
    "# models[architecture] = keras.Sequential([\n",
    "#     l_encode_character,\n",
    "#     l_char_to_word,\n",
    "#     l_encode_word,\n",
    "#     l_lstm,\n",
    "#     l_decode,\n",
    "#     l_decode_word,\n",
    "#     l_word_to_char,\n",
    "#     l_decode_character,\n",
    "# ])\n",
    "\n",
    "# models[architecture].compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "# print(models[architecture].summary())\n",
    "\n",
    "# # set pretrained embedding weights\n",
    "# x_embed.apply_encode(l_encode_character, l_encode_word)\n",
    "# y_embed.apply_decode(l_decode_word, l_decode_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save/Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DE = models['E-D-NN'].get_weights()\n",
    "#model_GRU_1 = model\n",
    "#model_GRU_2 = model\n",
    "#model_GRU_3 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.set_weights(GRU)\n",
    "#model = model_GRU_3\n",
    "#models['E-D-NN'].set_weights(DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Run and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size):\n",
    "    \"\"\"Peroform training on a model\n",
    "    \n",
    "    # Arguments\n",
    "        model: the model to train\n",
    "        epochs: number of epochs to train for\n",
    "        batch_size: batch size for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # split and shuffle data\n",
    "    ((x_train, x_test, x_preview), (y_train, y_test, y_preview)) = split_and_shuffle(x_onehot, y_onehot, sizes=(20000, 1000, 15))\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    # test\n",
    "    l, a = test(model, x_test, y_test, x_preview, y_preview)\n",
    "    \n",
    "    # show training history\n",
    "    training_log(x_name, y_name, architecture, batch_size, epochs, l, a)\n",
    "    print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20000/20000 [==============================] - 39s 2ms/step - loss: 1.0432 - mean_absolute_error: 0.0186 - categorical_accuracy: 0.7701 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.8937 - mean_absolute_error: 0.0181 - categorical_accuracy: 0.7850 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.7477 - mean_absolute_error: 0.0178 - categorical_accuracy: 0.8017 - exact_match_accuracy: 0.0000e+00: 4s - loss: 0.7536 - mean_absolute_error: 0.0178 - categorical_accuracy: 0.8004 - exact_match_accuracy: 0. - ETA: 4s - loss: 0.7527 - mean_absolute_error: 0.0178 - categorical_accur\n",
      "Epoch 4/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.6826 - mean_absolute_error: 0.0176 - categorical_accuracy: 0.8163 - exact_match_accuracy: 1.5000e-04\n",
      "Epoch 5/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.6263 - mean_absolute_error: 0.0173 - categorical_accuracy: 0.8281 - exact_match_accuracy: 2.0000e-04: 1s - loss: 0.6280 - mean_absolute_error: 0.0173 - categorical_accuracy: 0.8278 - exact_match_accurac\n",
      "Epoch 6/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.5843 - mean_absolute_error: 0.0171 - categorical_accuracy: 0.8388 - exact_match_accuracy: 5.5000e-04: 7s - loss: 0.5885 - mean_absolute_error: 0.0171 - categorical_accuracy: - ETA: 3s - loss: 0.5870 - mean_absolute_error: 0.0171 - categorical_accuracy: 0\n",
      "Epoch 7/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.5516 - mean_absolute_error: 0.0170 - categorical_accuracy: 0.8475 - exact_match_accuracy: 7.0000e-04: 11s - loss: 0.5555 - me - ETA: 3s - loss: 0.5533 - mean_absolute_error: 0.0170 - categorical_accuracy\n",
      "Epoch 8/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.5249 - mean_absolute_error: 0.0168 - categorical_accuracy: 0.8548 - exact_match_accuracy: 0.0023: 13s - loss: 0.5268 - me - ETA: 9s - loss: 0.5272 - mean_absolute_error: 0.0168 - categorical_accuracy: 0. - ETA: 6s - loss: 0.5255 - mean_absolute_error: 0.0168 - categorical_accur - ETA: 2s - loss: 0.5246 - mean_absolute_error: 0.0168 - categorical_accuracy: 0.8549 - ex\n",
      "Epoch 9/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.4988 - mean_absolute_error: 0.0166 - categorical_accuracy: 0.8611 - exact_match_accuracy: 0.0032: 4s - loss: 0.5003 - mean_absolute_error: 0.0166 - categorical_accuracy: 0.8608 - exact_ma - ETA: 2s - loss: 0.4995 - mean_absolute_error: 0.0166 - categorical_accuracy: 0.8609 \n",
      "Epoch 10/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.4742 - mean_absolute_error: 0.0164 - categorical_accuracy: 0.8674 - exact_match_accuracy: 0.0038: 11s - loss: 0.4798 - m - ETA: 3s - loss: 0.4758 - mean_absolute_error: 0.0164 - categorical_accuracy: 0.8668 - exact_match_accuracy: 0.003 - ETA: 3s - loss: 0.4757 - mean_absolute_error: 0.0164 - categorical_accuracy: \n",
      "Epoch 11/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.4523 - mean_absolute_error: 0.0162 - categorical_accuracy: 0.8733 - exact_match_accuracy: 0.0067: 4s - loss: 0.4542 - mean_absolute_error: 0.0162 - categorical_a\n",
      "Epoch 12/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.4319 - mean_absolute_error: 0.0159 - categorical_accuracy: 0.8791 - exact_match_accuracy: 0.0094: 8s - loss: 0.4348 - mean_absolute_error: 0.0159 -  - ETA: 2s - loss: 0.4322 - mean_absolute_error: 0.0159 - categorical_accuracy: 0.8790 - e\n",
      "Epoch 13/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.4125 - mean_absolute_error: 0.0156 - categorical_accuracy: 0.8844 - exact_match_accuracy: 0.0127: 3s - loss: 0.4126 - mean_absolute_error: 0.0156 - categorical_accuracy: 0.884 - ETA: 0s - loss: 0.4124 - mean_absolute_error: 0.0156 - categorical_accuracy: 0.8844 - exact_match_accuracy: 0\n",
      "Epoch 14/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3947 - mean_absolute_error: 0.0154 - categorical_accuracy: 0.8894 - exact_match_accuracy: 0.0168: 10s - loss: 0.3979 - mean_absolute_e - ETA: 5s - loss: 0.3953 - mean_absolute_error: 0.0154 - categorical_accuracy: 0.8892 - ex - ETA: 2s - loss: 0.3950 - mean_absolute_error: 0.0154 - categorical_accuracy: 0.8893 \n",
      "Epoch 15/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3780 - mean_absolute_error: 0.0151 - categorical_accuracy: 0.8936 - exact_match_accuracy: 0.0203: 13s - loss: 0.3810 - mean_absolute_error:\n",
      "Epoch 16/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3630 - mean_absolute_error: 0.0148 - categorical_accuracy: 0.8978 - exact_match_accuracy: 0.0278: 11s - loss: 0.3674 - mean_absolute_error: 0.0148 - categorical_accuracy: 0.8965 - exact_match_accuracy: 0. - ET - ETA: 2s - loss: 0.3632 - mean_absolute_error: 0.0148 - categorical_accuracy: 0.8977 - exac\n",
      "Epoch 17/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3490 - mean_absolute_error: 0.0145 - categorical_accuracy: 0.9015 - exact_match_accuracy: 0.0349: 7s - loss: 0.3506 - mean_absolute_error: 0.0146 - categorical_accuracy: 0.9010 - ETA: 4s - loss: 0.3507 - mean_absolute_error: 0.0145 - categorical_accuracy: 0.9011 - exact_match_accu - ETA: 3s - loss: 0.3502 - mean_absolute_error: 0.0145 - categorical_accuracy: 0.9012 - exact_match_accuracy: 0. - ETA: 3s - loss: 0.3503 - mean_absolute_error: 0.0145 - categorical_accuracy: 0\n",
      "Epoch 18/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3365 - mean_absolute_error: 0.0142 - categorical_accuracy: 0.9049 - exact_match_accuracy: 0.0413: 11s - loss - ETA: 3s - loss: 0.3378 - mean_absolute_error: 0.0143 - categorical_accura\n",
      "Epoch 19/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3253 - mean_absolute_error: 0.0140 - categorical_accuracy: 0.9084 - exact_match_accuracy: 0.0519: 24s - loss: 0.3349 -  - ETA: 9s - loss: 0.3288 - mean_absolute_erro - ETA: 3s - loss: 0.3252 - mean_absolute_error: 0.0140 - categorical_accuracy: 0.\n",
      "Epoch 20/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3140 - mean_absolute_error: 0.0138 - categorical_accuracy: 0.9114 - exact_match_accuracy: 0.0585: 5s - loss: 0.3151 - mean_absolute_error: 0.0138 - categorical_accuracy: 0.9111 - exact_ - ETA: 3s - loss: 0.3147 - mean_absolute_error: 0.0138 - categorical_accur\n",
      "Epoch 21/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.3031 - mean_absolute_error: 0.0136 - categorical_accuracy: 0.9148 - exact_match_accuracy: 0.0701\n",
      "Epoch 22/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2940 - mean_absolute_error: 0.0134 - categorical_accuracy: 0.9178 - exact_match_accuracy: 0.0797\n",
      "Epoch 23/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2843 - mean_absolute_error: 0.0132 - categorical_accuracy: 0.9207 - exact_match_accuracy: 0.0945: 7s - loss: 0.2856 - mean_absolute_error - ETA: 0s - loss: 0.2844 - mean_absolute_error: 0.0132 - categorical_accuracy: 0.9206 - exact_match_accur\n",
      "Epoch 24/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2758 - mean_absolute_error: 0.0130 - categorical_accuracy: 0.9236 - exact_match_accuracy: 0.1082\n",
      "Epoch 25/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2663 - mean_absolute_error: 0.0129 - categorical_accuracy: 0.9270 - exact_match_accuracy: 0.1286: 5s - loss: 0.2673 - mean_absolute_error: 0.0129 - categ\n",
      "Epoch 26/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2583 - mean_absolute_error: 0.0127 - categorical_accuracy: 0.9294 - exact_match_accuracy: 0.1457: 9s - loss: 0.2590 - me - ETA: 1s - loss: 0.2585 - mean_absolute_error: 0.0127 - categorical_accuracy: 0.9295 - exact_match_\n",
      "Epoch 27/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2497 - mean_absolute_error: 0.0125 - categorical_accuracy: 0.9326 - exact_match_accuracy: 0.1652: 6s - loss: 0.2511 - mean_absolute_error: \n",
      "Epoch 28/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2420 - mean_absolute_error: 0.0124 - categorical_accuracy: 0.9352 - exact_match_accuracy: 0.1843\n",
      "Epoch 29/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2348 - mean_absolute_error: 0.0121 - categorical_accuracy: 0.9376 - exact_match_accuracy: 0.2070: 4s - loss: 0.2348 - mean_absolute_error: 0.0121 - categor\n",
      "Epoch 30/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2277 - mean_absolute_error: 0.0120 - categorical_accuracy: 0.9402 - exact_match_accuracy: 0.2294\n",
      "Epoch 31/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.2210 - mean_absolute_error: 0.0118 - categorical_accuracy: 0.9427 - exact_match_accuracy: 0.2520: 11s - loss: 0.2242 - mean_absolute_error: 0.0 - ETA: 6s - loss: 0.2219 - mean_absolute_error: 0.0\n",
      "Epoch 32/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2142 - mean_absolute_error: 0.0117 - categorical_accuracy: 0.9449 - exact_match_accuracy: 0.2732: 10s - loss: 0.2148 - mean_absolute_error: 0. - ETA: 5s - loss: 0.2148 - mean_absolute_error: 0.0117 - \n",
      "Epoch 33/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2074 - mean_absolute_error: 0.0116 - categorical_accuracy: 0.9474 - exact_match_accuracy: 0.3014\n",
      "Epoch 34/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.2019 - mean_absolute_error: 0.0114 - categorical_accuracy: 0.9494 - exact_match_accuracy: 0.3226\n",
      "Epoch 35/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1950 - mean_absolute_error: 0.0112 - categorical_accuracy: 0.9519 - exact_match_accuracy: 0.3475: 9s - loss: 0.1963 - mean_absolute_error: 0.0112 - categorical_accuracy: 0. - ETA: 5s - loss: 0.1958 - mean_absolute_error: 0.0112 \n",
      "Epoch 36/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1896 - mean_absolute_error: 0.0111 - categorical_accuracy: 0.9537 - exact_match_accuracy: 0.3695: 20s - loss: 0.19 - ETA: 10s - loss: 0.1889 - mean_absolute_error: 0.0111 - categorical_accuracy: 0.9537 - exact_match_accu - ETA: 9s - loss: 0.1894 - mea - ETA: 1s - loss: 0.1894 - mean_absolute_error: 0.0111 - categorical_accuracy: 0.9537 - exact_match_\n",
      "Epoch 37/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1842 - mean_absolute_error: 0.0109 - categorical_accuracy: 0.9554 - exact_match_accuracy: 0.3875: 26s - loss: 0.1851 - mean_ - ETA: 17s - loss: 0.1854 - mean_absolute_error: 0.0110 - categorical_accuracy: 0.9548 - exact_matc - ETA: 16s - - ETA: 2s - loss: 0.1842 - mean_absolute_error: 0.0109 - categorical_accuracy: 0.9554 - exa\n",
      "Epoch 38/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1779 - mean_absolute_error: 0.0107 - categorical_accuracy: 0.9574 - exact_match_accuracy: 0.4117\n",
      "Epoch 39/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1731 - mean_absolute_error: 0.0106 - categorical_accuracy: 0.9592 - exact_match_accuracy: 0.4352: 3s - loss: 0.1725 - mean_absolute_error: 0.0107 - categorical_acc\n",
      "Epoch 40/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1685 - mean_absolute_error: 0.0104 - categorical_accuracy: 0.9604 - exact_match_accuracy: 0.4497: 4s - loss: 0.1688 - mean_absolute_error: 0.0104 - categorical_acc\n",
      "Epoch 41/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1639 - mean_absolute_error: 0.0103 - categorical_accuracy: 0.9618 - exact_match_accuracy: 0.4677: 4s - loss: 0.1647 - mean_absolute_error: 0.0103 - categorical_accuracy: 0.9616 - - ETA: 1s - loss: 0.1641 - mean_absolute_error: 0.0103 - categorical_accuracy: 0.9618 - exact_match_ - ETA: 0s - loss: 0.1639 - mean_absolute_error: 0.0103 - categorical_accuracy: 0.9618 - exact_match_accuracy: 0.46\n",
      "Epoch 42/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1591 - mean_absolute_error: 0.0102 - categorical_accuracy: 0.9634 - exact_match_accuracy: 0.4864: 11s - loss: 0.1608 - mean_absolu - ETA: 6s - loss: 0.1598 - mean_absolute_error: \n",
      "Epoch 43/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1543 - mean_absolute_error: 0.0101 - categorical_accuracy: 0.9647 - exact_match_accuracy: 0.5020: 1s - loss: 0.1546 - mean_absolute_error: 0.0101 - categorical_accuracy: 0.9646 - exact_m\n",
      "Epoch 44/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1507 - mean_absolute_error: 0.0099 - categorical_accuracy: 0.9658 - exact_match_accuracy: 0.5215: 6s - loss: 0.1516 - mean_absolute_error\n",
      "Epoch 45/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1461 - mean_absolute_error: 0.0098 - categorical_accuracy: 0.9671 - exact_match_accuracy: 0.5332: 6s - loss: 0.1449 - mean_absolute_error: 0.0098 - categorical_accuracy: 0.9675 - exact_match_accuracy: 0.534 - ETA: 6s - loss: 0.1448 - mean_absolute_error: 0.\n",
      "Epoch 46/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1423 - mean_absolute_error: 0.0097 - categorical_accuracy: 0.9683 - exact_match_accuracy: 0.5510: 3s - loss: 0.1421 - mean_absolute_error: 0.0097 - categorical_accuracy: 0.9\n",
      "Epoch 47/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1392 - mean_absolute_error: 0.0096 - categorical_accuracy: 0.9690 - exact_match_accuracy: 0.5612: 28s - loss: 0.1395 - mean_absolute_error: 0.0097 - categorical_accuracy: 0.9657 - exact_match_accu - ETA: 28s - loss: 0 - ETA: 4s - loss: 0.1392 - mean_absolute_error: 0.0096 - categorical\n",
      "Epoch 48/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1343 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9705 - exact_match_accuracy: 0.5787: 14s - loss: 0.1363 - mean_\n",
      "Epoch 49/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1316 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9716 - exact_match_accuracy: 0.5924\n",
      "Epoch 50/200\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.1277 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9724 - exact_match_accuracy: 0.6012\n",
      "Epoch 51/200\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1249 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9732 - exact_match_accuracy: 0.6112\n",
      "Epoch 52/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1224 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9739 - exact_match_accuracy: 0.6222\n",
      "Epoch 53/200\n",
      "20000/20000 [==============================] - 30s 2ms/step - loss: 0.1192 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9746 - exact_match_accuracy: 0.6279: 8s - loss: 0.1184 - m\n",
      "Epoch 54/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.1164 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9755 - exact_match_accuracy: 0.6399\n",
      "Epoch 55/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.1133 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9764 - exact_match_accuracy: 0.6511: 17s - loss: 0.1131 - mean_absolute_error: 0. - ETA: 9s - loss\n",
      "Epoch 56/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.1109 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9773 - exact_match_accuracy: 0.6637\n",
      "Epoch 57/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.1082 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9781 - exact_match_accuracy: 0.6723\n",
      "Epoch 58/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.1060 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9786 - exact_match_accuracy: 0.6821: 1s - loss: 0.1061 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9786 - exact_match_accur - ETA: 0s - loss: 0.1062 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9786 - exact_match_accuracy:\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.1031 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9795 - exact_match_accuracy: 0.6929: 17s - loss: 0.1010 - mean_absolute_error: 0.0090 - categoric - ETA: 14s - loss: 0.1020 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9799 - exact_match_accuracy:  - ETA: 14s - loss: 0.1023\n",
      "Epoch 60/200\n",
      "20000/20000 [==============================] - 30s 1ms/step - loss: 0.1013 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9798 - exact_match_accuracy: 0.6978\n",
      "Epoch 61/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0994 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9805 - exact_match_accuracy: 0.7068\n",
      "Epoch 62/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0968 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9812 - exact_match_accuracy: 0.7160: 0s - loss: 0.0968 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9812 - exact_match_accuracy: 0.7\n",
      "Epoch 63/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0955 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9815 - exact_match_accuracy: 0.7199: 27s - loss: 0.0994 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9808 - exact_match_ac - ETA: 26s - loss: 0.1017 - mean_absolute_error: 0.0090 - categoric - ETA: 7s - loss: 0.0953 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9813 - exa - ETA: 4s - loss: 0.0958 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9813 - exact_match_accuracy: 0.7 - ETA: 4s - loss: 0.0958 - mean_absolute_error: 0.0090 - catego\n",
      "Epoch 64/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0924 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9825 - exact_match_accuracy: 0.7343: 2s - loss: 0.0929 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9824 - exact_match_ - ETA: 1s - loss: 0.0923 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9825 - exact_match_accu\n",
      "Epoch 65/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0904 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9830 - exact_match_accuracy: 0.7409: 4s - loss: 0.0908 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9828 - exact - ETA: 2s - loss: 0.0909 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9828 -\n",
      "Epoch 66/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0884 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9836 - exact_match_accuracy: 0.7490\n",
      "Epoch 67/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0874 - mean_absolute_error: 0.0089 - categorical_accuracy: 0.9838 - exact_match_accuracy: 0.7536\n",
      "Epoch 68/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0852 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9844 - exact_match_accuracy: 0.7607:  - ETA: 12s - loss: 0.0865 - mean_absolute_err - ETA: 7s - loss: 0.0858 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9843 - exact_match_accuracy:  - ETA: 7s - loss: 0.0859 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9843 - exact_m - ETA: 5s - loss: 0.0856 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9844 - exact_match - ETA: 4s - loss: 0.0853 - mean_absolute_error: 0.0090 - categorical_accuracy - ETA: 0s - loss: 0.0852 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9844 - exact_match_accuracy: 0.7 - ETA: 0s - loss: 0.0852 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9844 - exact_match_accuracy: 0.\n",
      "Epoch 69/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0838 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9847 - exact_match_accuracy: 0.7664: 20s - loss: 0.0796 - mean_absolute_error: 0.0090 - categorical - ETA: 17s - loss: 0.0813 - mean_absolute_e - ETA: 9s - los\n",
      "Epoch 70/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0821 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9851 - exact_match_accuracy: 0.7719: 14s - loss: 0.0815 - mean_absolute_error: 0.0 - ETA: 3s - loss: 0.0820 - mean_absolute_error: 0.0090 - categorical_accura\n",
      "Epoch 71/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0804 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9857 - exact_match_accuracy: 0.7801: 21s - loss: 0.0788 - ETA: 17s  - ETA: 12s - loss - ETA: 6s - loss: 0.0798 - mean_absolute\n",
      "Epoch 72/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0791 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9859 - exact_match_accuracy: 0.7819: 23s - loss: 0.0822 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9853 -  - ETA: 22s - loss: 0.0835 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9850 - exact_ - ETA: 21s - loss: 0.0824 - mean_absolute_error: 0.0091 - categ - ETA: 18s - loss: 0.0807 - mean_absolute_erro\n",
      "Epoch 73/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0779 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9863 - exact_match_accuracy: 0.7901\n",
      "Epoch 74/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0758 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9869 - exact_match_accuracy: 0.7955\n",
      "Epoch 75/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0750 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9871 - exact_match_accuracy: 0.8009: 3s - loss: 0.0748 - mean_absolute_error: 0.0091 - categorical_accuracy: \n",
      "Epoch 76/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0732 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9875 - exact_match_accuracy: 0.8051\n",
      "Epoch 77/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0718 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9880 - exact_match_accuracy: 0.8121\n",
      "Epoch 78/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0710 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9880 - exact_match_accuracy: 0.8145: 7s - loss: 0.0716 - mean_absolute_error: 0 - ETA: 1s - loss: 0.0710 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9880 - exact_match_ac\n",
      "Epoch 79/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0695 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9884 - exact_match_accuracy: 0.8190: 0s - loss: 0.0694 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9885 - exact_match_accuracy: 0.81\n",
      "Epoch 80/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0691 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9885 - exact_match_accuracy: 0.8216\n",
      "Epoch 81/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0682 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9887 - exact_match_accuracy: 0.8237: 9s - loss: 0.0684 - mean_absolute_error: 0.0 - ETA: 3s - loss: 0.0686 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9886 - exact_match_accuracy: 0.822 - ETA: 3s - loss: 0.0686 - mean_absolute_error: 0.0092 - categorical_accura\n",
      "Epoch 82/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0659 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9893 - exact_match_accuracy: 0.8342: 9s - loss: 0.0677 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9890 - exact_match_accura - ETA: 8s - loss: 0.0676 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9890 - exact_match_a - ETA: 6s - loss: 0.0675 - mean_absolute_error: 0.0092 - categorical_accuracy - ETA: 3s - loss: 0.0663 - mean_absolute_error: 0.0092 - categorical_accuracy\n",
      "Epoch 83/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0659 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9893 - exact_match_accuracy: 0.8342: 1s - loss: 0.0658 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9893 - exact_ma\n",
      "Epoch 84/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0645 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9897 - exact_match_accuracy: 0.8380: 7s - loss: 0.0648 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9895 - - ETA: 4s - loss: 0.0644 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9897 - exact_match - ETA: 3s - loss: 0.0641 - mean_absolute_error: 0.0093 - categorical_accuracy:\n",
      "Epoch 85/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0625 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9901 - exact_match_accuracy: 0.8457: 1s - loss: 0.0626 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9901 - exact_m\n",
      "Epoch 86/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0628 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9900 - exact_match_accuracy: 0.8443\n",
      "Epoch 87/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0619 - mean_absolute_error: 0.0093 - categorical_accuracy: 0.9902 - exact_match_accuracy: 0.8493\n",
      "Epoch 88/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0600 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9908 - exact_match_accuracy: 0.8569 ETA: 8s - loss: 0.05\n",
      "Epoch 89/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0592 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9910 - exact_match_accuracy: 0.8593: 0s - loss: 0.0592 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9910 - exact_match_accurac\n",
      "Epoch 90/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0581 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9912 - exact_match_accuracy: 0.8617: 7s - loss: 0.0579 - mean_\n",
      "Epoch 91/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0573 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9913 - exact_match_accuracy: 0.8641: 3s - loss: 0.0565 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9915 - - ETA: 1s - loss: 0.0570 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9914 - exact_match_acc\n",
      "Epoch 92/200\n",
      "20000/20000 [==============================] - 28s 1ms/step - loss: 0.0569 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9913 - exact_match_accuracy: 0.8641: 6s - loss: 0.0576 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.99 - ETA: 3s - loss: 0.0574 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9912 - exact_mat - ETA: 2s - loss: 0.0574 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9912 - exact\n",
      "Epoch 93/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0562 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9916 - exact_match_accuracy: 0.8676: 8s - loss: 0.0562 - mean - ETA: 0s - loss: 0.0564 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9915 - exact_match_accur\n",
      "Epoch 94/200\n",
      "20000/20000 [==============================] - 29s 1ms/step - loss: 0.0553 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9916 - exact_match_accuracy: 0.8700: 1s - loss: 0.0554 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9916 - exact_m\n",
      "Epoch 95/200\n",
      " 4160/20000 [=====>........................] - ETA: 22s - loss: 0.0533 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9920 - exact_match_accuracy: 0.8748"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-4da8ca8b815d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LSTM-RNN3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-28dd5d0e30d1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs, batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chris\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(models['LSTM-RNN3'], 100, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
