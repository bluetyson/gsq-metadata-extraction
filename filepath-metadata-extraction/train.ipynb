{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Jupyter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'H:\\\\gsq-metadata-extraction\\\\filepath-metadata-extraction\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import preprocessing as pp\n",
    "import sys, inspect, argparse, importlib\n",
    "\n",
    "importlib.reload(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# percentage of samples that exactly match\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    argmax_true = tf.math.argmax(y_true, axis=2)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_pred = tf.math.argmax(y_pred, axis=2)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    match_char = tf.math.equal(argmax_true, argmax_pred)    # match characters              (batch, width:int) -> (batch, width:bool)\n",
    "    match_word = tf.math.reduce_all(match_char, axis=1)     # require all character in sample to match      (batch, width:bool) -> (batch:bool)\n",
    "    match_int = tf.cast(match_word, tf.float32)             # bool to int                                   (batch:bool) -> (batch:int)\n",
    "    return tf.reduce_mean(match_int)                        # percentage of samples that are an exact match (batch:int) -> int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def log(*l, **d): \n",
    "    if verbose: print(*l, **d)\n",
    "        \n",
    "training_history = []\n",
    "\n",
    "def training_log(x, y, a, b, e, l, m):\n",
    "    training_history.append({'x':x, 'y':y, 'architecture':a, 'batch size':b, 'epochs':e, 'loss':l, 'accuracy':m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pp.load('training_data.p')\n",
    "voc_size = pp.char_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x\t(4000, 87)\ttrain_y\t(4000, 23)\n",
      "train_x\t(4000, 87, 79)\ttrain_y\t(4000, 12, 79)\ttest_x\t(1000, 87, 79)\ttest_y\t(1000, 12, 79)\tshowcase_x\t(5, 87, 79)\tshowcase_y\t(5, 12, 79)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# Unique Record ID\tFileName\tOriginal_FileName\tSurveyNum\tSurveyName\tLineName\tSurveyType\tPrimaryDataType\tSecondaryDataType\tTertiaryDataType\tQuaternary\tFile_Range\n",
    "# First_SP_CDP\tLast_SP_CDP\tCompletionYear\tTenureType\tOperator Name\tGSQBarcode\tEnergySource\tLookupDOSFilePath\n",
    "\n",
    "train_x, train_y, test_x, test_y, showcase_x, showcase_y, x_shape_char, x_shape_ones, y_shape_char, y_shape_ones, x_name, y_name\\\n",
    "    = pp.preprocess(data, x_name='FileName', y_name='LineName', x_categorical=True, y_categorical= True, epoch_size=5000, x_cut_s=None, x_cut_e=None, y_cut_s=None, y_cut_e=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test and show samlpe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_x, test_y, showcase_x, showcase_y):\n",
    "    p_one_hot = model.predict(showcase_x)\n",
    "    p_vector = np.argmax(p_one_hot, 2)\n",
    "    p_strings = pp.decode_data(p_vector)\n",
    "\n",
    "    y_vector = np.argmax(showcase_y, 2)\n",
    "    y_strings = pp.decode_data(y_vector)\n",
    "\n",
    "    x_vector = np.argmax(showcase_x, 2)\n",
    "    x_strings = pp.decode_data(x_vector)\n",
    "\n",
    "    x_strings = [s.replace('<Padding>', '') for s in x_strings]\n",
    "    y_strings = [s.replace('<Padding>', '') for s in y_strings]\n",
    "    p_strings = [s.replace('<Padding>', '') for s in p_strings]\n",
    "    x_w, y_w, p_w = max([len(s) for s in x_strings]), max([len(s) for s in y_strings]), max([len(s) for s in p_strings])\n",
    "    y_p_strings = ['  '.join([x.ljust(x_w), y.ljust(y_w), p.ljust(p_w), str(y==p)]) for x, y, p in zip(x_strings, y_strings, p_strings)]\n",
    "\n",
    "    print(*y_p_strings, sep='\\n', end='\\n\\n')\n",
    "\n",
    "    # accuracy on entire training set\n",
    "    accuracies = model.evaluate(test_x, test_y)\n",
    "    print(*list(zip([loss]+metrics, accuracies)), sep='\\n', end='\\n\\n') # evaluate and list loss and each metric\n",
    "    \n",
    "    return accuracies[0], accuracies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 15\n",
    "character_embedding_size = 10\n",
    "architecture = ''\n",
    "\n",
    "metrics = ['mean_absolute_error', 'categorical_accuracy', 'binary_accuracy', exact_match_accuracy]\n",
    "loss = 'mean_squared_logarithmic_error' # poisson mean_squared_logarithmic_error categorical_crossentropy\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lh (Dense)                   (None, 10)                800       \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 79)                869       \n",
      "=================================================================\n",
      "Total params: 1,669\n",
      "Trainable params: 1,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Character-Embedding'\n",
    "\n",
    "model_E_D_NN = keras.Sequential()\n",
    "model_E_D_NN.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(voc_size,)))\n",
    "model_E_D_NN.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "#model_E_D_NN.add(keras.layers.Dropout(0.001))\n",
    "model_E_D_NN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', 'binary_accuracy'])\n",
    "models[architecture] = model_E_D_NN\n",
    "print(model_E_D_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "348000/348000 [==============================] - 26s 75us/step - loss: 0.0436 - acc: 0.9917 - mean_absolute_error: 0.0296 - categorical_accuracy: 0.7083 - binary_accuracy: 0.9917\n",
      "Epoch 2/5\n",
      "348000/348000 [==============================] - 9s 27us/step - loss: 7.3635e-04 - acc: 0.9999 - mean_absolute_error: 5.7665e-04 - categorical_accuracy: 0.9972 - binary_accuracy: 0.9999\n",
      "Epoch 3/5\n",
      "348000/348000 [==============================] - 10s 27us/step - loss: 5.0045e-05 - acc: 1.0000 - mean_absolute_error: 3.3886e-05 - categorical_accuracy: 0.9998 - binary_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "348000/348000 [==============================] - 11s 30us/step - loss: 1.1405e-05 - acc: 1.0000 - mean_absolute_error: 6.1701e-06 - categorical_accuracy: 0.9999 - binary_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "348000/348000 [==============================] - 11s 31us/step - loss: 4.9833e-06 - acc: 1.0000 - mean_absolute_error: 2.7186e-06 - categorical_accuracy: 0.9999 - binary_accuracy: 1.0000\n",
      "87000/87000 [==============================] - 8s 91us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.15132975557487e-05,\n",
       " 0.999995635460163,\n",
       " 5.570285267031094e-06,\n",
       " 0.9996551724137931,\n",
       " 0.999995635460163]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "model = models['Character-Embedding']\n",
    "\n",
    "model.fit(train_x.reshape(-1, voc_size), train_x.reshape(-1, voc_size), batch_size=batch_size, epochs=epochs)\n",
    "model.evaluate(test_x.reshape(-1, voc_size), test_x.reshape(-1, voc_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3858225 ,  1.0831605 , -1.2977322 , -0.1188067 ,  0.54794115,\n",
       "       -1.2280831 ,  0.78454804, -1.4279423 , -1.0548017 , -1.165022  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w1 = model.layers[0].get_weights()\n",
    "# w2 = model.layers[1].get_weights()\n",
    "\n",
    "# w = [np.copy(w1[0]), np.zeros(w1[1].shape)]\n",
    "# wi = [np.linalg.pinv(w1[0]), np.zeros(w2[1].shape)]\n",
    "\n",
    "# m = keras.Sequential()\n",
    "# m.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(voc_size,)))\n",
    "# m.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "# m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', 'binary_accuracy'])\n",
    "\n",
    "# m.layers[0].set_weights(w)\n",
    "# m.layers[1].set_weights(wi)\n",
    "\n",
    "# model.evaluate(test_x.reshape(-1, voc_size), test_x.reshape(-1, voc_size))\n",
    "model.layers[0].get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x1e48a308438>,\n",
       " <keras.layers.core.Dense at 0x1e48a308d68>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#models['Character-Embedding'].get_weights()\n",
    "models['Character-Embedding'].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### P-NN: Input, Embedding, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-984d49fe0004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'P-NN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_P_NN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                       \u001b[1;31m# flatten to 1D per sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'P-NN'\n",
    "\n",
    "model_P_NN = keras.Sequential()\n",
    "model_P_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_P_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_P_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_P_NN.add(keras.layers.Dropout(0.001))                                                                  # dropout to prevent overfitting\n",
    "model_P_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_P_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_P_NN\n",
    "print(model_P_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FF-NN: Input, Embedding, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 460)               0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1264)              582704    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 948)               1199220   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 1,783,504\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'FF-NN'\n",
    "hidden_size = (y_shape_ones*embedding_size + y_shape_char*y_shape_ones) // 2\n",
    "\n",
    "model_FF_NN = keras.Sequential()\n",
    "model_FF_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_FF_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_FF_NN.add(keras.layers.Dense(hidden_size, activation='exponential', name='lh'))                         # dense layer\n",
    "model_FF_NN.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_FF_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_FF_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_FF_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_FF_NN\n",
    "print(model_FF_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN1: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 948)               3674448   \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 3,676,028\n",
      "Trainable params: 3,676,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN1'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN1 = keras.Sequential()\n",
    "model_LSTM_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_LSTM_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN1.add(keras.activation.exponential())\n",
    "model_LSTM_RNN1.add(keras.layers.LSTM(y_shape_char * y_shape_ones, activation='exponential', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN1\n",
    "print(model_LSTM_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN2: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b75df2613e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                    \u001b[1;31m# dropout to prevent overfitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# lstm recurrent cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_char\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my_shape_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                              \u001b[1;31m# dense combine time series into single output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_shape_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                           \u001b[1;31m# un flatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 raise TypeError('All layers in a Sequential model '\n\u001b[0m\u001b[0;32m    184\u001b[0m                                 \u001b[1;34m'should have a single output tensor. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                                 \u001b[1;34m'For multi-output layers, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API."
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN2'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN2 = keras.Sequential()\n",
    "model_LSTM_RNN2.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.LSTM(lstm_hidden_size, return_sequences=True, return_state=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_shape_char * y_shape_ones))                                              # dense combine time series into single output\n",
    "model_LSTM_RNN2.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN2\n",
    "print(model_LSTM_RNN2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN1: Input, Embedding, (GRU), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 948)               2755836   \n",
      "_________________________________________________________________\n",
      "reshape_24 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 2,757,416\n",
      "Trainable params: 2,757,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN1'\n",
    "lstm_hidden_size = voc_size * 15\n",
    "\n",
    "model_GRU_RNN1 = keras.Sequential()\n",
    "model_GRU_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_GRU_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_GRU_RNN1.add(keras.layers.GRU(y_shape_char * y_shape_ones, activation='relu', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_GRU_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_GRU_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN1\n",
    "print(model_GRU_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU-RNN2: Imput Embedding, (GRU), Decoder, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 87, 10)            800       \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, 160)               82080     \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               19320     \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 12, 79)            869       \n",
      "=================================================================\n",
      "Total params: 103,069\n",
      "Trainable params: 101,400\n",
      "Non-trainable params: 1,669\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN2'\n",
    "lstm_hidden_size = y_shape_char * character_embedding_size + 40\n",
    "\n",
    "model_GRU_RNN2 = keras.Sequential()\n",
    "model_GRU_RNN2.add(keras.layers.Dense(character_embedding_size, activation='linear', name='char_encode', trainable=False, input_shape=(x_shape_char, x_shape_ones)))                 # embed characters into dense embedded space\n",
    "#model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.GRU(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='gru1'))     # gru recurrent cell\n",
    "#model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_shape_char * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.add(keras.layers.Reshape((y_shape_char, character_embedding_size)))                                        # un flatten\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_shape_ones, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN2\n",
    "print(model_GRU_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_GRU_RNN2.layers[0].set_weights( models['Character-Embedding'].layers[0].get_weights())\n",
    "model_GRU_RNN2.layers[-1].set_weights(models['Character-Embedding'].layers[-1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save/Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DE = models['E-D-NN'].get_weights()\n",
    "#model_GRU_1 = model\n",
    "#model_GRU_2 = model\n",
    "#model_GRU_3 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.set_weights(GRU)\n",
    "#model = model_GRU_3\n",
    "#models['E-D-NN'].set_weights(DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 768/4000 [====>.........................] - ETA: 5:33 - loss: 0.0748 - mean_absolute_error: 0.2342 - categorical_accuracy: 0.0016 - binary_accuracy: 0.8382 - exact_match_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 128\n",
    "model = models['GRU-RNN2']\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs)\n",
    "l, a = test(model, test_x, test_y, showcase_x, showcase_y)\n",
    "training_log(x_name, y_name, architecture, batch_size, epochs, l, a)\n",
    "print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
