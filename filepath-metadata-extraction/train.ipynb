{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Jupyter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing as pp\n",
    "import sys, inspect, argparse, importlib, traceback, re \n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# percentage of samples that exactly match\n",
    "def exact_match_accuracy(y_true, y_pred):\n",
    "    argmax_true = tf.math.argmax(y_true, axis=-1)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    argmax_pred = tf.math.argmax(y_pred, axis=-1)            # onehot to index               (batch, width, onehot:int) -> (batch, width:int)\n",
    "    match_char = tf.math.equal(argmax_true, argmax_pred)     # match characters              (batch, width:int) -> (batch, width:bool)\n",
    "    match_word = tf.math.reduce_all(match_char, axis=-1)     # require all character in sample to match      (batch, width:bool) -> (batch:bool)\n",
    "    match_int = tf.cast(match_word, tf.float32)              # bool to int                                   (batch:bool) -> (batch:float)\n",
    "    return tf.reduce_mean(match_int)                         # percentage of samples that are an exact match (batch:float) -> float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "def log(*l, **d): \n",
    "    if verbose: print(*l, **d)\n",
    "        \n",
    "training_history = []\n",
    "\n",
    "def training_log(x, y, a, b, e, l, m):\n",
    "    training_history.append({'x':x, 'y':y, 'architecture':a, 'batch size':b, 'epochs':e, 'loss':l, 'accuracy':m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tokens and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokens used to communicate non character entities\n",
    "# tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>', '<SurveyNum>', '<SurveyName>', '<LineName>', '<SurveyType>', '<PrimaryDataType>', '<SecondaryDataType>', '<TertiaryDataType>', '<Quaternary>', '<File_Range>', '<First_SP_CDP>', '<Last_SP_CDP>', '<CompletionYear>', '<TenureType>', '<Operator Name>', '<GSQBarcode>', '<EnergySource>', '<LookupDOSFilePath>', '<Source Of Data>']\n",
    "tokens = ['<Padding>', '<Go>', '<EndOfString>', '<UnknownChar>']\n",
    "\n",
    "# get set of characters to be used, use static preset list of characters\n",
    "#available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890-_().,\\\\/\\\"':&\")\n",
    "available_chars = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-_().,\\\\/\\\"':&\")\n",
    "\n",
    "# generate character to int and int to character maps\n",
    "char_to_int = {c: i for i, c in enumerate(tokens + available_chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "char_count = len(char_to_int) # number of character available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Unique Record ID', 'FileName', 'Original_FileName', 'SurveyNum', 'SurveyName', 'LineName', 'SurveyType', 'PrimaryDataType', 'SecondaryDataType', 'TertiaryDataType', 'Quaternary', 'File_Range', 'First_SP_CDP', 'Last_SP_CDP', 'CompletionYear', 'TenureType', 'Operator Name', 'GSQBarcode', 'EnergySource', 'LookupDOSFilePath', 'Source Of Data', 'LookupDOSFilePath_Words', 'FileName_Words', 'LineName_Words'])\n"
     ]
    }
   ],
   "source": [
    "raw_source_file = 'SHUP 2D Files Training Data.csv'\n",
    "\n",
    "# read raw training data\n",
    "data_df = pd.read_csv(raw_source_file, dtype=str)\n",
    "data = {feature:data_df[feature].values for feature in data_df.columns.values}\n",
    "\n",
    "# split strings into words\n",
    "delimiters = r'( |_|-|\\.|\\,|/|\\\\|\\(|\\)|&|:|\\'|\")' # any of '-_().,\\\\/\\\"':& '\n",
    "replacement = r'\\0\\g<1>\\0' # surround delimiter with splitting token\n",
    "\n",
    "# split strings acording to RegEx\n",
    "data['LookupDOSFilePath_Words'] = np.array([re.sub(delimiters, replacement, s).split() for s in data['LookupDOSFilePath']])\n",
    "data['FileName_Words'] = np.array([re.sub(delimiters, replacement, s).split() for s in data['FileName']])\n",
    "data['LineName_Words'] = np.array([re.sub(delimiters, replacement, s).split() for s in data['LineName']])\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Vectorize Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data):\n",
    "    if type(data) == str:\n",
    "        return [char_to_int[char] for char in data.upper()]\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            return [vectorize_data(d) for d in data]\n",
    "        except Exception as e:\n",
    "            #traceback.print_exc()\n",
    "            #print(data, type(data))\n",
    "            return []\n",
    "\n",
    "\n",
    "def devectorise_data(data):\n",
    "        \n",
    "    length = data.shape[0]\n",
    "    \n",
    "    data = data.reshape(length, -1)\n",
    "    strings = np.full((length,), '', dtype=object)\n",
    "    \n",
    "    for i in range(length):\n",
    "        strings[i] = ''.join([int_to_char[int(i)] for i in data[i]])\n",
    "    \n",
    "    return strings\n",
    "    \n",
    "    \n",
    "#     ndim = data.ndim\n",
    "#     if data.dtype != object:\n",
    "#         data = data.astype(object)\n",
    "    \n",
    "#     # decode vector into string\n",
    "#     if ndim == 1:\n",
    "#         return ''.join([int_to_char[int(i)] for i in data])\n",
    "    \n",
    "#     # go to next level\n",
    "#     else:\n",
    "#         for i in range(len(data)):\n",
    "#             data[i] = devectorise_data(data[i])\n",
    "            \n",
    "#         return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Add Padding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find size of largest array across each dimension to computer shape of bounding ndarray\n",
    "def size(data):\n",
    "    \n",
    "    if type(data) == int:\n",
    "        return ()\n",
    "    \n",
    "    this_size = len(data)\n",
    "    \n",
    "    if this_size > 0:\n",
    "        inner_sizes = np.array([size(d) for d in data])\n",
    "        inner_sizes = tuple(np.amax(inner_sizes, axis=0))\n",
    "    else:\n",
    "        inner_sizes = ()\n",
    "    \n",
    "    return (this_size,) + inner_sizes\n",
    "    \n",
    "    \n",
    "def insert_vector(matrix, data, indices):\n",
    "    if type(data) == int:\n",
    "        matrix[indices] = data\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            insert_vector(matrix, data[i], indices + (i,))\n",
    "    \n",
    "\n",
    "def pad_vector_data(data, pad_token, pad_shape=None):\n",
    "    \n",
    "    shape = size(data)\n",
    "    if pad_shape != None:\n",
    "        shape = tuple(np.maximum(pad_shape, shape))\n",
    "\n",
    "    # empty matrix\n",
    "    matrix = np.full(shape, pad_token, np.int32)\n",
    "\n",
    "    insert_vector(matrix, data, ())\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split(data, sizes):\n",
    "    sizes = list(sizes)\n",
    "    \n",
    "    for i in range(1, len(sizes)):\n",
    "        sizes[i] += sizes[i-1]\n",
    "    \n",
    "    slices = [slice(i,j) for i, j in zip([0]+sizes, sizes)]\n",
    "    \n",
    "    return [data[s] for s in slices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shuffle(*data):\n",
    "    order = np.arange(len(data[0]))         # default order of elements\n",
    "    np.random.shuffle(order)                # randomise order\n",
    "    return [d[order] for d in data]         # new array with items in the randimised order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract(*keys, **cuts):\n",
    "    \n",
    "    onehots, shapes = [], []\n",
    "        \n",
    "    for key in keys:\n",
    "        \n",
    "        # get data from dictionary\n",
    "        onehot = onehot_data[key]\n",
    "\n",
    "        # apply cuts\n",
    "        cut = cuts.get(key, [[None]])\n",
    "        cut = [slice(*c) for c in cut]\n",
    "        cut = len(onehot.shape)*[slice(None)] + cut + [slice(None)]\n",
    "        cut = tuple(cut[-len(onehot.shape):])\n",
    "        onehot = onehot[cut]\n",
    "        \n",
    "        # calculate shape\n",
    "        shape = (None, *onehot.shape[1:])[-3:]\n",
    "\n",
    "        onehots.append(onehot)\n",
    "        shapes.append(shape)\n",
    "\n",
    "    return onehots, shapes\n",
    "    \n",
    "def split_and_shuffle(*onehots, sizes=None, shuffle_before=False, shuffle_after=True):\n",
    "    sizes = sizes or [None]\n",
    "    key_count = len(onehots)\n",
    "    subset_count = len(sizes)\n",
    "    \n",
    "    if shuffle_before:\n",
    "        onehots = shuffle(*onehots)\n",
    "\n",
    "    onehots_subsets = np.full((key_count, subset_count), None)\n",
    "    onehots_subsets[:,:] = [split(onehot, sizes) for onehot in onehots]\n",
    "\n",
    "    if shuffle_after:\n",
    "        for i in range(subset_count):\n",
    "            onehots_subsets[:,i] = shuffle(*onehots_subsets[:,i])\n",
    "\n",
    "    return onehots_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unique Record ID':            (23903, 6, 53)\n",
      "'FileName':                    (23903, 87, 53)\n",
      "'Original_FileName':           (23903, 71, 53)\n",
      "'SurveyNum':                   (23903, 5, 53)\n",
      "'SurveyName':                  (23903, 39, 53)\n",
      "'LineName':                    (23903, 23, 53)\n",
      "'SurveyType':                  (23903, 6, 53)\n",
      "'PrimaryDataType':             (23903, 14, 53)\n",
      "'SecondaryDataType':           (23903, 36, 53)\n",
      "'TertiaryDataType':            (23903, 17, 53)\n",
      "'Quaternary':                  (23903, 8, 53)\n",
      "'File_Range':                  (23903, 13, 53)\n",
      "'First_SP_CDP':                (23903, 8, 53)\n",
      "'Last_SP_CDP':                 (23903, 7, 53)\n",
      "'CompletionYear':              (23903, 4, 53)\n",
      "'TenureType':                  (23903, 3, 53)\n",
      "'Operator Name':               (23903, 47, 53)\n",
      "'GSQBarcode':                  (23903, 17, 53)\n",
      "'EnergySource':                (23903, 29, 53)\n",
      "'LookupDOSFilePath':           (23903, 181, 53)\n",
      "'Source Of Data':              (23903, 8, 53)\n",
      "'LookupDOSFilePath_Words':     (23903, 62, 13, 53)\n",
      "'FileName_Words':              (23903, 29, 13, 53)\n",
      "'LineName_Words':              (23903, 7, 11, 53)\n"
     ]
    }
   ],
   "source": [
    "# extract LookupDOSFilePath for speccial processing\n",
    "\n",
    "vectorized_data = {f: vectorize_data(data[f])    for f in data}\n",
    "padded_data =     {f: pad_vector_data(vectorized_data[f], char_to_int['<Padding>'])    for f in vectorized_data}\n",
    "onehot_data =     {f: keras.utils.to_categorical(padded_data[f], char_count)    for f in padded_data}\n",
    "\n",
    "for f in onehot_data: print(f\"'{f}':\".ljust(30), onehot_data[f].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function: Test and show samlpe output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(model, x_test=None, y_test=None, x_preview=None, y_preview=None):\n",
    "    \n",
    "    if (x_preview is not None) and (y_preview is not None):\n",
    "        t_size = len(x_preview)\n",
    "        \n",
    "        p_one_hot = model.predict(x_preview)\n",
    "        p_vector = np.argmax(p_one_hot, -1)\n",
    "        p_vector = p_vector.reshape((t_size, -1))\n",
    "        p_strings = devectorise_data(p_vector)\n",
    "\n",
    "        y_vector = np.argmax(y_preview, -1)\n",
    "        y_vector = y_vector.reshape((t_size, -1))\n",
    "        y_strings = devectorise_data(y_vector)\n",
    "\n",
    "        x_vector = np.argmax(x_preview, -1)\n",
    "        x_vector = x_vector.reshape((t_size, -1))\n",
    "        x_strings = devectorise_data(x_vector)\n",
    "\n",
    "        n_strings = [f'{i}. ' for i in range(t_size)]\n",
    "        x_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in x_strings]\n",
    "        y_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in y_strings]\n",
    "        p_strings = [re.sub('(<Padding>)+', ' ', s).strip() for s in p_strings]\n",
    "        n_w, x_w, y_w, p_w = max([len(s) for s in n_strings]), max([len(s) for s in x_strings]), max([len(s) for s in y_strings]), max([len(s) for s in p_strings])\n",
    "        y_p_strings = [\"'  '\".join([n.ljust(n_w), x.ljust(x_w), y.ljust(y_w), p.ljust(p_w), str(y==p)]) for n, x, y, p in zip(n_strings, x_strings, y_strings, p_strings)]\n",
    "\n",
    "        print(*y_p_strings, sep='\\n', end='\\n\\n')\n",
    "\n",
    "    if (x_test is not None) and (y_test is not None):\n",
    "        \n",
    "        # metric names\n",
    "        metrics = [model.loss] + model.metrics\n",
    "        \n",
    "        # accuracy on entire training set\n",
    "        accuracies = model.evaluate(x_test, y_test)\n",
    "        print(*list(zip(metrics, accuracies)), sep='\\n', end='\\n\\n') # evaluate and list loss and each metric\n",
    "\n",
    "        return accuracies[0], accuracies[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = ''\n",
    "\n",
    "metrics = ['mean_absolute_error', 'categorical_accuracy', exact_match_accuracy] # binary_accuracy\n",
    "loss = 'categorical_crossentropy' # poisson mean_squared_logarithmic_error categorical_crossentropy\n",
    "\n",
    "embed_loss='categorical_crossentropy'\n",
    "embed_metrics=['accuracy', 'mean_absolute_error', 'categorical_accuracy', exact_match_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Character Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Auto Encoder Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create offset input and output sequences to training a preditive embedding model.\n",
    "\n",
    "(x_char_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract('LookupDOSFilePath')\n",
    "(x_char_onehot,) = shuffle(x_char_onehot)\n",
    "shape = x_char_onehot.shape\n",
    "x_embed_size = len(x_char_onehot)\n",
    "\n",
    "# create columns of padding tokens\n",
    "padding = np.full((*shape[:-2], 1), char_to_int['<Padding>'])\n",
    "#padding = np.array([[char_to_int['<Padding>']]] * x_embed_size)\n",
    "padding = keras.utils.to_categorical(padding, char_count)\n",
    "padding = padding.reshape(*shape[:-2], 1, shape[-1])\n",
    "\n",
    "# 'abcd' -> ('_abcd', 'abcd_')\n",
    "x_embed_train = np.concatenate((x_char_onehot, padding), axis=-2)\n",
    "y_embed_train = np.concatenate((padding, x_char_onehot), axis=-2)\n",
    "x_embed_test = np.concatenate((x_char_onehot, padding), axis=-2)\n",
    "y_embed_test = np.concatenate((padding, x_char_onehot), axis=-2)\n",
    "\n",
    "embed_char_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Auto Encoder: Input, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lh (Dense)                   (None, 182, 10)           540       \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 182, 53)           583       \n",
      "=================================================================\n",
      "Total params: 1,123\n",
      "Trainable params: 1,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'Character-Embedding'\n",
    "\n",
    "model_E_D_NN = keras.Sequential()\n",
    "model_E_D_NN.add(keras.layers.Dense(character_embedding_size, name='lh', input_shape=(embed_char_count, char_count,)))\n",
    "model_E_D_NN.add(keras.layers.Dense(char_count, activation='sigmoid', name='lo'))\n",
    "#model_E_D_NN.add(keras.layers.Dropout(0.001))\n",
    "model_E_D_NN.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "models[architecture] = model_E_D_NN\n",
    "print(model_E_D_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23903/23903 [==============================] - 20s 822us/step - loss: 2.1194 - acc: 0.4504 - mean_absolute_error: 0.1151 - categorical_accuracy: 0.4504 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "23903/23903 [==============================] - 17s 704us/step - loss: 1.4233 - acc: 0.5622 - mean_absolute_error: 0.0324 - categorical_accuracy: 0.5622 - exact_match_accuracy: 0.0000e+00\n",
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY'  'A_SSUA\\ED_SORSG_AUOROS__SEDTAED_SUUROADETATA\\198A\\OTA_\\GA_SEGA\\OTA_\\GD989CSADOTAD_FEOTASED_O8\\\\\\10D\\11\\911_EG'  'False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> y   predictive\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, y_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], y_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23903/23903 [==============================] - 18s 746us/step - loss: 0.2023 - acc: 0.9332 - mean_absolute_error: 0.0205 - categorical_accuracy: 0.9332 - exact_match_accuracy: 0.0303\n",
      "Epoch 2/5\n",
      "23903/23903 [==============================] - 17s 697us/step - loss: 0.0148 - acc: 0.9891 - mean_absolute_error: 0.0174 - categorical_accuracy: 0.9891 - exact_match_accuracy: 0.1965\n",
      "Epoch 3/5\n",
      "23903/23903 [==============================] - 17s 696us/step - loss: 0.0093 - acc: 0.9892 - mean_absolute_error: 0.0171 - categorical_accuracy: 0.9892 - exact_match_accuracy: 0.1987\n",
      "Epoch 4/5\n",
      "23903/23903 [==============================] - 17s 723us/step - loss: 0.0046 - acc: 0.9973 - mean_absolute_error: 0.0173 - categorical_accuracy: 0.9973 - exact_match_accuracy: 0.7994\n",
      "Epoch 5/5\n",
      "23903/23903 [==============================] - 17s 725us/step - loss: 2.4996e-04 - acc: 1.0000 - mean_absolute_error: 0.0173 - categorical_accuracy: 1.0000 - exact_match_accuracy: 1.0000\n",
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY'  'True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x -> x   direct\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "models['Character-Embedding'].fit(x_embed_train, x_embed_train, batch_size=batch_size, epochs=epochs)\n",
    "test(models['Character-Embedding'], None, None, x_embed_train[:1], x_embed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# w1 = model.layers[0].get_weights()\n",
    "# w2 = model.layers[1].get_weights()\n",
    "\n",
    "# w = [np.copy(w1[0]), np.zeros(w1[1].shape)]\n",
    "# wi = [np.linalg.pinv(w1[0]), np.zeros(w2[1].shape)]\n",
    "\n",
    "# m = keras.Sequential()\n",
    "# m.add(keras.layers.Dense(character_embedding_size, activation='linear', name='lh', input_shape=(embed_char_count, voc_size,)))\n",
    "# m.add(keras.layers.Dense(voc_size, activation='sigmoid', name='lo'))\n",
    "# m.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "# m.layers[0].set_weights(w)\n",
    "# m.layers[1].set_weights(wi)\n",
    "\n",
    "# encode_weights, decode_weights = w, wi\n",
    "\n",
    "# accuracy = model.evaluate(embed_test_x, embed_test_x)\n",
    "# metric_names = [embed_loss] + embed_metrics\n",
    "# dict(zip(metric_names, accuracy))\n",
    "\n",
    "embeddings['Character-Embedding'] = [models['Character-Embedding'].layers[0].get_weights(), models['Character-Embedding'].layers[1].get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY                                         '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY                                         '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRANSBY\\SEGY\\BRANSBY_80-KHW_RAW_MIGRATED_QR011264_167289.SGY                                         '  'True\n",
      "1. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EEH_FINAL_QR020041_170077.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EEH_FINAL_QR020041_170077.SGY'  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1990\\GREGORY_2D_JACKSON_AND_BOLAN_3D\\SEGY\\GREGORY_2D_JACKSON_AND_BOLAN_3D_93-EEH_FINAL_QR020041_170077.SGY'  'True\n",
      "2. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\RIVERSLEA\\SEGY\\RIVERSLEA_HSB-830_PROCESSED_SDU07181TA_198647.SGY                                     '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\RIVERSLEA\\SEGY\\RIVERSLEA_HSB-830_PROCESSED_SDU07181TA_198647.SGY                                     '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\RIVERSLEA\\SEGY\\RIVERSLEA_HSB-830_PROCESSED_SDU07181TA_198647.SGY                                     '  'True\n",
      "3. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRAHE\\SEGY\\BRAHE_84-TGT_FILTERED_MIGRATION_QR010894_138265.SGY                                       '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRAHE\\SEGY\\BRAHE_84-TGT_FILTERED_MIGRATION_QR010894_138265.SGY                                       '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\BRAHE\\SEGY\\BRAHE_84-TGT_FILTERED_MIGRATION_QR010894_138265.SGY                                       '  'True\n",
      "4. '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\PORTLAND\\SEGY\\PORTLAND_A87A-311_PROCESSED_QR021669_197163.SGY                                        '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\PORTLAND\\SEGY\\PORTLAND_A87A-311_PROCESSED_QR021669_197163.SGY                                        '  '\\SHUP\\2D_SURVEYS\\PROCESSED_AND_SUPPORT_DATA\\1980\\PORTLAND\\SEGY\\PORTLAND_A87A-311_PROCESSED_QR021669_197163.SGY                                        '  'True\n",
      "\n",
      "23903/23903 [==============================] - 7s 304us/step\n",
      "('categorical_crossentropy', 0.00015475044764043055)\n",
      "('accuracy', 1.0)\n",
      "('mean_absolute_error', 0.017213060492419406)\n",
      "('categorical_accuracy', 1.0)\n",
      "(<function exact_match_accuracy at 0x00000211DAA0DE18>, 1.0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00015475044764043055, 1.0)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test(models['Character-Embedding'], x_embed_train, x_embed_train, x_embed_train[:5], x_embed_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Auto Encoder Folder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WordEmbedding():\n",
    "    def __init__(self, x_name, compression=0.1):\n",
    "\n",
    "        # extract and shuffle data\n",
    "        (x_word_onehot,), ((embed_word_count, embed_char_count, embed_ones_count),) = extract(x_name)\n",
    "        (x_word_onehot,) = shuffle(x_word_onehot)\n",
    "\n",
    "        # if data is not split into words, create extra word diemnstion\n",
    "        if(embed_word_count == None):\n",
    "            embed_word_count = 1\n",
    "            x_word_onehot = np.expand_dims(x_word_onehot, 1)\n",
    "        \n",
    "        self.x_word_onehot = x_word_onehot\n",
    "\n",
    "        # define embedding sizes\n",
    "        self.character_embedding_size = character_embedding_size\n",
    "        self.word_embedding_size = int(embed_char_count * self.character_embedding_size * compression) + 1\n",
    "        c_size, w_size = self.character_embedding_size, self.word_embedding_size\n",
    "        \n",
    "        \n",
    "        # define model\n",
    "\n",
    "        # character embedding\n",
    "        self.l_encode_character = keras.layers.Dense(c_size, name='char_encode', input_shape=(embed_word_count, embed_char_count, embed_ones_count))\n",
    "        self.l_char_to_word = keras.layers.Reshape((embed_word_count, embed_char_count * c_size,))\n",
    "\n",
    "        # word auto encoder\n",
    "        self.l_encode_word = keras.layers.Dense(w_size, name='lh', input_shape=(embed_char_count, char_count,))\n",
    "        self.l_decode_word = keras.layers.Dense(embed_char_count * c_size, activation='sigmoid', name='lo')\n",
    "\n",
    "        # character de embedding\n",
    "        self.l_word_to_char = keras.layers.Reshape((embed_word_count, embed_char_count, c_size))\n",
    "        self.l_decode_character = keras.layers.Dense(embed_ones_count, activation='sigmoid', name='char_decode')\n",
    "\n",
    "        self.model = keras.Sequential([\n",
    "            self.l_encode_character,\n",
    "            self.l_char_to_word,\n",
    "            self.l_encode_word,\n",
    "            self.l_decode_word,\n",
    "            self.l_word_to_char,\n",
    "            self.l_decode_character,\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam', loss=embed_loss, metrics=embed_metrics)\n",
    "\n",
    "        \n",
    "        # set pre-trained character embedding\n",
    "        self.l_encode_character.set_weights(deepcopy(embeddings['Character-Embedding'][0]))\n",
    "        self.l_decode_character.set_weights(deepcopy(embeddings['Character-Embedding'][1]))\n",
    "\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "    def train(self, epochs=5, batch_size=32):\n",
    "        # perform training\n",
    "        self.model.fit(self.x_word_onehot, self.x_word_onehot, batch_size=batch_size, epochs=epochs)\n",
    "        test(self.model, self.x_word_onehot, self.x_word_onehot, self.x_word_onehot[:3], self.x_word_onehot[:3])\n",
    "\n",
    "    def apply_encode(self, new_encode_character, new_encode_word):\n",
    "        new_encode_character.set_weights(deepcopy(self.l_encode_character.get_weights()))\n",
    "        new_encode_word.set_weights(deepcopy(self.l_encode_word.get_weights()))\n",
    "\n",
    "    def apply_decode(self, new_decode_word, new_decode_character):\n",
    "        new_decode_word.set_weights(deepcopy(self.l_decode_word.get_weights()))\n",
    "        new_decode_character.set_weights(deepcopy(self.l_decode_character.get_weights()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### LineName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 1, 23, 10)         540       \n",
      "_________________________________________________________________\n",
      "reshape_93 (Reshape)         (None, 1, 230)            0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1, 35)             8085      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 1, 230)            8280      \n",
      "_________________________________________________________________\n",
      "reshape_94 (Reshape)         (None, 1, 23, 10)         0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 1, 23, 53)         583       \n",
      "=================================================================\n",
      "Total params: 17,488\n",
      "Trainable params: 17,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "23903/23903 [==============================] - 14s 587us/step - loss: 1.1695 - acc: 0.7591 - mean_absolute_error: 0.0731 - categorical_accuracy: 0.7591 - exact_match_accuracy: 0.0161\n",
      "Epoch 2/10\n",
      "23903/23903 [==============================] - 10s 421us/step - loss: 0.3005 - acc: 0.9588 - mean_absolute_error: 0.0108 - categorical_accuracy: 0.9588 - exact_match_accuracy: 0.4657\n",
      "Epoch 3/10\n",
      "23903/23903 [==============================] - 10s 415us/step - loss: 0.1281 - acc: 0.9915 - mean_absolute_error: 0.0105 - categorical_accuracy: 0.9915 - exact_match_accuracy: 0.8591\n",
      "Epoch 4/10\n",
      "23903/23903 [==============================] - 12s 504us/step - loss: 0.0595 - acc: 0.9960 - mean_absolute_error: 0.0113 - categorical_accuracy: 0.9960 - exact_match_accuracy: 0.9293\n",
      "Epoch 5/10\n",
      "23903/23903 [==============================] - 11s 478us/step - loss: 0.0325 - acc: 0.9974 - mean_absolute_error: 0.0119 - categorical_accuracy: 0.9974 - exact_match_accuracy: 0.9530\n",
      "Epoch 6/10\n",
      "23903/23903 [==============================] - 12s 520us/step - loss: 0.0201 - acc: 0.9981 - mean_absolute_error: 0.0124 - categorical_accuracy: 0.9981 - exact_match_accuracy: 0.9677\n",
      "Epoch 7/10\n",
      "23903/23903 [==============================] - 10s 428us/step - loss: 0.0133 - acc: 0.9987 - mean_absolute_error: 0.0126 - categorical_accuracy: 0.9987 - exact_match_accuracy: 0.9785\n",
      "Epoch 8/10\n",
      "23903/23903 [==============================] - 10s 435us/step - loss: 0.0095 - acc: 0.9990 - mean_absolute_error: 0.0127 - categorical_accuracy: 0.9990 - exact_match_accuracy: 0.9847\n",
      "Epoch 9/10\n",
      "23903/23903 [==============================] - 11s 448us/step - loss: 0.0070 - acc: 0.9992 - mean_absolute_error: 0.0127 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.9891\n",
      "Epoch 10/10\n",
      "23903/23903 [==============================] - 11s 441us/step - loss: 0.0057 - acc: 0.9993 - mean_absolute_error: 0.0127 - categorical_accuracy: 0.9993 - exact_match_accuracy: 0.9891\n",
      "0. '  '96-HCL  '  '96-HCL  '  '96-HCL  '  'True\n",
      "1. '  '86-D7   '  '86-D7   '  '86-D7   '  'True\n",
      "2. '  'S88E-220'  'S88E-220'  'S88E-220'  'True\n",
      "\n",
      "23903/23903 [==============================] - 4s 175us/step\n",
      "('categorical_crossentropy', 0.005094889805349501)\n",
      "('accuracy', 0.9994106635844683)\n",
      "('mean_absolute_error', 0.012799916734660787)\n",
      "('categorical_accuracy', 0.9994106635844683)\n",
      "(<function exact_match_accuracy at 0x00000211DAA0DE18>, 0.9905451198594318)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['LineName_Embedding'] = WordEmbedding('LineName', 0.15)\n",
    "embeddings['LineName_Embedding'].train(10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### FileName_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 29, 13, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_99 (Reshape)         (None, 29, 130)           0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 29, 20)            2620      \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 29, 130)           2730      \n",
      "_________________________________________________________________\n",
      "reshape_100 (Reshape)        (None, 29, 13, 10)        0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 29, 13, 53)        583       \n",
      "=================================================================\n",
      "Total params: 6,473\n",
      "Trainable params: 6,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/8\n",
      "23903/23903 [==============================] - 36s 1ms/step - loss: 1.0024 - acc: 0.7951 - mean_absolute_error: 0.0572 - categorical_accuracy: 0.7951 - exact_match_accuracy: 0.4881\n",
      "Epoch 2/8\n",
      "23903/23903 [==============================] - 34s 1ms/step - loss: 0.2717 - acc: 0.9204 - mean_absolute_error: 0.0115 - categorical_accuracy: 0.9204 - exact_match_accuracy: 0.6959\n",
      "Epoch 3/8\n",
      "23903/23903 [==============================] - 35s 1ms/step - loss: 0.1521 - acc: 0.9621 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9621 - exact_match_accuracy: 0.7953\n",
      "Epoch 4/8\n",
      "23903/23903 [==============================] - 35s 1ms/step - loss: 0.0798 - acc: 0.9853 - mean_absolute_error: 0.0101 - categorical_accuracy: 0.9853 - exact_match_accuracy: 0.8936\n",
      "Epoch 5/8\n",
      "23903/23903 [==============================] - 35s 1ms/step - loss: 0.0442 - acc: 0.9920 - mean_absolute_error: 0.0116 - categorical_accuracy: 0.9920 - exact_match_accuracy: 0.9365: 7s - loss: 0.0464 - acc: 0.9918 - mean_absolute_error: 0.0115 - categorical_accuracy: 0.9918 - exact_match_accuracy\n",
      "Epoch 6/8\n",
      "23903/23903 [==============================] - 34s 1ms/step - loss: 0.0297 - acc: 0.9941 - mean_absolute_error: 0.0125 - categorical_accuracy: 0.9941 - exact_match_accuracy: 0.9519\n",
      "Epoch 7/8\n",
      "23903/23903 [==============================] - 34s 1ms/step - loss: 0.0223 - acc: 0.9953 - mean_absolute_error: 0.0132 - categorical_accuracy: 0.9953 - exact_match_accuracy: 0.9618\n",
      "Epoch 8/8\n",
      "23903/23903 [==============================] - 37s 2ms/step - loss: 0.0181 - acc: 0.9961 - mean_absolute_error: 0.0136 - categorical_accuracy: 0.9961 - exact_match_accuracy: 0.9690\n",
      "0. '  'AD94 _ AD94 - 17 _ FINAL _ MIGRATED _ SDU10912TA _ 131057 . SGY     '  'AD94 _ AD94 - 17 _ FINAL _ MIGRATED _ SDU10912TA _ 131057 . SGY     '  'AD94 _ AD94 - 17 _ FINAL _ MIGRATED _ SDU10912TA _ 131057 . SGY     '  'True\n",
      "1. '  'SQ98 _ 99 - JEM _ FILTERED _ MIGRATION _ QR023327 _ 175578 . SGY    '  'SQ98 _ 99 - JEM _ FILTERED _ MIGRATION _ QR023327 _ 175578 . SGY    '  'SQ98 _ 99 - JEM _ FILTERED _ MIGRATION _ QR023327 _ 179578 . SGY    '  'False\n",
      "2. '  'COOLIBAH _ CREEK _ HQ83 - 96 _ PROCESSED _ SDU02287TA _ 198453 . SGY'  'COOLIBAH _ CREEK _ HQ83 - 96 _ PROCESSED _ SDU02287TA _ 198453 . SGY'  'COOLIBAH _ CREEK _ HQ83 - 96 _ PROCESSED _ SDU02287TA _ 198453 . SGY'  'True\n",
      "\n",
      "23903/23903 [==============================] - 16s 678us/step\n",
      "('categorical_crossentropy', 0.01642537659912895)\n",
      "('accuracy', 0.9965488270940638)\n",
      "('mean_absolute_error', 0.013797355717972163)\n",
      "('categorical_accuracy', 0.9965488270940638)\n",
      "(<function exact_match_accuracy at 0x00000211DAA0DE18>, 0.9716541118583525)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['FileName_Words_Embedding'] = WordEmbedding('FileName_Words', 0.15)\n",
    "embeddings['FileName_Words_Embedding'].train(8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SurveyName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 1, 39, 10)         540       \n",
      "_________________________________________________________________\n",
      "reshape_97 (Reshape)         (None, 1, 390)            0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1, 59)             23069     \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 1, 390)            23400     \n",
      "_________________________________________________________________\n",
      "reshape_98 (Reshape)         (None, 1, 39, 10)         0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 1, 39, 53)         583       \n",
      "=================================================================\n",
      "Total params: 47,592\n",
      "Trainable params: 47,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "23903/23903 [==============================] - 20s 834us/step - loss: 1.2221 - acc: 0.7207 - mean_absolute_error: 0.0785 - categorical_accuracy: 0.7207 - exact_match_accuracy: 0.0218\n",
      "Epoch 2/10\n",
      "23903/23903 [==============================] - 17s 717us/step - loss: 0.2485 - acc: 0.9639 - mean_absolute_error: 0.0107 - categorical_accuracy: 0.9639 - exact_match_accuracy: 0.4293\n",
      "Epoch 3/10\n",
      "23903/23903 [==============================] - 16s 674us/step - loss: 0.0956 - acc: 0.9892 - mean_absolute_error: 0.0111 - categorical_accuracy: 0.9892 - exact_match_accuracy: 0.7241\n",
      "Epoch 4/10\n",
      "23903/23903 [==============================] - 16s 666us/step - loss: 0.0479 - acc: 0.9931 - mean_absolute_error: 0.0124 - categorical_accuracy: 0.9931 - exact_match_accuracy: 0.7843\n",
      "Epoch 5/10\n",
      "23903/23903 [==============================] - 19s 775us/step - loss: 0.0275 - acc: 0.9958 - mean_absolute_error: 0.0133 - categorical_accuracy: 0.9958 - exact_match_accuracy: 0.8696\n",
      "Epoch 6/10\n",
      "23903/23903 [==============================] - 17s 728us/step - loss: 0.0201 - acc: 0.9961 - mean_absolute_error: 0.0139 - categorical_accuracy: 0.9961 - exact_match_accuracy: 0.88547s - loss: 0.0215 - acc: 0.9960 - m - ETA: 2s - loss: 0.0208 - acc: 0.9960 - mean_absolute_error: 0.0139 - categorical_accu\n",
      "Epoch 7/10\n",
      "23903/23903 [==============================] - 18s 770us/step - loss: 0.0124 - acc: 0.9972 - mean_absolute_error: 0.0144 - categorical_accuracy: 0.9972 - exact_match_accuracy: 0.9041\n",
      "Epoch 8/10\n",
      "23903/23903 [==============================] - 18s 765us/step - loss: 0.0102 - acc: 0.9980 - mean_absolute_error: 0.0148 - categorical_accuracy: 0.9980 - exact_match_accuracy: 0.9343\n",
      "Epoch 9/10\n",
      "23903/23903 [==============================] - 18s 746us/step - loss: 0.0079 - acc: 0.9986 - mean_absolute_error: 0.0151 - categorical_accuracy: 0.9986 - exact_match_accuracy: 0.9575\n",
      "Epoch 10/10\n",
      "23903/23903 [==============================] - 17s 710us/step - loss: 0.0070 - acc: 0.9987 - mean_absolute_error: 0.0154 - categorical_accuracy: 0.9987 - exact_match_accuracy: 0.9648\n",
      "0. '  'R80       '  'R80       '  'R80       '  'True\n",
      "1. '  'PARENJELLY'  'PARENJELLY'  'PARENJELLY'  'True\n",
      "2. '  'AR91      '  'AR91      '  'AR91      '  'True\n",
      "\n",
      "23903/23903 [==============================] - 6s 238us/step\n",
      "('categorical_crossentropy', 0.005119248909504204)\n",
      "('accuracy', 0.999182594623924)\n",
      "('mean_absolute_error', 0.015634422392508368)\n",
      "('categorical_accuracy', 0.999182594623924)\n",
      "(<function exact_match_accuracy at 0x00000211DAA0DE18>, 0.9729322679161612)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['SurveyName_Embedding'] = WordEmbedding('SurveyName', 0.15)\n",
    "embeddings['SurveyName_Embedding'].train(10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23903/23903 [==============================] - 24s 1ms/step - loss: 0.0052 - acc: 0.9991 - mean_absolute_error: 0.0158 - categorical_accuracy: 0.9991 - exact_match_accuracy: 0.9693: 5s - loss: 0.0052 - acc: 0.9991 - mean_absolute_error: 0 - ETA: 1s - loss: 0.0052 - acc: 0.9991 - mean_absolute_error: 0.0158 - categorical_accuracy: 0.9991 - exact_m\n",
      "Epoch 2/2\n",
      "23903/23903 [==============================] - 23s 953us/step - loss: 0.0046 - acc: 0.9992 - mean_absolute_error: 0.0160 - categorical_accuracy: 0.9992 - exact_match_accuracy: 0.9730\n",
      "0. '  'R80       '  'R80       '  'R80       '  'True\n",
      "1. '  'PARENJELLY'  'PARENJELLY'  'PARENJELLY'  'True\n",
      "2. '  'AR91      '  'AR91      '  'AR91      '  'True\n",
      "\n",
      "23903/23903 [==============================] - 5s 190us/step\n",
      "('categorical_crossentropy', 0.004410482474722919)\n",
      "('accuracy', 0.999087123777489)\n",
      "('mean_absolute_error', 0.016016207693145223)\n",
      "('categorical_accuracy', 0.999087123777489)\n",
      "(<function exact_match_accuracy at 0x00000211DAA0DE18>, 0.9692925574195708)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings['SurveyName_Embedding'].train(2, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### P-NN: Input, Embedding, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-984d49fe0004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'P-NN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_P_NN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_P_NN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                                                                       \u001b[1;31m# flatten to 1D per sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'P-NN'\n",
    "\n",
    "model_P_NN = keras.Sequential()\n",
    "model_P_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_P_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_P_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_P_NN.add(keras.layers.Dropout(0.001))                                                                  # dropout to prevent overfitting\n",
    "model_P_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_P_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_P_NN\n",
    "print(model_P_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FF-NN: Input, Embedding, Hidden, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 460)               0         \n",
      "_________________________________________________________________\n",
      "lh (Dense)                   (None, 1264)              582704    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1264)              0         \n",
      "_________________________________________________________________\n",
      "lo (Dense)                   (None, 948)               1199220   \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 1,783,504\n",
      "Trainable params: 1,783,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'FF-NN'\n",
    "hidden_size = (y_shape_ones*embedding_size + y_shape_char*y_shape_ones) // 2\n",
    "\n",
    "model_FF_NN = keras.Sequential()\n",
    "model_FF_NN.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "model_FF_NN.add(keras.layers.Flatten())                                                                       # flatten to 1D per sample\n",
    "model_FF_NN.add(keras.layers.Dense(hidden_size, activation='exponential', name='lh'))                         # dense layer\n",
    "model_FF_NN.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_FF_NN.add(keras.layers.Dense(y_shape_char*y_shape_ones, activation='exponential', name='lo'))           # dense layer\n",
    "model_FF_NN.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_FF_NN.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_FF_NN\n",
    "print(model_FF_NN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN1: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "le (Embedding)               (None, 23, 20)            1580      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 948)               3674448   \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 12, 79)            0         \n",
      "=================================================================\n",
      "Total params: 3,676,028\n",
      "Trainable params: 3,676,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN1'\n",
    "lstm_hidden_size = embedding_size * 15\n",
    "\n",
    "model_LSTM_RNN1 = keras.Sequential()\n",
    "model_LSTM_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_LSTM_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_LSTM_RNN1.add(keras.activation.exponential())\n",
    "model_LSTM_RNN1.add(keras.layers.LSTM(y_shape_char * y_shape_ones, activation='exponential', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_LSTM_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_LSTM_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN1\n",
    "print(model_LSTM_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM-RNN2: Input, Embedding, (LSTM), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 6, 20)             1080      \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 240)               250560    \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               28920     \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 6, 20)             0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 6, 53)             1113      \n",
      "=================================================================\n",
      "Total params: 281,673\n",
      "Trainable params: 279,480\n",
      "Non-trainable params: 2,193\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN2'\n",
    "\n",
    "x_name, y_name = 'LineName', 'LineName'\n",
    "cuts = {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = y_char_count * character_embedding_size * 2\n",
    "\n",
    "model_LSTM_RNN2 = keras.Sequential()\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_char_count, x_ones_count)))                 # embed characters into dense embedded space\n",
    "#model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.LSTM(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='lstm1'))     # lstm recurrent cell\n",
    "#model_LSTM_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_LSTM_RNN2.add(keras.layers.Reshape((y_char_count, character_embedding_size)))                                        # un flatten\n",
    "model_LSTM_RNN2.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_LSTM_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_LSTM_RNN2\n",
    "print(model_LSTM_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_LSTM_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_LSTM_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-RNN3: Input, Embed Characters, Embed Words, (LSTM), De-embed Words, De-embed Characters, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 29, 13, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_107 (Reshape)        (None, 29, 130)           0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 29, 20)            2620      \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 35)                3535      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 230)               8280      \n",
      "_________________________________________________________________\n",
      "reshape_108 (Reshape)        (None, 23, 10)            0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 23, 53)            583       \n",
      "=================================================================\n",
      "Total params: 63,958\n",
      "Trainable params: 63,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'LSTM-RNN3'\n",
    "\n",
    "x_name, y_name = 'FileName_Words', 'LineName'\n",
    "cuts = {} # {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "x_embed = embeddings[x_name+'_Embedding']\n",
    "y_embed = embeddings[y_name+'_Embedding']\n",
    "\n",
    "lstm_hidden_size = 100\n",
    "\n",
    "\n",
    "# character embedding\n",
    "l_encode_character = keras.layers.Dense(   x_embed.character_embedding_size, input_shape=(x_word_count, x_char_count, x_ones_count))\n",
    "\n",
    "# word embedding\n",
    "l_char_to_word =     keras.layers.Reshape( (x_word_count, x_char_count * x_embed.character_embedding_size,))\n",
    "l_encode_word =      keras.layers.Dense(   x_embed.word_embedding_size)\n",
    "\n",
    "# lstm processing\n",
    "l_lstm =             keras.layers.LSTM(    lstm_hidden_size,   activation='sigmoid',   implementation=2,   unroll=True)\n",
    "l_decode =           keras.layers.Dense(   y_embed.word_embedding_size,   activation='sigmoid')\n",
    "\n",
    "l_decode_word =      keras.layers.Dense(   y_char_count * y_embed.character_embedding_size,   activation='sigmoid')\n",
    "l_word_to_char =     keras.layers.Reshape( (y_char_count, character_embedding_size))\n",
    "\n",
    "l_decode_character = keras.layers.Dense(   y_ones_count,   activation='sigmoid')\n",
    "\n",
    "\n",
    "models[architecture] = keras.Sequential([\n",
    "    l_encode_character,\n",
    "    l_char_to_word,\n",
    "    l_encode_word,\n",
    "    l_lstm,\n",
    "    l_decode,\n",
    "    l_decode_word,\n",
    "    l_word_to_char,\n",
    "    l_decode_character,\n",
    "])\n",
    "\n",
    "models[architecture].compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "print(models[architecture].summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "x_embed.apply_encode(l_encode_character, l_encode_word)\n",
    "y_embed.apply_decode(l_decode_word, l_decode_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN1: Input, Embedding, (GRU), Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voc_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-05384da3bcb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marchitecture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'GRU-RNN1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlstm_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoc_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_GRU_RNN1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_GRU_RNN1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_shape_ones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'le'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_shape_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# embed characters into dense embedded space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'voc_size' is not defined"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN1'\n",
    "lstm_hidden_size = voc_size * 15\n",
    "\n",
    "model_GRU_RNN1 = keras.Sequential()\n",
    "model_GRU_RNN1.add(keras.layers.Embedding(y_shape_ones, embedding_size, name='le', input_length=x_shape_char))   # embed characters into dense embedded space\n",
    "#model_GRU_RNN1.add(keras.layers.Dropout(0.2))                                                                    # dropout to prevent overfitting\n",
    "model_GRU_RNN1.add(keras.layers.GRU(y_shape_char * y_shape_ones, activation='relu', implementation=2, unroll=True))                # lstm recurrent cell\n",
    "model_GRU_RNN1.add(keras.layers.Reshape((y_shape_char, y_shape_ones)))                                           # un flatten\n",
    "model_GRU_RNN1.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN1\n",
    "print(model_GRU_RNN1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GRU-RNN2: Imput Embedding, (GRU), Decoder, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_encode (Dense)          (None, 12, 10)            540       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, 160)               82080     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "decode (Dense)               (None, 120)               19320     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 12, 10)            0         \n",
      "_________________________________________________________________\n",
      "char_decode (Dense)          (None, 12, 53)            583       \n",
      "=================================================================\n",
      "Total params: 102,523\n",
      "Trainable params: 101,400\n",
      "Non-trainable params: 1,123\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN2'\n",
    "\n",
    "x_name, y_name = 'LineName', 'LineName'\n",
    "cuts = {x_name:[[None, 12]] , y_name:[[None, 12]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "lstm_hidden_size = y_char_count * character_embedding_size + 40\n",
    "\n",
    "model_GRU_RNN2 = keras.Sequential()\n",
    "model_GRU_RNN2.add(keras.layers.Dense(character_embedding_size, name='char_encode', trainable=False, input_shape=(x_char_count, x_ones_count)))                 # embed characters into dense embedded space\n",
    "model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.GRU(lstm_hidden_size, activation='sigmoid', implementation=2, unroll=True, name='gru1'))     # gru recurrent cell\n",
    "model_GRU_RNN2.add(keras.layers.Dropout(0.2))                                                                            # dropout to prevent overfitting\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_char_count * character_embedding_size, activation='sigmoid', name='decode'))  # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.add(keras.layers.Reshape((y_char_count, character_embedding_size)))                                        # un flatten\n",
    "model_GRU_RNN2.add(keras.layers.Dense(y_ones_count, activation='sigmoid', trainable=False, name='char_decode'))                             # dense layer, decode/de-embed\n",
    "model_GRU_RNN2.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "models[architecture] = model_GRU_RNN2\n",
    "print(model_GRU_RNN2.summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "model_GRU_RNN2.layers[0].set_weights(char_encode_weights)\n",
    "model_GRU_RNN2.layers[-1].set_weights(char_decode_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_77 (Dense)             (None, 29, 13, 10)        540       \n",
      "_________________________________________________________________\n",
      "reshape_119 (Reshape)        (None, 29, 130)           0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 29, 20)            2620      \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 177)               105138    \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 59)                10502     \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 390)               23400     \n",
      "_________________________________________________________________\n",
      "reshape_120 (Reshape)        (None, 39, 10)            0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 39, 53)            583       \n",
      "=================================================================\n",
      "Total params: 142,783\n",
      "Trainable params: 142,783\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "architecture = 'GRU-RNN3'\n",
    "\n",
    "x_name, y_name = 'FileName_Words', 'SurveyName'\n",
    "cuts = {} # {x_name:[[None, 6]] , y_name:[[None, 6]]}\n",
    "(x_onehot, y_onehot), ((x_word_count, x_char_count, x_ones_count),(y_word_count, y_char_count, y_ones_count)) = extract(x_name, y_name, **cuts)\n",
    "\n",
    "x_embed = embeddings[x_name+'_Embedding']\n",
    "y_embed = embeddings[y_name+'_Embedding']\n",
    "\n",
    "gru_hidden_size = int(y_embed.word_embedding_size * 3)\n",
    "\n",
    "\n",
    "# character embedding\n",
    "l_encode_character = keras.layers.Dense(   x_embed.character_embedding_size, input_shape=(x_word_count, x_char_count, x_ones_count))\n",
    "\n",
    "# word embedding\n",
    "l_char_to_word =     keras.layers.Reshape( (x_word_count, x_char_count * x_embed.character_embedding_size,))\n",
    "l_encode_word =      keras.layers.Dense(   x_embed.word_embedding_size)\n",
    "\n",
    "# lstm processing\n",
    "l_lstm =             keras.layers.GRU(     gru_hidden_size,   activation='sigmoid',   implementation=2,   unroll=True)\n",
    "l_decode =           keras.layers.Dense(   y_embed.word_embedding_size,   activation='sigmoid')\n",
    "\n",
    "l_decode_word =      keras.layers.Dense(   y_char_count * y_embed.character_embedding_size,   activation='sigmoid')\n",
    "l_word_to_char =     keras.layers.Reshape( (y_char_count, character_embedding_size))\n",
    "\n",
    "l_decode_character = keras.layers.Dense(   y_ones_count,   activation='sigmoid')\n",
    "\n",
    "\n",
    "models[architecture] = keras.Sequential([\n",
    "    l_encode_character,\n",
    "    l_char_to_word,\n",
    "    l_encode_word,\n",
    "    l_lstm,\n",
    "    l_decode,\n",
    "    l_decode_word,\n",
    "    l_word_to_char,\n",
    "    l_decode_character,\n",
    "])\n",
    "\n",
    "models[architecture].compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "print(models[architecture].summary())\n",
    "\n",
    "# set pretrained embedding weights\n",
    "x_embed.apply_encode(l_encode_character, l_encode_word)\n",
    "y_embed.apply_decode(l_decode_word, l_decode_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save/Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DE = models['E-D-NN'].get_weights()\n",
    "#model_GRU_1 = model\n",
    "#model_GRU_2 = model\n",
    "#model_GRU_3 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.set_weights(GRU)\n",
    "#model = model_GRU_3\n",
    "#models['E-D-NN'].set_weights(DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, batch_size):\n",
    "    \n",
    "    # split and shuffle data\n",
    "    ((x_train, x_test, x_preview), (y_train, y_test, y_preview)) = split_and_shuffle(x_onehot, y_onehot, sizes=(20000, 1000, 15))\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    # test\n",
    "    l, a = test(model, x_test, y_test, x_preview, y_preview)\n",
    "    \n",
    "    # show training history\n",
    "    training_log(x_name, y_name, architecture, batch_size, epochs, l, a)\n",
    "    print(*training_history[::-1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "20000/20000 [==============================] - 40s 2ms/step - loss: 1.1868 - mean_absolute_error: 0.0188 - categorical_accuracy: 0.7446 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.7810 - mean_absolute_error: 0.0188 - categorical_accuracy: 0.8012 - exact_match_accuracy: 5.0000e-05\n",
      "Epoch 3/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.6752 - mean_absolute_error: 0.0188 - categorical_accuracy: 0.8186 - exact_match_accuracy: 2.0000e-04\n",
      "Epoch 4/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.5983 - mean_absolute_error: 0.0187 - categorical_accuracy: 0.8357 - exact_match_accuracy: 7.5000e-04\n",
      "Epoch 5/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.5341 - mean_absolute_error: 0.0186 - categorical_accuracy: 0.8517 - exact_match_accuracy: 0.0021\n",
      "Epoch 6/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.4829 - mean_absolute_error: 0.0185 - categorical_accuracy: 0.8658 - exact_match_accuracy: 0.0037\n",
      "Epoch 7/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.4402 - mean_absolute_error: 0.0185 - categorical_accuracy: 0.8775 - exact_match_accuracy: 0.0072\n",
      "Epoch 8/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.4036 - mean_absolute_error: 0.0184 - categorical_accuracy: 0.8879 - exact_match_accuracy: 0.0161\n",
      "Epoch 9/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.3718 - mean_absolute_error: 0.0182 - categorical_accuracy: 0.8961 - exact_match_accuracy: 0.0234\n",
      "Epoch 10/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.3438 - mean_absolute_error: 0.0181 - categorical_accuracy: 0.9040 - exact_match_accuracy: 0.0399\n",
      "Epoch 11/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.3181 - mean_absolute_error: 0.0180 - categorical_accuracy: 0.9114 - exact_match_accuracy: 0.0606\n",
      "Epoch 12/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.2956 - mean_absolute_error: 0.0179 - categorical_accuracy: 0.9179 - exact_match_accuracy: 0.0820\n",
      "Epoch 13/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.2748 - mean_absolute_error: 0.0177 - categorical_accuracy: 0.9239 - exact_match_accuracy: 0.1143\n",
      "Epoch 14/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.2566 - mean_absolute_error: 0.0175 - categorical_accuracy: 0.9296 - exact_match_accuracy: 0.1480\n",
      "Epoch 15/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.2393 - mean_absolute_error: 0.0174 - categorical_accuracy: 0.9346 - exact_match_accuracy: 0.1860\n",
      "Epoch 16/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.2230 - mean_absolute_error: 0.0172 - categorical_accuracy: 0.9395 - exact_match_accuracy: 0.2289\n",
      "Epoch 17/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.2088 - mean_absolute_error: 0.0171 - categorical_accuracy: 0.9442 - exact_match_accuracy: 0.2743\n",
      "Epoch 18/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1955 - mean_absolute_error: 0.0168 - categorical_accuracy: 0.9485 - exact_match_accuracy: 0.3158\n",
      "Epoch 19/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1834 - mean_absolute_error: 0.0167 - categorical_accuracy: 0.9524 - exact_match_accuracy: 0.3533\n",
      "Epoch 20/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.1717 - mean_absolute_error: 0.0164 - categorical_accuracy: 0.9563 - exact_match_accuracy: 0.3987\n",
      "Epoch 21/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1623 - mean_absolute_error: 0.0162 - categorical_accuracy: 0.9590 - exact_match_accuracy: 0.4300\n",
      "Epoch 22/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1528 - mean_absolute_error: 0.0159 - categorical_accuracy: 0.9622 - exact_match_accuracy: 0.4673\n",
      "Epoch 23/70\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.1440 - mean_absolute_error: 0.0157 - categorical_accuracy: 0.9649 - exact_match_accuracy: 0.4993\n",
      "Epoch 24/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1353 - mean_absolute_error: 0.0153 - categorical_accuracy: 0.9676 - exact_match_accuracy: 0.5349\n",
      "Epoch 25/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1278 - mean_absolute_error: 0.0150 - categorical_accuracy: 0.9700 - exact_match_accuracy: 0.5654\n",
      "Epoch 26/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.1212 - mean_absolute_error: 0.0147 - categorical_accuracy: 0.9718 - exact_match_accuracy: 0.5847\n",
      "Epoch 27/70\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.1146 - mean_absolute_error: 0.0144 - categorical_accuracy: 0.9736 - exact_match_accuracy: 0.6073\n",
      "Epoch 28/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.1083 - mean_absolute_error: 0.0138 - categorical_accuracy: 0.9754 - exact_match_accuracy: 0.6352\n",
      "Epoch 29/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.1030 - mean_absolute_error: 0.0137 - categorical_accuracy: 0.9767 - exact_match_accuracy: 0.6526\n",
      "Epoch 30/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0972 - mean_absolute_error: 0.0132 - categorical_accuracy: 0.9786 - exact_match_accuracy: 0.6771\n",
      "Epoch 31/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0924 - mean_absolute_error: 0.0130 - categorical_accuracy: 0.9797 - exact_match_accuracy: 0.6932\n",
      "Epoch 32/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0883 - mean_absolute_error: 0.0126 - categorical_accuracy: 0.9808 - exact_match_accuracy: 0.7090\n",
      "Epoch 33/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0833 - mean_absolute_error: 0.0123 - categorical_accuracy: 0.9822 - exact_match_accuracy: 0.7288\n",
      "Epoch 34/70\n",
      "20000/20000 [==============================] - 31s 2ms/step - loss: 0.0799 - mean_absolute_error: 0.0120 - categorical_accuracy: 0.9831 - exact_match_accuracy: 0.7402\n",
      "Epoch 35/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0762 - mean_absolute_error: 0.0117 - categorical_accuracy: 0.9838 - exact_match_accuracy: 0.7533\n",
      "Epoch 36/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0729 - mean_absolute_error: 0.0114 - categorical_accuracy: 0.9847 - exact_match_accuracy: 0.7656\n",
      "Epoch 37/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0693 - mean_absolute_error: 0.0112 - categorical_accuracy: 0.9857 - exact_match_accuracy: 0.7808\n",
      "Epoch 38/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0661 - mean_absolute_error: 0.0108 - categorical_accuracy: 0.9864 - exact_match_accuracy: 0.7910\n",
      "Epoch 39/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0635 - mean_absolute_error: 0.0107 - categorical_accuracy: 0.9871 - exact_match_accuracy: 0.8001\n",
      "Epoch 40/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0611 - mean_absolute_error: 0.0105 - categorical_accuracy: 0.9876 - exact_match_accuracy: 0.8089\n",
      "Epoch 41/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0581 - mean_absolute_error: 0.0103 - categorical_accuracy: 0.9884 - exact_match_accuracy: 0.8214\n",
      "Epoch 42/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0559 - mean_absolute_error: 0.0100 - categorical_accuracy: 0.9887 - exact_match_accuracy: 0.8270\n",
      "Epoch 43/70\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.0540 - mean_absolute_error: 0.0099 - categorical_accuracy: 0.9892 - exact_match_accuracy: 0.8338\n",
      "Epoch 44/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0520 - mean_absolute_error: 0.0096 - categorical_accuracy: 0.9896 - exact_match_accuracy: 0.8396\n",
      "Epoch 45/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0494 - mean_absolute_error: 0.0095 - categorical_accuracy: 0.9903 - exact_match_accuracy: 0.8507\n",
      "Epoch 46/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0477 - mean_absolute_error: 0.0094 - categorical_accuracy: 0.9907 - exact_match_accuracy: 0.8559\n",
      "Epoch 47/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0461 - mean_absolute_error: 0.0092 - categorical_accuracy: 0.9909 - exact_match_accuracy: 0.8601\n",
      "Epoch 48/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0450 - mean_absolute_error: 0.0091 - categorical_accuracy: 0.9912 - exact_match_accuracy: 0.8637\n",
      "Epoch 49/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0428 - mean_absolute_error: 0.0090 - categorical_accuracy: 0.9917 - exact_match_accuracy: 0.8714\n",
      "Epoch 50/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0415 - mean_absolute_error: 0.0088 - categorical_accuracy: 0.9920 - exact_match_accuracy: 0.8749\n",
      "Epoch 51/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0404 - mean_absolute_error: 0.0087 - categorical_accuracy: 0.9922 - exact_match_accuracy: 0.8780\n",
      "Epoch 52/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0386 - mean_absolute_error: 0.0086 - categorical_accuracy: 0.9926 - exact_match_accuracy: 0.8848\n",
      "Epoch 53/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0374 - mean_absolute_error: 0.0084 - categorical_accuracy: 0.9930 - exact_match_accuracy: 0.8907\n",
      "Epoch 54/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0358 - mean_absolute_error: 0.0084 - categorical_accuracy: 0.9934 - exact_match_accuracy: 0.8961\n",
      "Epoch 55/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0350 - mean_absolute_error: 0.0082 - categorical_accuracy: 0.9933 - exact_match_accuracy: 0.8950\n",
      "Epoch 56/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0337 - mean_absolute_error: 0.0082 - categorical_accuracy: 0.9938 - exact_match_accuracy: 0.9020\n",
      "Epoch 57/70\n",
      "20000/20000 [==============================] - 37s 2ms/step - loss: 0.0330 - mean_absolute_error: 0.0081 - categorical_accuracy: 0.9938 - exact_match_accuracy: 0.9024\n",
      "Epoch 58/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0324 - mean_absolute_error: 0.0080 - categorical_accuracy: 0.9939 - exact_match_accuracy: 0.9052\n",
      "Epoch 59/70\n",
      "20000/20000 [==============================] - 37s 2ms/step - loss: 0.0316 - mean_absolute_error: 0.0080 - categorical_accuracy: 0.9941 - exact_match_accuracy: 0.9071\n",
      "Epoch 60/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0306 - mean_absolute_error: 0.0079 - categorical_accuracy: 0.9943 - exact_match_accuracy: 0.9108\n",
      "Epoch 61/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0291 - mean_absolute_error: 0.0078 - categorical_accuracy: 0.9947 - exact_match_accuracy: 0.9166\n",
      "Epoch 62/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0287 - mean_absolute_error: 0.0077 - categorical_accuracy: 0.9947 - exact_match_accuracy: 0.9170\n",
      "Epoch 63/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0270 - mean_absolute_error: 0.0077 - categorical_accuracy: 0.9952 - exact_match_accuracy: 0.9246\n",
      "Epoch 64/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0271 - mean_absolute_error: 0.0076 - categorical_accuracy: 0.9951 - exact_match_accuracy: 0.9226\n",
      "Epoch 65/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0264 - mean_absolute_error: 0.0076 - categorical_accuracy: 0.9953 - exact_match_accuracy: 0.9263\n",
      "Epoch 66/70\n",
      "20000/20000 [==============================] - 32s 2ms/step - loss: 0.0255 - mean_absolute_error: 0.0075 - categorical_accuracy: 0.9956 - exact_match_accuracy: 0.9299\n",
      "Epoch 67/70\n",
      "20000/20000 [==============================] - 34s 2ms/step - loss: 0.0258 - mean_absolute_error: 0.0074 - categorical_accuracy: 0.9953 - exact_match_accuracy: 0.9272\n",
      "Epoch 68/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0243 - mean_absolute_error: 0.0074 - categorical_accuracy: 0.9957 - exact_match_accuracy: 0.9317\n",
      "Epoch 69/70\n",
      "20000/20000 [==============================] - 35s 2ms/step - loss: 0.0241 - mean_absolute_error: 0.0073 - categorical_accuracy: 0.9957 - exact_match_accuracy: 0.9323\n",
      "Epoch 70/70\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.0237 - mean_absolute_error: 0.0073 - categorical_accuracy: 0.9959 - exact_match_accuracy: 0.9342\n",
      "0.  '  'AR91 _ AR91 - 30 _ FINAL _ FILTERED _ SDU02007TA _ 238982 . SGY                                      '  'AR91-30 '  'AR91-30 '  'True\n",
      "1.  '  'LYNDON _ CAVES _ AND _ EXTENSION _ 84 - L8 _ FINAL _ FILTERED _ MIGRATED _ SDU02007TA _ 238975 . SGY '  '84-L8   '  '84-L8   '  'True\n",
      "2.  '  'WOODDUCK _ 87 - WD15 _ FINAL _ FILTERED _ SDU02007TA _ 238983 . SGY                                  '  '87-WD15 '  '87-WD15 '  'True\n",
      "3.  '  'NIELLA _ AND _ EXTENSION _ 85 - N50 _ FINAL _ FILTERED _ SDU02007TA _ 238984 . SGY                   '  '85-N50  '  '85-N50  '  'True\n",
      "4.  '  'AR91 _ AR91 - 31 _ FINAL _ FILTERED _ SDU02007TA _ 238981 . SGY                                      '  'AR91-31 '  'AR91-31 '  'True\n",
      "5.  '  'AR91 _ AR91 - 31 _ FINAL _ FILTERED _ MIGRATED _ SDU02007TA _ 238977 . SGY                           '  'AR91-31 '  'AR91-31 '  'True\n",
      "6.  '  'WALLABELLA _ AND _ EXTENSION _ 84 - W32 _ FINAL _ FILTERED _ MIGRATED _ SDU02007TA _ 238971 . SGY    '  '84-W32  '  '84-W32  '  'True\n",
      "7.  '  'LYNDON _ CAVES _ AND _ EXTENSION _ 84 - L68 _ RAW _ MIGRATED _ SDU02007TA _ 238973 . SGY             '  '84-L68  '  '84-L68  '  'True\n",
      "8.  '  'EAST _ BALLYMENA _ NOMBY _ BRENTWOOD _ M97 - EB03 _ FINAL _ MIGRATED _ SDU02062TA _ 238979 . SGY     '  'M97-EB03'  'M97-EB03'  'True\n",
      "9.  '  'LYNDON _ CAVES _ AND _ EXTENSION _ 84 - L9 _ FINAL _ FILTERED _ MIGRATED _ SDU02007TA _ 238974 . SGY '  '84-L9   '  '84-L9   '  'True\n",
      "10. '  'NIELLA _ AND _ EXTENSION _ 85 - N31 _ FINAL _ FILTERED _ SDU02007TA _ 238985 . SGY                   '  '85-N31  '  '85-N31  '  'True\n",
      "11. '  'LYNDON _ CAVES _ AND _ EXTENSION _ 84 - L61 _ FINAL _ FILTERED _ MIGRATED _ SDU02007TA _ 238972 . SGY'  '84-L61  '  '84-L61  '  'True\n",
      "12. '  'AR91 _ AR91 - 32 _ FINAL _ FILTERED _ SDU02007TA _ 238980 . SGY                                      '  'AR91-32 '  'AR91-32 '  'True\n",
      "13. '  'LYNDON _ CAVES _ AND _ EXTENSION _ 84 - L8 _ FINAL _ FILTERED _ SDU02007TA _ 238978 . SGY            '  '84-L8   '  '84-L8   '  'True\n",
      "14. '  'WALLABELLA _ AND _ EXTENSION _ 84 - W36 _ FINAL _ FILTERED _ MIGRATED _ SDU02007TA _ 238970 . SGY    '  '84-W36  '  '84-W36  '  'True\n",
      "\n",
      "1000/1000 [==============================] - 4s 4ms/step\n",
      "('categorical_crossentropy', 0.04770743289589882)\n",
      "('mean_absolute_error', 0.006021645601838827)\n",
      "('categorical_accuracy', 0.9874782557487488)\n",
      "(<function exact_match_accuracy at 0x00000211DAA0DE18>, 0.828)\n",
      "\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'GRU-RNN3', 'batch size': 64, 'epochs': 70, 'loss': 0.04770743289589882, 'accuracy': 0.828}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 70, 'loss': 0.02715689045190811, 'accuracy': 0.214}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 70, 'loss': 0.001975665943697095, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 70, 'loss': 0.002201953295618296, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 2, 'loss': 0.324474746465683, 'accuracy': 0.227}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 2, 'loss': 0.33672117018699643, 'accuracy': 0.271}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 50, 'loss': 0.34114727401733397, 'accuracy': 0.181}\n",
      "{'x': 'FileName_Words', 'y': 'SurveyName', 'architecture': 'LSTM-RNN4', 'batch size': 64, 'epochs': 70, 'loss': nan, 'accuracy': 0.0}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 70, 'loss': 0.07678727588057518, 'accuracy': 0.699}\n",
      "{'x': 'FileName_Words', 'y': 'LineName', 'architecture': 'LSTM-RNN3', 'batch size': 64, 'epochs': 1, 'loss': 0.7949419174194335, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "train(models['GRU-RNN3'], 70, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 1.1959 - mean_absolute_error: 0.0188 - categorical_accuracy: 0.7444 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 2/70\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.7983 - mean_absolute_error: 0.0188 - categorical_accuracy: 0.8000 - exact_match_accuracy: 0.0000e+00\n",
      "Epoch 3/70\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.6771 - mean_absolute_error: 0.0188 - categorical_accuracy: 0.8180 - exact_match_accuracy: 1.0000e-04\n",
      "Epoch 4/70\n",
      "20000/20000 [==============================] - 50s 2ms/step - loss: 0.6052 - mean_absolute_error: 0.0187 - categorical_accuracy: 0.8344 - exact_match_accuracy: 6.5000e-04\n",
      "Epoch 5/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.5431 - mean_absolute_error: 0.0186 - categorical_accuracy: 0.8507 - exact_match_accuracy: 0.0013\n",
      "Epoch 6/70\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.4891 - mean_absolute_error: 0.0185 - categorical_accuracy: 0.8667 - exact_match_accuracy: 0.0033\n",
      "Epoch 7/70\n",
      "20000/20000 [==============================] - 50s 2ms/step - loss: 0.4451 - mean_absolute_error: 0.0184 - categorical_accuracy: 0.8789 - exact_match_accuracy: 0.0085\n",
      "Epoch 8/70\n",
      "20000/20000 [==============================] - 50s 2ms/step - loss: 0.4080 - mean_absolute_error: 0.0183 - categorical_accuracy: 0.8886 - exact_match_accuracy: 0.0150\n",
      "Epoch 9/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.3757 - mean_absolute_error: 0.0182 - categorical_accuracy: 0.8972 - exact_match_accuracy: 0.0247\n",
      "Epoch 10/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.3468 - mean_absolute_error: 0.0181 - categorical_accuracy: 0.9047 - exact_match_accuracy: 0.0427\n",
      "Epoch 11/70\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.3211 - mean_absolute_error: 0.0180 - categorical_accuracy: 0.9115 - exact_match_accuracy: 0.0657\n",
      "Epoch 12/70\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.2987 - mean_absolute_error: 0.0179 - categorical_accuracy: 0.9174 - exact_match_accuracy: 0.0879\n",
      "Epoch 13/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.2794 - mean_absolute_error: 0.0177 - categorical_accuracy: 0.9229 - exact_match_accuracy: 0.1129\n",
      "Epoch 14/70\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.2607 - mean_absolute_error: 0.0176 - categorical_accuracy: 0.9283 - exact_match_accuracy: 0.1439\n",
      "Epoch 15/70\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.2445 - mean_absolute_error: 0.0175 - categorical_accuracy: 0.9332 - exact_match_accuracy: 0.1795\n",
      "Epoch 16/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.2296 - mean_absolute_error: 0.0173 - categorical_accuracy: 0.9376 - exact_match_accuracy: 0.2109\n",
      "Epoch 17/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.2156 - mean_absolute_error: 0.0172 - categorical_accuracy: 0.9417 - exact_match_accuracy: 0.2451\n",
      "Epoch 18/70\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.2026 - mean_absolute_error: 0.0170 - categorical_accuracy: 0.9457 - exact_match_accuracy: 0.2804\n",
      "Epoch 19/70\n",
      " 8704/20000 [============>.................] - ETA: 26s - loss: 0.1950 - mean_absolute_error: 0.0169 - categorical_accuracy: 0.9482 - exact_match_accuracy: 0.3028 ETA: 34s - loss"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-3073333692bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GRU-RNN3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-185-3a18175d5e98>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(models['GRU-RNN3'], 70, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
